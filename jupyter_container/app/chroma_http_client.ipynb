{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with no authentication\n",
    "chroma_client = chromadb.HttpClient(host='chromadb', port=8000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with token authentication\n",
    "chroma_client = chromadb.HttpClient(host='chromadb', port=8000,\n",
    "    settings=Settings(\n",
    "        chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n",
    "        chroma_client_auth_credentials=\"test-token\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect with role-based authentication\n",
    "# chroma_client = chromadb.HttpClient(host='chromadb', port=8000,\n",
    "#     settings=Settings(\n",
    "#         chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n",
    "#         chroma_server_authn_provider=\"chromadb.auth.simple_rbac_authz.SimpleRBACAuthorizationProvider\",\n",
    "#         chroma_client_auth_credentials=\"test-token-readonly\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# Create the collection, aka vector database. Or, if database already exist, then use it. Specify the model that we want to use to do the embedding.\n",
    "collection = chroma_client.get_or_create_collection(name=\"grc_docs\", embedding_function=sentence_transformer_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed document: Schneider Electric understand the total sustainability impact of liion UPS batteries.pdf\n",
      "Indexed document: Immersion_Cooling_for_High-Density_Sustainable_Computing.pdf\n",
      "Indexed document: Wyoming_Use_Case_v2.pdf\n",
      "Indexed document: Navigating Liquid Cooling Architectures for Data Centers with AI Workloads.pdf\n",
      "Indexed document: shell-immersion-cooling-fluid-s5-x-brochure.pdf\n",
      "Indexed document: Site Readiness Checklist Template JAN 2024 (2).pdf\n",
      "Indexed document: Submer Thermodynamics.pdf\n",
      "Indexed document: Castrol ON DC 15 - UK - EN .pdf\n",
      "Indexed document: Vertiv-LiquidCooling-KIH-WP-EN-NA-SL.pdf\n",
      "Indexed document: MergeIT-SustainableAppAdjacentVDIForAI&HPCWorkloads-Infographic-1280x720px-RGB-mk1.pdf\n",
      "Indexed document: DC 20 - SDS .pdf\n",
      "Indexed document: Five reasons to adop liquid cooling.pdf\n",
      "Indexed document: Telefonica-Case-Study.pdf\n",
      "Indexed document: Hypertec  Immersion-Born Trident Servers  5-13-2024.pdf\n",
      "Indexed document: Capital Cost Analysis Immersion vs Air Cooled.pdf\n",
      "Indexed document: Sustainability-With-Substance-White-Paper.pdf\n",
      "Indexed document: Asperitas_Immersed_Computing_ 06 12-05-2022.pdf\n",
      "Indexed document: GRC-iceraq-series10-data-sheet Quad - Duo.pdf\n",
      "Indexed document: Supermicro Applications get a boost with Immersion Cooling.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Directory containing PDF documents\n",
    "docs_dir = \"/app/pdfs\"\n",
    "\n",
    "# Prepare documents for indexing\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "id = 1\n",
    "\n",
    "for filename in os.listdir(docs_dir):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(docs_dir, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        documents.append(text)\n",
    "        metadatas.append({\"filename\": filename})\n",
    "        ids.append(str(id))\n",
    "        id += 1\n",
    "        print(f\"Indexed document: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Load sample data (a restaurant menu of items)\n",
    "# with open('./../menu_items.csv') as file:\n",
    "#     lines = csv.reader(file)\n",
    "\n",
    "#     # Store the name of the menu items in this array. In Chroma, a \"document\" is a string i.e. name, sentence, paragraph, etc.\n",
    "#     documents = []\n",
    "\n",
    "#     # Store the corresponding menu item IDs in this array.\n",
    "#     metadatas = []\n",
    "\n",
    "#     # Each \"document\" needs a unique ID. This is like the primary key of a relational database. We'll start at 1 and increment from there.\n",
    "#     ids = []\n",
    "#     id = 1\n",
    "\n",
    "#     # Loop thru each line and populate the 3 arrays.\n",
    "#     for i, line in enumerate(lines):\n",
    "#         if i==0:\n",
    "#             # Skip the first row (the column headers)\n",
    "#             continue\n",
    "\n",
    "#         documents.append(line[1])\n",
    "#         metadatas.append({\"item_id\": line[0]})\n",
    "#         ids.append(str(id))\n",
    "#         id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the data to the vector database. ChromaDB automatically converts and stores the text as vector embeddings. This may take a few minutes.\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" \\n \\n \\n \\nUnderstanding the Total  \\nSustainability Impact of Li-ion \\nUPS Batteries \\nExecutive summary \\nInterest and demand continue to grow for lithium-ion batteries as a \\nreplacement for VRLA batteries for UPS applications because of \\nbenefits such as smaller size and longer life expectancy. But there \\nare many questions decision makers still have regarding the impact \\nthese batteries have on sustainability. The full environmental and \\nsocial impact includes mining practices of the raw materials, the \\nmanufacturing process of the batteries, operations, and how to re-\\npurpose or recycle them when they reach the end of their useful \\nlife. In this paper, we demonstrate that, while not a black and white \\ntopic, on balance, li-ion has an overall lower impact compared to \\nVRLA over the complete lifecycle today, and we anticipate this to \\nfurther improve in the future. For each life cycle phase, we describe \\nbest practices and attributes to look for in vendors and providers to \\nensure responsible, ethical, economical, and sustainable solutions \\nare deployed. \\nVersion 1 \\nWhite Paper 71 \\nby Raymond Lizotte and Wendy Torell \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     2 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\nSince their invention in 1985, Lithium-ion (li-ion) battery technology has advanced \\nsignificantly and is now capable of being used in applications that historically have \\nbeen supported by lead acid or valve-regulated lead-acid (VRLA) batteries. One \\nsuch application is uninterruptible power supplies (UPSs) in data centers1. But as \\nthe industry increasingly transitions to li-ion battery technology for these UPS appli-\\ncations, there is interest in understanding and questions that arise regarding the \\nsustainability2 implications of the transition. And as data center owners drive to-\\nwards sustainable lifecycle practices, this topic increasingly comes up.3  \\n \\nVRLA batteries have been used in data centers for decades, and as such, the haz-\\nards and environmental issues associated with them are well known. The industry is \\nexperienced in managing these batteries throughout their life cycle to minimize their \\nenvironmental impact. On the other hand, the hazards and environmental issues as-\\nsociated with li-ion batteries are both less known and different than their lead acid \\ncounterparts, and the industry is still evolving. This makes it challenging to make an \\ninformed choice between the two battery technologies on the basis of environmen-\\ntal sustainability. \\n \\nDecision makers are raising questions or concerns about the wisdom of selecting li-\\nion over VRLA, due largely to claims of child labor in the extraction of cobalt used in \\nsome chemistries of li-ion batteries, excessive carbon intensity to manufacture \\nthese batteries, concerns regarding their safety in transport and in use, and con-\\ncerns regarding end of life management. These claims typically give an incomplete \\npicture because they only focus on certain (negative) aspects of li-ion batteries ra-\\nther than viewing it holistically over the life compared to VRLA technology.   \\n \\nWe conducted a quantitative study comparing VRLA to Li-ion batteries to assess \\nthe sustainability impact over their life cycle, also known as a life cycle assessment \\n(LCA). Figure 1 summarizes the findings and shows that li-ion has a lower environ-\\nmental impact than VRLA in eight of the nine categories, including carbon emis-\\nsions. White Paper 91, Quantitative Life Cycle assessment (LCA) of VRLA vs. Li-ion \\nUPS Batteries, walks through this analysis in depth.  \\n \\n0%\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\nAbiotic depletion\\n(elements)\\n(ADPe)\\nAbiotic depletion\\n(fossil fuels)\\n(ADPf)\\nAcidification\\npotential of soil\\nand water (A)\\nAir pollution (AP)\\nEutrophication\\n(EP)\\nGlobal warming\\n(GWP)\\nOzone layer\\ndepletion ODP\\nsteady state\\n(ODP)\\nPhotochemical\\noxidation (POCP)\\nWater Pollution\\n(WP)\\nVRLA\\nLi-ion\\n \\n \\n1 Other applications include electric vehicles and battery energy storage systems. See White Paper 229, \\nBattery Technology for Data Centers: VRLA vs. Li-ion, and White Paper 231, FAQs for Using Lithium-\\nion Batteries with a UPS, for more information on the technology differences and drivers. \\n2 In 1987, the United Nations Brundtland Commission defined sustainability as: “Meeting the needs of \\nthe present without compromising the ability of future generations to meet their own needs.” Sustaina-\\nbility includes three main categories: environmental, social, and governance (ESG). \\n3 White Paper 64, Four Key Drivers for Colocation Data Centers to Prioritize Environmental Sustainability, \\ndescribes the reasons behind this increasing trend.  \\nIntroduction \\nFigure 1 \\nResults of quantitative \\nanalysis comparing \\nVRLA and li-ion batteries \\non nine environmental \\nsustainability metrics \\n \\nSource: Schneider  \\nElectric White Paper 91 \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     3 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\nThe full environmental impact includes mining practices of the raw materials, the \\nmanufacturing process of the batteries, operations, and how to repurpose (through \\nsecondary uses) and recycle them when they reach the end of their useful life in \\nUPS applications. Embracing the circular economy and managing each phase of \\nthe life cycle is critical to sustainable design and operation. Figure 2 illustrates the \\nkey categories across the life cycle diagram where sustainability differences should \\nbe understood between VRLA and li-ion.  \\n \\nSupply \\nchain\\nOperation\\nEnd of life\\n•\\nRaw material \\nextraction\\n•\\nManufacturing \\nprocess\\n•\\nDistribution & \\ntransportation\\n•\\nInstallation & \\nhandling\\n•\\nEnergy consumption \\n& carbon emissions\\n•\\nLife span\\n•\\nDelaying end of \\nlife with secondary \\nuse applications\\n•\\nRecycling process\\n \\n \\nIn this paper, we discuss how these factors impact the environment today as well \\nas our expectation for the future. For each, we describe practices and attributes to \\nlook for in vendors and partners to ensure responsible, ethical, economical, and en-\\nvironmentally sustainable solutions are deployed. \\n \\n \\nMany of the sustainability questions that arise relate to the battery supply chain. \\nThis includes (1) raw material extraction, (2) the manufacturing process, and (3) the \\ndistribution and transportation of the batteries after they are manufactured. Below \\nwe explain the sustainability impact at each phase, discuss some common miscon-\\nceptions, and describe how VRLA and li-ion batteries differ. \\n \\nRaw material extraction \\nThe raw materials needed in the manufacturing of new batteries can come either \\nfrom mining the materials or extracting them from recycled materials. In the case \\nof VRLA batteries, the recycling system is almost an ecological closed loop. The \\nplastic parts of the battery are recycled into more battery plastic. The sulfuric acid \\nis collected and resold as commodity acid. The lead is smelted and returned back \\nto batteries or applied to other uses of lead. According to a study conducted by \\nBattery Council International (BCI), Chicago, and Essential Energy Every day, lead \\nbatteries have a return for recycling rate of 99.3 percent, making them the No.1 re-\\ncycled consumer product in the U.S.4 The recycling is highly regulated in the US \\nand EU and well established internationally, but with need for more regulation (see \\nthe Toxicity section below). By partnering with reputable UPS suppliers or battery \\nmanufacturers, most VRLA battery owners can dispose of their spent batteries free \\nof charge. \\n \\n4 https://www.recyclingtoday.com/article/battery-council-international-lead-battery-recycling/ \\nSupply chain \\nFigure 2 \\nAll stages of the battery \\nlife cycle have an impact \\non sustainability \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     4 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\nLi-ion battery minerals are largely obtained through mining since recycling prac-\\ntices are still developing. Later, in the “End of life” section of this paper, we discuss \\nthe implications of this further. \\n \\nWhen contrasting the sustainability implications of VRLA to li-ion during raw material \\nextraction, there are three main considerations: \\n \\n• the toxicity of the processes \\n• the safety and ethics of the mining practices \\n• the mass of material needed \\n \\nToxicity – During raw material extraction of VRLA batteries, including the recycling \\nprocess, lead is released and can have a significant health impact. World Health \\nOrganization summarized these concerns in a 2017 report, Recycling used lead-\\nacid batteries: health considerations. “Recycling used lead-acid batteries is of pub-\\nlic health concern because this industry is associated with a high level of occupa-\\ntional exposure and environmental emissions. Furthermore, there is no known safe \\nlevel of exposure to lead, and the health impacts of lead exposure are significant. \\nBased on 2016 data, it is estimated that lead exposure accounted for 495,550 \\ndeaths and 9.3 million disability-adjusted life years (DALYs) lost due to long term \\nimpacts on health, with the highest burden in low- and middle-income countries \\n(IHME, 2016). Young children and women of childbearing age are particularly vul-\\nnerable to exposure to, and the toxic effects of, lead.”  \\n \\nIt’s been clear for many decades (and even centuries5) that the toxicity of lead is \\nhazardous to human wellbeing. Over the years, we saw many public health move-\\nments to remove it from gasoline, paint, solder, water-system piping, and so on, to \\navoid air and water contamination. The toxicity of lead is the reason VRLA battery \\nmanufacturing and recycling processes are so regulated in many parts of the world. \\nProcesses include the wearing of respirators, constant cleaning, air filters/controls, \\netc. to prevent lead from escaping into the environment. But there is still environ-\\nmental contamination and human exposure happening at mines and recycling facili-\\nties in countries where it is poorly regulated, as the World Health Organization \\n(WHO) report referenced above noted. Figure 3 shows the many points where lead \\nis released in the recycling process. \\n \\nFor the reasons above, we’ve seen the elimination of lead in many aspects of our \\nlives as safer alternatives became available, and we can expect that same trend to \\ncontinue in other applications, such as automobiles and UPSs/energy storage.  \\n \\nWhile regulatory bodies across the globe classify lead acid batteries as toxic haz-\\nardous materials with health and environmental implications, li-ion batteries, which \\ncontain less toxic metals, are generally classified as non-hazardous waste. One ex-\\nample of such a test used to classify a battery as hazardous or not is the Toxicity \\nCharacteristic Leaching Procedure, also known as TCLP testing. This analysis simu-\\nlates conditions within a landfill and determines which of the contaminants identified \\nby the United States Environmental Protection Agency (EPA) are present and at \\nwhat concentrations. 6  \\n \\n  \\n \\n \\n5 https://environmentalhistory.org/about/ethyl-leaded-gasoline/lead-history-timeline/ \\n6 https://leadlab.com/what-is-a-tclp-test/ \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     5 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\nDraining \\nelectrolyte\\nBreaking up \\nbatteries into \\ncomponent \\nparts\\nConveying \\nbroken battery \\nparts to the \\nsmelter\\nSmelting \\nand refining\\nWorkers\\ngo home \\nwithout \\nwashing and \\nchanging \\nclothes\\nLead\\ncontamination of\\nsoil & water\\nToxic smoke\\n(i.e. sulfur dioxide) & \\nlead-contaminated \\nwaste\\nLead fragments & \\nlead oxide dust \\ndispersed in air, settle\\non soil, surfaces,\\nworkers’ hair/clothing\\nLead fumes in air, \\ninhaled by workers; \\nparticles settle\\non soil, surfaces,\\nworkers’ hair/clothing\\nLead\\ncontamination of\\nsoil & water\\n \\n \\nMining practices – While the raw materials within li-ion batteries don’t have the \\nsame toxicity concerns as batteries with lead, the mining practices have raised con-\\ncerns over social and environmental impacts. Most of the questionable mining prac-\\ntices are based on li-ion battery chemistries that include the raw material cobalt, an \\ningredient often used in larger 3-phase UPS batteries.7 \\n \\nCobalt is largely mined in the Democratic Republic of Congo. Mining is not always \\nregulated, and some mines and smelters may conduct unethical labor practices, in-\\ncluding the use of child labor and unsafe work conditions. The Responsible Miner-\\nals Initiative has a process where smelters and refiners can register their organiza-\\ntion and they get 3rd party independently audited to confirm ethical practices, in line \\nwith current global standards. It’s important to look for UPS vendors that only \\nsource batteries from manufacturers/distributors that only source materials from \\nmines and smelters that are part of this initiative (and can prove it with documenta-\\ntion), to ensure ethical practices. Since supply chains evolve over time, it’s also im-\\nportant to ensure vendors are using providers that are annually recertified.   \\n \\nThe mining of lithium has also been criticized as having a negative environmental \\nimpact. Because today it is largely sourced from mining (since recycling processes \\nare still developing), there are some parts of the world where the mining practices \\nare not sustainable. For instance, in the Atacama Salt Flats in Chile, which has the \\ngreatest source of the world’s lithium (see Figure 4), “the region’s ecosystem is \\nfragile and there is lack of consensus regarding the impacts and risks of lithium \\nmining and other economic activity in the region. Potential risks from changes in \\nwater and brine table levels could potentially harm the ecosystems and affect local \\nlivelihoods.”8 \\n \\nBut the demand for li-ion batteries, driven largely by the rapidly growing electric ve-\\nhicle (EV) market, is moving the industry towards more stringent requirements for \\nminers to improve the sustainability of the EV supply chains. UPS applications will \\nbenefit from that momentum. And as the recycling practices mature, we will move to \\na more circular economy with a larger percentage of the raw materials obtained \\nfrom recycled sources.  \\n \\nEven with the current state of development of these mining and recycling practices, \\nthe environmental impacts of li-ion batteries are still lower than their VRLA counter-\\nparts. Specifically, this is due to the toxicity of lead. The analysis in White Paper 91, \\n \\n7 See Schneider Electric White Paper 284, Considerations for Selecting a Lithium-ion Battery System for \\nUPSs and Energy Storage Systems, for more about chemistries. \\n8 https://www.globalminingreview.com/environment-sustainability/09062021/companies-start-partner-\\nship-for-sustainable-lithium-mining-in-chile/ \\nFigure 3 \\nPoints where lead is released \\nduring the recycling process \\n \\nSource: World Health  \\nOrganization, Recycling used \\nlead-acid batteries: \\nhealth considerations \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     6 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nQuantitative Life Cycle assessment (LCA) of VRLA vs. Li-ion UPS Batteries, quanti-\\nfies these environmental impacts, and shows that lead has greater water and air \\npollution impacts due largely to lead’s toxicity (See Figure 1) . \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nMass of material – The other key environmental risk factor for raw material sourcing \\nis the amount of material needed. The energy density of a li-ion battery compared to \\nits equivalent VRLA battery drives a significant reduction in size and weight. For ex-\\nample, a 1MW UPS battery with 6 minutes of runtime may have a battery solution \\nweighing over 11,340 kg (25,000 lbs), vs a li-ion battery weighing only 2,767 kg \\n(6,100 lbs)9. That’s a significant reduction of 300%.  \\n \\n \\nKEY TAKEAWAY – The smaller mass of material and significant decrease in tox-\\nicity of li-ion results in lower environmental impact overall in the material sourc-\\ning phase of the battery life cycle. \\n \\n \\nManufacturing process \\nThe variables that drive environmental impact differences for VRLA and li-ion batter-\\nies during the manufacturing process include: \\n \\n• The battery “system” component requirements \\n• Manufacturing process complexity \\n• Lifespan impact \\n \\nThe battery “system” component requirements – Li-ion batteries are not able to \\nwithstand the same level of variation of charging/discharging parameters as VRLA \\nbatteries. Because lithium battery cells are very sensitive to environmental factors \\n(i.e. voltage, temperature), the battery system is more complex to ensure its safety. \\n \\n9 White Paper 229, Battery Technology for Data Centers: VRLA vs. Li-ion, \\nhttps://www.se.com/us/en/download/document/SPD_VAVR-A5AJXY_EN \\nFigure 4 \\nCountries with major lithium production and reserve \\nSource: Volkswagen AG 2021 \\n0.5 \\n0.6 \\n0.8 \\n1.6 \\n6.2 \\n8 \\n16 \\n51 \\n35 \\n54 \\n60 \\n70 \\n2,000 \\n1,000 \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     7 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nTable 1 illustrates the components needed for both VRLA and li-ion battery sys-\\ntems.  \\n \\nVRLA system \\nLi-ion system \\n• \\nEnclosure \\n• \\nBattery cells \\n• \\nConnections/busbars \\n \\n• \\nEnclosure \\n• \\nBattery cells \\n• \\nConnections/busbars \\n• \\nBattery-level BMS \\n• \\nRack-level BMS \\n• \\nSystem-level BMS \\n• \\nSwitchgear \\n• \\nPower supplies \\n \\n \\nAs shown in the table illustrates, there are added components like battery manage-\\nment systems (BMS) at various points in the system – for individual batteries, for the \\ncollection of batteries in a rack/enclosure, and for the overall system; there is also \\nswitchgear needed to manage electrical flow to/from the batteries and power sup-\\nplies for controlled charging. These additional components are designed into the \\nsystem to ensure safety and longevity of the battery. But this equipment overhead \\nadds to the complexity of the manufacturing, as we describe next. Note, some \\nVRLA battery systems do include BMSs and system-level sensors, which adds to \\nthe manufacturing complexity of VRLA, narrowing the difference between the two \\nbattery types. \\n \\nManufacturing process complexity – The manufacturing process of VRLA batteries \\nis simple in comparison to that of li-ion batteries. VRLA processes are very mechan-\\nical and involve primarily casting or stamping operations10. On the other hand, with \\nli-ion, manufacturing circuit boards (the BMSs), switches, and power supplies re-\\nquires a more complex and diverse supply chain with more comprehensive and in-\\ntensive operations. Manufacturing integrated circuits requires micron-level toler-\\nances, micro lasers to cut things, and other advanced technologies. These carry a \\ngreater environmental burden – in terms of air pollution (i.e. ozone depleting sub-\\nstances in the manufacturing of circuit boards), carbon emissions, and water pollu-\\ntion – when you compare it on a 1 to 1 battery basis. This is where many of the \\nquestions stem from. \\n \\nLifespan impact – Although this is largely an “Operations” discussion and will be \\ncovered in greater detail in the next main section, the lifespan impact is an im-\\nportant factor in the environmental impact of the manufacturing processes. Alt-\\nhough as we said earlier, on a 1 to 1 or unit for unit basis, the li-ion has a greater \\nenvironmental impact than VRLA, the reality is that this is a biased comparison, \\ngiven the performance attributes of the batteries. When we consider the environ-\\nmental impact of manufacturing not only the initial VRLA batteries, but also their \\nreplacements over a typical UPS life (often 2 replacements in 10 years), the en-\\nvironmental impact of manufacturing looks very different. The formulas in Figure \\n5 convey this important consideration. You may be led to the wrong conclusion if \\nyou just evaluate one for one battery solutions. This is analogous to making design \\ndecisions on capex vs. TCO. \\n \\n \\n \\n \\n10 https://www3.epa.gov/ttnchie1/ap42/ch12/final/c12s15.pdf \\nTable 1 \\nBattery “system”  \\ncomponents comparison \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     8 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\nX\\nEnvironmental impact\\nof 1 VRLAsystem\\nEnvironmental impact \\nof 1 li-ion system\\nLess \\nthan\\nEnvironmental impact \\nof 1 initial \\n+ 2 replacement \\nVRLA systems\\nEnvironmental Impact \\nof 1 li-ion system\\n1 FOR 1\\nNEEDS OVER \\nUPS LIFETIME\\n\\uf0fc\\nGreater \\nthan\\nA paradigm shift for comparing environmental impact of batteries\\n \\n \\nFor example, this is important for companies looking at embodied carbon footprint \\nof the two approaches as part of their decision criteria. If you don’t take into ac-\\ncount the proper boundaries (the complete life cycle), and instead look only at the \\ndifferences in manufacturing one li-ion vs. one VRLA, you may misinterpret the re-\\nsults and head down the wrong path to a solution that, in fact, has a greater embod-\\nied carbon footprint. \\n \\n \\nKEY TAKEAWAY – For typical installations, when replacement batteries over the \\nUPS lifetime are considered, the environmental impact of manufacturing for li-ion \\nsystems is less than VRLA systems, even with the added equipment overhead \\nfor li-ion. \\n \\n \\nDistribution & transportation \\nThe concerns often brought up regarding li-ion batteries and transportation have to \\ndo with safety because of the transportation “incidents” we hear about in the news. \\nLi-ion batteries are categorized as “dangerous goods” by international regulations \\nand by many transport companies, and therefore, strict regulations and processes \\nmake transporting them more complex. For instance, specialized transportation is \\nrequired, with temperature and pressure-controlled spaces, which drives up the \\ncost to ship li-ion per kg. It’s a learning curve for many companies, and it’s im-\\nportant to align with vendors and suppliers that have expertise in handling them.  \\n \\nAs discussed earlier, the higher energy density of li-ion results in a significantly \\nlighter weight (typically 60-80% less) compared to VRLA, so although the cost/kg \\nmay be greater, it is offset by the lighter weight. Environmental impact from the \\ntransportation and distribution of batteries is also driven largely by the size and \\nweight of the batteries.  \\n \\n \\nKEY TAKEAWAY – Although the complexity of distribution & transportation is in-\\ncreased, the lighter weight of li-ion over VRLA enables a reduced environmental \\nimpact. \\n \\n \\n \\n \\n \\n \\nFigure 5 \\nProperly defining the \\nboundaries (to include \\ninitial batteries AND nec-\\nessary replacements) in \\nan environmental impact \\nanalysis is crucial to a \\nmeaningful conclusion. \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     9 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\nThe operations or “use” phase represents the bulk of the sustainability impact over \\nthe battery life cycle. Below we describe key environmental considerations from (1) \\ninstallation & handling, (2) energy consumption, and (3) lifespan differences that re-\\nsult in a lower overall impact for li-ion. \\n \\nInstallation & handling \\nThe concerns often mentioned regarding li-ion during installation and handling re-\\nvolve around safety (i.e. dealing with fire codes), not environmental sustainability. \\nThis topic of safety and the unique code requirements of li-ion will be addressed in \\na future white paper. \\n \\nThe differences in environmental impact that occur during installation and handling \\nof VRLA vs li-ion, although fairly small, is mainly driven by battery weight differ-\\nences. For instance, since li-ion is a lighter solution, the solution can often be car-\\nried into place from the loading dock vs. driven with a forklift that uses fuel or elec-\\ntricity. The differences also result from the fact that over the life cycle, with VRLA, \\nthere are replacement units that must be installed/handled, whereas li-ion batteries \\ngenerally last the life of the UPS. \\n \\n \\nKEY TAKEAWAY – Although a minor impact overall, the lighter weight and need \\nfor less or no li-ion battery replacements results in decreased environmental im-\\npact during installation & handling compared to VRLA. \\n \\n \\nEnergy consumption & carbon emissions \\nThe environmental indicator most questioned with regard to energy consumption is \\nglobal warming potential (GWP) or the scope 2 CO2e emissions11. The magnitude \\nof the carbon footprint that results from the energy consumption depends on two \\nkey factors: \\n \\n• the generation sources for the electricity consumed, which varies significantly \\nfrom state to state and country to country. The emissions factor (kg \\nCO2e/kWh) is greater when sources of generation include coal, natural gas, \\nand oil; it is lower when it includes renewable sources like solar, wind, and nu-\\nclear. The emissions factor (a rate) is the same whether you deploy VRLA or li-\\nion batteries, but the total emissions will differ.   \\n• the energy losses (kWh), include the fixed losses from trickle charging the \\nbattery, and the transient losses from discharging or charging the battery after \\na power outage. For a given emissions factor, the percent reduction in emis-\\nsions is directly proportional to the reduction in losses. \\n \\nWith a goal of achieving net-zero emissions for so many businesses, we are often \\nasked about the impact li-ion technology has on reducing emissions. UPSs are use-\\ndriven products from a CO2e emissions standpoint. In fact, more than 90% of a \\nUPS’s emissions occur in this phase of the life cycle, because of its energy con-\\nsumption. Batteries represent a small portion of this energy consumption. \\n \\nWhen comparing the two types of batteries, li-ion consumes less energy than VRLA \\nbatteries during operation. This is because of battery chemistry differences that \\n \\n11 https://www.epa.gov/climateleadership/scope-1-and-scope-2-inventory-guid-\\nance#:~:text=Scope%202%20emissions%20are%20indirect,of%20the%20organization's%20en-\\nergy%20use. \\nOperation \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     10 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nresult in slower self-discharge rates.12 Batteries must be charged to offset their rate \\nof self-discharge. Typical losses over the lifetime of a VRLA battery are 0.2% of the \\nrated UPS capacity, whereas for li-ion, it is roughly half of that energy, or 0.1%.  \\n \\n \\nKEY TAKEAWAY – Roughly half the energy is needed for keeping li-ion            \\nbatteries charged compared to VRLA, resulting in a decrease in CO2e         \\nemissions during the lifetime operation of the batteries.  \\n \\n \\nLife span \\nAs we briefly mentioned earlier in the manufacturing process section, lifespan is the \\nkey enabler to the environmental impact differences of li-ion over VRLA. Li-ion has \\nslower degradation (more discharge cycles) than VRLA batteries. The differences in \\ncycle life become even greater at higher operating temperature, as VRLA are highly \\nsensitive to operating temperature.  \\n \\nBecause of these differences, VRLA batteries are generally specified for a service \\nlife of 3-5 years, whereas li-ion are specified at 10+ years (actual timeframe de-\\npends largely on power quality and how frequent the battery gets dis-\\ncharged/charged). In the life expectancy of a UPS, this often means 2 VRLA battery \\nreplacements when none are needed for li-ion. For every replacement VRLA battery \\nsolution, the impact on sustainability is multiplied – more manufacturing waste im-\\npacts, more distribution/transportation impacts, more installation & handling im-\\npacts, and so on. The “Lithium-Ion vs. VRLA UPS Battery TCO Calculator” TradeOff \\nTool demonstrates the impact on energy consumption and allows you to vary the \\nassumption of how long each battery technology can remain in operation before re-\\nplacement. The sustainability impact looks very much like the cumulative TCO \\nchart. While it starts out higher on day 1, over time, as replacements of VRLA are \\nneeded, li-ion generally becomes the better alternative. \\n \\n  \\nTAKEAWAY – Li-ion’s longer lifespan than VRLA enables lower sustainability  \\nimpact during operation. The more replacements of VRLA needed over the  \\nUPS lifetime, the better li-ion performs. \\n \\n \\nMany of the questions about the sustainability impact of li-ion focus on end of life \\nand recycling. With VRLA batteries, the recycling practices are mature. Cost effec-\\ntive processes lead to a high percentage of recycled materials. Vendors like \\nSchneider Electric have well established collection, re-use and recycling processes \\nfor more than 98% of the VRLA UPS batteries we place in the market.  \\n \\nWith li-ion, on the other hand, the industry is still maturing, which is why many ques-\\ntions arise. It’s important to find a vendor that is committed to providing their cus-\\ntomers with products that support the same type of circular economy as with VRLA \\nbatteries. A key part of this is ensuring that products are managed at their end of \\nlife. This begins with a safe takeback process for defective and end of life batteries. \\nThis requires knowledge and partnerships in transportation logistics of battery sys-\\ntems subject to stringent safety requirements, and a commitment to customers that \\nthe vendor will recover the batteries when it can no longer serve its primary func-\\ntion.   \\n \\n \\n12 https://www.myussi.com/glossary/battery-self-discharge/ \\nEnd of life \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     11 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nThere are two key elements to ensuring a sustainable end-of life for li-ion batteries \\nonce a vendor takes back the systems. The vendor must be committed to:  \\n \\n• qualified secondary uses that delay end of life \\n• sustainable recycling processes  \\n \\nDelaying end of life with secondary use applications \\nSince the lithium-ion battery industry is still maturing and the economics of recy-\\ncling are not particularly favorable, end of life options are not as sustainable and \\ncost effective as they will be in the future. Some companies are promoting a de-\\nlayed end of life strategy to ensure that when units have to be managed, the neces-\\nsary recycling infrastructure is in place. Delaying end of life serves as a bridge to \\nfull circularity. \\n \\nThe electric vehicle (EV) market, which represents the greatest percentage of li-ion \\nbatteries, is paving the way for second use applications. It is therefore beneficial to \\nfind UPS vendors that are aligning with and leveraging the efforts of this market, in \\nterms of processes. The economics of “second life” represents a promising means \\nof extending the time before the batteries must be recycled, to allow time for the \\neconomics to become more favorable.  \\n \\nBatteries fall into two general categories – energy batteries and power batteries, \\nwith “energy” batteries being optimized for long runtimes and “power” batteries be-\\ning optimized for short duration runtimes. Secondary uses as “energy” batteries are \\npossible because even when power batteries have degraded to (i.e. to 60%-70%), \\nand they are no longer suitable for UPS applications, they can still offer significant \\nbenefit.13 \\n \\nThe National Renewable Energy Laboratory (NREL) conducted an analysis on the \\nfeasibility of such applications when EV batteries reach the end of their useful life. \\nThey found that “the most promising application identified for second use batteries \\nis to replace grid-connected combustion turbine peaker plants and provide peak-\\nshaving services.” In applications like this, second use lifetimes can be on the order \\nof 10 years, meaning batteries won’t need to be recycled for 20+ years from their \\ninitial first use. Their analysis also demonstrated the significant value to the broader \\ncommunity in terms of reduction of greenhouse gas emissions and fossil fuel con-\\nsumption, as well as deferral of battery recycling. \\n \\nThere are also niche markets for using the li-ion power batteries as energy batteries \\nin their second use. A short list of possible second uses is below, but this is far from \\nan exhaustive list, as many more applications exist, and the list will continue to \\ngrow. \\n \\n• Microgrid applications \\n• Agriculture applications, to peak shave the spikey energy use of machinery \\n• Transportation uses, such as powering interior comfort electronics of tour \\nbuses \\n• Hospital uses, such as to support emergency lighting \\n• Home energy storage applications \\n \\n \\n13 See White Paper 229, Battery Technology for Data Centers: VRLA vs. Li-ion, for a description of en-\\nergy vs. power cells. \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     12 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nDeveloping infrastructure to support the logistics, evaluation, and repurposing of \\nthe batteries is crucial to a successful second life system. This includes the testing \\nof batteries to ensure they are capable of providing a second life and identifying \\nand/or qualifying new applications for these batteries. UPS vendors must be com-\\nmitted to second life and partner with the right organizations, to ensure (1) their bat-\\nteries that they take back conform to standards & are certified, and (2) qualified \\nsecond use applications are suitable. \\n \\nConformance to standards: In 2019, UL announced a new standard, UL 197414, \\nthe Standard for Evaluation for Repurposing Batteries. This is focused first on the \\neffective reuse of EV batteries for energy storage systems and covers the sorting \\nand grading process of batteries that were originally configured and used for other \\npurposes and that are intended for a second use application. It is important to be \\naligned with UPS vendors that will ensure standards are met in terms of safety and \\nperformance. \\n \\nSuitable second use applications: UPS vendors in collaboration with external part-\\nners can make sure batteries can be safely and effectively repurposed. Vendors \\nmust be committed to only shipping batteries (reclaimed from customers) to provid-\\ners of validated 2nd uses and not simply rely on claims of 2nd use.  \\n \\nThis secondary use trend was very similar to VRLA many years back when the value \\nof lead was much lower than it is now and the return on lead acid batteries from re-\\ncycling was much lower. There was a very active second use market where compa-\\nnies collected lead acid batteries, tested them, and sold them into markets that re-\\nquired large quantities of low-cost batteries.  The most common was for certain \\nemergency lighting applications.  Around 10-15 years ago, the value of lead be-\\ncame high enough that there was more value obtained from recycling than from re-\\nuse.  That trend has continued such that the worldwide reuse market doesn’t really \\nexist right now.   \\n \\nKEY TAKEAWAY – UPS li-ion batteries have the opportunity for a 2nd life so it can \\ncontinue to provide value beyond its first designed purpose. Using UPS vendors \\nthat are committed to this ensures batteries made today will reach true end of \\nlife when the economics of recycling are cost effective. \\n \\n \\nRecycling process \\nLi-ion battery recycling is in its infancy and therefore the subject of growing con-\\ncern. “There are many reasons why li-ion battery recycling is not yet a universally \\nwell-established practice,” says Linda L. Gaines of Argonne National Laboratory15. \\nA specialist in materials and life-cycle analysis, Gaines says the reasons include \\ntechnical constraints, economic barriers, logistic issues, and regulatory gaps. This \\nmeans that today, much of the li-ion battery materials are incinerated with low re-\\ncovery rates.  There is a high degree of confidence, however, that the industry will \\nmature to minimize the impacts these batteries will have on the environment be-\\ncause: \\n \\n• Robust processes aren’t needed for a number of years: the life expectancy \\nof li-ion batteries of 10+ years, plus time used in 2nd life applications. \\n• The value of the metals is increasing: this is in part due to mining/extraction \\nbecoming more restricted (by 2030, cobalt and lithium will be harder to get \\n \\n14 https://www.ul.com/services/second-life-electric-vehicle-battery-repurposing-facility-certification \\n15 https://cen.acs.org/materials/energy-storage/time-serious-recycling-lithium/97/i28 \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     13 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nfrom mining, and recycled materials will become the primary means to obtain \\nthose materials); but also because the demand for li-ion batteries is growing; \\nso while the cost of extraction today is less costly than recycling, that state-\\nment will not be true in the near future. \\n• The EV market is driving investment and research: with the growth of EVs, \\nand realization of the potential end of life impact, technologies and infrastruc-\\nture are emerging to maximize recycled materials and minimize cost. \\n• Regulations will further drive the recycling maturity: “The U.S. Environmental \\nProtection Agency has created alternative regulatory controls for recycling \\ncertain materials, like lead-acid batteries, to encourage the collection and re-\\ncycling of hazardous waste. A similar designation for lithium-ion batteries \\ncould reduce liability concerns and make the economics of recycling more de-\\nsirable.”16  Other countries such as South Korea and Canada also manage re-\\ncycled materials with alternative programs in ways that reduce the regulatory \\nburden and promote recycling.   \\n• New recycling technologies are emerging: Alternative approaches to incin-\\neration are proving to recover more of the battery materials which result in \\nmore recovered value at the end of the process. \\n \\nThere are two main approaches to li-ion recycling discussed today – Pyrometallur-\\ngical and Hydrometallurgical. Table 2 summarizes these approaches at a high \\nlevel.  \\n \\nPyrometallurgical methods collecting heavy metals in the ash, are most used today. \\nBut companies such as li-cycle are making significant advancements for the indus-\\ntry. Their hydrometallurgical process, for example, claims to support all chemistries \\nand formats of li-ion batteries, recovers 95%+ of the constituent materials found in \\nlithium-ion batteries, and avoids landfilled waste during the process. Through hy-\\ndrometallurgical process, the cathode and anode materials can be processed into \\nbattery grade end-products for reuse in lithium-ion battery production or other ap-\\nplications in the broader economy.17 \\n \\n \\n \\n \\n16 https://www.nrel.gov/news/program/2021/pathways-to-achieve-new-circular-vision-for-lithium-ion-bat-\\nteries.html \\n17 https://li-cycle.com/technology/ \\nComparison \\nPyrometallurgical \\nHydrometallurgical \\nApproach \\nA process that uses the application \\nof heat (incineration, smelting) for \\nextraction and purification of metals.  \\nA process that uses water as a \\nsolvent to extract and recover \\nvaluable elements from complex \\nmixes of compounds. \\nAdvantages \\nSimple and efficient process for re-\\ncovering cobalt and nickel \\nHigh recovery rate of metals \\nHigh purity output \\nLow energy consumption \\nDisadvantages \\nSmall percentage of battery is re-\\ncovered \\nLithium and manganese are not re-\\ncovered \\nMore waste gas and cost of waste \\ngas treatment \\nGreater amount of wastewater \\nLonger, more complex process \\n \\nTable 2 \\nTwo methods of \\nrecycling \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     14 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nA successful recycling system depends on more than just the right technology \\nthough. The recycling process must consider transportation costs, locations of re-\\ncycling facilities, pre-processing costs to disassemble and discharge the batteries, \\nthe efficiency and cost of the recycling process, the environmental impact of the \\nprocesses, as well as the battery chemistry. The diagram in Figure 6 illustrates \\nthese considerations. \\nMinerals\\nWater\\nEnergy\\nLabor\\nCapital\\nRevenue\\nWaste\\nEmissions\\nSpent li-ion batteries\\nRecovered materials\\nCollection\\nTransportation\\nDismantling\\nand discharge\\nPyrometallurgical \\nrecycling\\nHydrometallurgical\\nrecycling\\n \\nVendors like Schneider Electric are committed to working with recycling proces-\\nsors/logistics firms as well as the recycling companies to ensure a circular process \\ngoing forward. While there are certainly some complexities, the processes and eco-\\nnomics are beginning to align. The end goal is a cost effective, simple, timely, and \\nsustainable process. A vendor committed to end of life management of their cus-\\ntomers’ systems should be fully transparent and be able to show their documented \\nprocesses and partnerships and provide proof of validation/certificate that their bat-\\nteries are being recycled in the way described (i.e. long-term contracts). They \\nshould also be committed to evolving their processes as technology improves.  \\n \\n \\nKEY TAKEAWAY – While the recycling infrastructure is not mature today, given \\nthe longevity of li-ion batteries, by the time new batteries reach end of life, we \\nbelieve a fully functioning recycling system will exist for a sustainable end of life \\nat minimal to no cost.  \\n \\n \\n \\nAs interest and demand continue to grow for li-ion batteries over traditional VRLA \\nbatteries for UPS applications, concerns are arising about the circularity of these \\nsystems including questions about the carbon impact of producing these batteries, \\ntheir operational impact, and their limited recyclability. In this paper, we discussed \\nthe impacts over each phase of the li-ion life cycle. Although there are certain as-\\npects with higher impact than VRLA batteries, overall, li-ion has a lower environ-\\nmental impact over the life cycle, and we anticipate this to improve further in the \\nfuture. The key take-aways are as follows: \\n \\n• Raw material extraction – The smaller mass of material and significant de-\\ncrease in toxicity of li-ion result in lower environmental impact overall in the \\nmaterial sourcing phase of the life cycle. \\n• Manufacturing process – For typical installations, when replacement batteries \\nover the lifetime are considered, the environmental impact of manufacturing \\nConclusion \\nFigure 6 \\nFactors in recycling li-ion \\nbatteries \\n \\nSource: Recycling strate-\\ngies for End-of-Life Li-ion \\nBatteries from Heavy \\nElectric Vehicles \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     15 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\nfor li-ion systems is less than VRLA systems, even with the added equipment \\noverhead for li-ion. \\n• Distribution & transportation – Although the complexity of distribution and \\ntransportation is increased, the lighter weight of li-ion over VRLA enables the \\nenvironmental impact to be reduced. \\n• Installation & handling – Although a minor impact overall, the lighter weight \\nand need for less or no li-ion battery replacements results in decreased envi-\\nronmental impact during installation and handling compared to VRLA. \\n• Energy consumption and carbon emissions – Roughly half the energy is \\nneeded to keep li-ion batteries charged compared to VRLA, resulting in a de-\\ncrease in CO2e emissions during the lifetime operation of the batteries.  \\n• Life span – The longer lifespan of li-ion enables improved operational effi-\\nciency and overall lower sustainability impact. The more replacements of \\nVRLA needed over the lifetime, the better li-ion performs. \\n• 2nd life applications – UPS li-ion batteries have the opportunity for a 2nd life so \\nit can continue to provide value beyond its first designed purpose. Using UPS \\nvendors that are committed to this ensures batteries made today will reach \\ntrue end of life when the economics of recycling are cost effective. \\n• Recycling – While the recycling infrastructure is not mature today, given the \\nlongevity of li-ion batteries, by the time new batteries reach end of life, we be-\\nlieve a fully functioning recycling system will exist for a sustainable end of life \\nat minimal to no cost.  \\n \\nVendors that are committed to circularity for their customers will transparently share \\ninformation at the detailed level and commit resources and work with leading edge \\npartners to ensure responsible, ethical, economical, and sustainable solutions are \\ndeployed. \\n \\n \\n \\n \\n \\nAbout the authors \\nRay Lizotte is a Senior Sustainability Engineer and Senior Distinguished Engineer (Edison Level II) \\nat Schneider Electric. He directs efforts to design and develop product offers that meet global \\nregulations such as European ROHS and REACH directives. He specializes in making products \\nmore sustainable through less toxic and greener materials and application of circular end-of-life \\napproaches. For the past few years, he has been working with the company’s procurement organi-\\nzation to responsibly source conflict minerals including cobalt. He has been involved in sustaina-\\nble product design for the past 30 years. Ray studied environmental engineering at MIT where he \\ngraduated with a BS in 1985. \\nWendy Torell is a Senior Research Analyst at Schneider Electric’s Data Center Research Center. \\nIn this role, she researches best practices in data center design and operation, publishes white \\npapers & articles, and develops TradeOff Tools to help clients optimize the availability, efficiency, \\nand cost of their data center environments. She also consults with clients on availability science \\napproaches and design practices to help them meet their data center performance objectives. \\nShe received her bachelor’s degree in Mechanical Engineering from Union College in  \\nSchenectady, NY and her MBA from University of Rhode Island. Wendy is an ASQ Certified Relia-\\nbility Engineer.  \\n \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric – Energy Management Research Center  White Paper 71   Ver 1     16 \\nUnderstanding the Total Sustainability Impact of Li-ion UPS Batteries  \\n \\n \\nWhite Paper 229, Battery Technology for Data Centers: VRLA vs. Li-ion  \\nWhite Paper 229 \\n \\nFAQs for Using Lithium-ion Batteries with a UPS   \\nWhite Paper 231 \\n \\nConsiderations for Selecting a Lithium-ion Battery System for UPSs and Energy \\nStorage Systems   \\nWhite Paper 284 \\n \\nWhy Data Centers Must Prioritize Environmental Sustainability: Four Key \\nDriverspapers  \\nWhite Paper 64 \\n \\nQuantitative Life Cycle assessment (LCA) of VRLA vs. Li-ion UPS Batteries  \\nWhite Paper 91 \\n \\n \\n \\n \\n \\n \\nLi-Ion vs. VRLA UPS Battery TCO Comparison Calculator  \\nTradeOff Tool 19 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNote: Internet links can become obsolete over time. The referenced links were available at the time this  \\npaper was written but may no longer be available now. \\n \\n \\n \\n \\n \\nContact us \\nFor feedback and comments about the content of this white paper: \\nSchneider Electric Energy Management Research Center \\ndcsc@schneider-electric.com \\nIf you are a customer and have questions specific to your data center project: \\nContact your Schneider Electric representative at \\nwww.apc.com/support/contact/index.cfm \\n \\nBrowse all  \\nwhite papers  \\nwhitepapers.apc.com  \\ntools.apc.com  \\nBrowse all  \\nTradeOff Tools™ \\nResources \\n© 2021 Schneider Electric. All rights reserved. \\n\", 'Adapt or Fail: Immersion Cooling for \\nHigh-Density, Sustainable Computing\\nSpecial Report \\n2024\\nContent\\nIntroduction\\n04\\n02\\n06\\n08\\n12\\n14\\n17\\n20\\n21\\nKey Forces\\nPart 1 -  A Critical Imperative for Data Center \\nPart 2 - Top 5 Design and Deployment Considerations\\nTop 5 Immersion Cooling Myths\\nHow to Become Ready for Immersion? \\nPart 4 - The Immersion Cooling Journey  \\nInnovation in Infrastructure\\nPart 3 - Immersion Cooling Readiness Checklist\\nFinal Thoughts\\nSummary\\nThis paper analyzes the transformative benefits of im-\\nmersion cooling for data centers. Leaders stand to gain \\nsignificant cost reductions, enhanced scalability, and \\nboosted sustainability through its implementation.\\nThe relentless surge of AI spending, forecast to exceed $300 billion by 2026, is fuelling an unprec-\\nedented data center transformation.  Yet, maximizing power and capacity within limited space  \\nremains a critical bottleneck. Traditional air-cooling systems struggle to keep pace with escalating \\nheat densities and scalability demands. The choice is stark: Adapt or Fail.\\nImmersion cooling emerges as the vital adaptation. By submerging IT equipment in non- \\nconductive liquid, it radically improves thermal management. This translates to dramatic cost \\nreductions – organizations can achieve OPEX savings surpassing 90% while lowering CAPEX. \\nIncreased computational density unlocks new levels of performance.  Two-phase immersion, with its \\nsuperior heat transfer capabilities, is especially potent for high-performance computing.\\nCool Gray\\nBlack\\nSPECIAL REPORT - HYPERTEC\\nPAGE 3\\nIntroduction\\nThe urgency of maximizing efficiency, minimizing foot-\\nprint with immersion cooling solutions\\nOur critical infrastructure is in a state of rapid evolution. Data centers, those pillars of the digital age, are \\npushed to their limits by the unyielding demands of Artificial Intelligence, Automation, and Machine \\nLearning.  Legacy cooling solutions gasp to keep up with escalating power densities and thermal bottle-\\nnecks. Constraints on power, heat dissipation, and adaptability threaten the very foundations of cloud \\ncomputing and big data. The imperative for efficiency is clear - the time for radical innovation is now.\\nIn this crucible of necessity, immersion \\ncooling shines as a beacon of transformative \\npotential.  Recent studies expose the stark \\nreality: servers and cooling systems devour \\nthe majority of data center energy (Figure 1). \\nA single large-scale data center, housing tens \\nKey Forces\\nLet’s examine the key forces driving this transformation\\n\\t\\x83 Escalating Power Demands with Environmental Consequences:  \\nData centers and networks already account for 1% of global energy-related GHG emissions (IEA), and projections \\nindicate an accelerating growth in their energy consumption.\\n\\t\\x83 HPC, AI, and Big Data Demand a New Approach:  \\nHigh-performance computing (HPC) is vital for research and complex analysis, but its heat output overwhelms \\ntraditional data center cooling. Immersion cooling offers superior heat management for densely packed HPC \\nsystems, unlocking greater computational power. As data-intensive applications proliferate, the need for both \\nefficient and sustainable solutions positions immersion cooling as a game-changer.\\n\\t\\x83 Disruption Extends Beyond HPC:  \\nMachine learning, financial services, healthcare, CAD modeling, and rendering all require  \\nincreased density, reliability, and sustainability – driving them to explore immersion cooling.\\nof thousands of IT devices, can consume the \\nequivalent power of 80,000 U.S. households \\n(U.S. DOE 2020). This glaring fact mandates a \\nrapid evolution in data center cooling, main-\\ntenance, and optimization.\\nImmersion cooling, while not a new concept, \\nis experiencing a technological renaissance \\nthat’s fuelling its rapid adoption across the \\ndata center industry.  Intensifying invest-\\nments in high-density technologies, the insa-\\ntiable demands of HPC, and the explosion of \\nAI-driven workloads are pushing data centers \\nto their limits.  \\nOperators must embrace innovative cooling \\nsolutions to ensure reliability and efficiency. \\nCurrently, data centers and data transmission \\nnetworks consume 2-3% of global energy and \\ncontribute 1% of energy-related GHG emis-\\nsions (IEA). Immersion cooling eliminates \\nwater waste, transforms heat dissipation, \\nand offers a compelling path to reduce both \\npower consumption and carbon impact.  This \\nis why it’s emerging as a critical solution for \\nforward-thinking data center design.\\nMarket research confirms a surge in immersion cooling adoption driven by its superior efficiency. Technavio \\nhighlights its rapid uptake compared to air-based cooling, with liquid-based cooling leading the way. Dell \\nEMC’s in-depth study reveals the economic imperative behind this shift: 50% of operators report a significant \\nimpact of cooling costs on overall operating expenses, and 78% acknowledge at least some impact.\\nThis translates into real-world adoption:  \\n25% already employ liquid cooling in production environments, with 32% utilizing it in some capacity. \\n \\nThe survey demonstrates the versatility of liquid cooling:\\n39% \\t\\nuse it for high-density cabinets or racks\\n20% \\t\\ndeploy it for high-performance computing (HPC)\\n17% \\t\\nleverage it for big data processing\\n15%\\t\\nintegrate full-package systems with primarily liquid-based heat removal\\nWith the rising demand for dense, compute-intensive solutions, the scalability of traditional \\n air-based ecosystems is in question. Immersion cooling pushes the boundaries of efficiency  \\nand density, offering a vital bridge to support next-generation customer workloads.\\n \\nModern immersion solutions are vastly improved, making adoption increasingly accessible.  \\nLet’s explore how immersion cooling revolutionizes data center design.\\nFigure 1.\\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 4\\nImmersion Cooling: A Critical Imperative for Data Center \\nSustainability and Efficiency\\nRising energy costs, environmental concerns, and the ever-increasing need for computational power are \\nsqueezing the data center industry. Efficiency is no longer a luxury - it’s a necessity.  Immersion cooling \\npresents a compelling solution, redefining thermal management and unlocking significant increases \\nin equipment density.  This technology holds the potential to reshape the very principles of data center \\ndesign.\\nMeeting environmentally conscious design goals and addressing poten-\\ntial regulatory pressures.\\nGlobal shifts towards remote work, \\ncoupled with the proliferation of \\ndata-intensive applications, have \\nfundamentally transformed both \\nconnectivity and digital infrastruc-\\nture requirements. This explosion \\nof remote users and workloads \\nstrains legacy systems.  The conver-\\ngence of HPC, cloud computing, \\nand virtualized environments (VDI) \\nfurther complicates the landscape.  \\nTechnology leaders seek versatile \\nplatforms capable of adapting \\nto these dynamic forces, opti-\\nmizing resource deployment for \\nmaximum return on investment \\n(ROI) while maintaining superior \\nuser experiences. \\nKey challenges facing digital infra-\\nstructure include: \\nConstraints on both space and \\ntime-to-market pose a critical chal-\\nlenge for data center leaders.  As \\nDataCenterHawk \\nreports, \\nNorth \\nAmerican primary market vacancy \\nhas plummeted to a record low of \\n4.4%, with regions like Northern \\nVirginia nearing a mere 1%.  \\nThis drastically limited availability \\nforces a fundamental rethinking of \\ndata center design and deployment \\nstrategies. The imperative is clear: \\nmaximizing capacity and efficiency \\nwithin existing footprints. \\nData \\ncenter \\nleaders \\nface \\nthe \\ndaunting \\ntask \\nof \\nmaximizing \\noutput within constrained foot-\\nprints while simultaneously driving \\ndown operational costs (OPEX).  \\nImmersion cooling offers a compel-\\nling solution, enabling significantly \\nhigher density and slashing cool-\\ning-related OPEX by over 95%. \\nMoreover, the streamlined infrastruc-\\nture requirements of immersion \\ncooling translate into a substantial \\n50% reduction in capital expendi-\\ntures (CAPEX) for new builds.\\nSUSTAINABILITY: \\nNew business demands translate \\nto new infrastructure challenges\\nPart 1\\nData centers urgently need to maximize performance while driving  \\nradical improvements in energy efficiency. Immersion cooling presents \\na compelling solution for cost reduction, streamlined management, \\nand a significant efficiency boost.  These advantages align with both  \\nfinancial and operational goals, making immersion cooling a key  \\ntechnology for infrastructure transformation.\\nLegacy infrastructure poses a significant obstacle to technological innovation and agility.  Data center leaders recog-\\nnize that while existing systems may be functional, they often hinder modernization efforts and fail to deliver the \\nvalue modern workloads demand. This realization is driving initiatives to transform and upgrade critical infrastructure. \\nImportantly, modernization doesn\\'t always necessitate a complete \"rip and replace\" approach.  Strategic integration \\nof cutting-edge technologies alongside legacy systems can offer a smoother, less disruptive transition. This paves the \\nway for transformative solutions like immersion cooling, explicitly designed for the demands of today\\'s data centers.  \\nImmersion cooling supports high-efficiency, high-density deployments powering HPC, data-intensive applications, \\nIoT, edge computing, and cloud architectures. \\nInnovative immersion platforms often feature modular, integrated \\ndesigns resulting from industry partnerships. These modern solutions \\naddress key challenges, including:\\nMitigating rising energy costs and the insatiable power demands of \\nadvanced CPUs and GPUs.\\nPOWER CONCERNS:  \\nThe urgent need for less power-hungry cooling systems.\\nEFFICIENCY IMPERATIVE: \\nLet’s dive into the groundbreaking advancements in  \\nimmersion cooling and see how they directly address the \\nchallenges we’ve outlined.\\nIn a dedicated subsection, we’ll debunk a few outdated \\nmyths surrounding immersion cooling.\\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 6\\nPAGE 7\\nPart 2\\nImmersion Cooling: Top Five Design and Deployment \\nConsiderations\\nLegacy Infrastructure \\nand Integration\\nAirflow at  \\nBreaking Point\\nThe urgency to embrace immersion cooling is undeniable. Regulations, escalating power demands, and \\nAI’s computational thirst necessitate a break from traditional cooling methods. Immersion cooling provides \\na compelling solution, but successful deployment demands strategic planning.  While its flexibility allows \\ngradual integration alongside existing air-cooled systems, the focus for maximum impact should be on its \\noptimization of energy efficiency and data center performance.\\nTechnologies like solid-state drives, converged systems, HPC, and supercomputing underscore the need for \\ninnovative cooling approaches. These technologies often have specific operating temperature thresholds. \\nSub-optimal temperatures within traditional data centers lead to wasted energy without commensurate \\nperformance gains.\\nPlanning for a Seamless Transition.\\nAssessing your existing infrastructure is crucial \\nwhen considering immersion cooling. Older data \\ncenters often necessitate wider overhauls, while \\neven modern systems can reach their limits when \\nfaced with the extreme demands of HPC, AI, and \\nML. Strategic integration of legacy systems with \\nimmersion cooling can ensure a smoother transi-\\ntion for critical workloads, minimizing disruption. \\nThe rising power density of cutting-edge chips \\nputs a strain on traditional air-cooled approach-\\nes. According to a recent study by the Uptime \\nInstitute, the average server rack density in 2023 \\nis 8kW, with 25% of new deployments exceed-\\ning 20kW. This highlights the growing need for \\nhigh-density solutions like immersion cooling, \\nespecially since 90% of data centers still primarily \\nrely on air-cooling.\\nImmersion Cooling as the Vital Solution. \\nWhile airflow management has seen contin-\\nuous improvement, the relentless demands of \\nAI, HPC, and other data-intensive workloads are \\noverwhelming traditional air-cooling systems.  In \\nthe past, the focus was on managing increasing \\nserver power densities.  As cabinet densities \\nexceeded 5-10kW, solutions like rear-door heat \\nexchangers and in-row cooling systems became \\nmore common.\\nHowever, the widespread adoption of acceler-\\nators like GPUs, ASICs, and FPGAs presents a \\nnew challenge: extreme computational density. \\nAir-cooling often falters in these scenarios, \\nleading to underutilization or inefficient space \\nusage.  \\n01\\n02\\nA phased approach, with parallel deployments of \\nimmersion alongside existing systems, helps manage \\ncomplexity and risk. Advanced management tools \\nand system integrators can streamline this process, \\nensuring \\nseamless \\nintegration \\nacross \\ndifferent \\ncooling technologies. Importantly, a well-planned \\ntransition offers IT staff, leaders, and stakeholders \\nvaluable insights for future capacity optimization. \\nWhile a retrofit might be viable, exploring immer-\\nsion-born designs where possible can significantly \\nsimplify deployment and long-term management.\\nWhile retrofitting existing servers for immersion \\ncooling might seem like a cost-effective option, \\nit often introduces significant complexity to your \\ndata center.  Retrofits require specialized exper-\\ntise, either from the server vendor or a system inte-\\ngrator, to remove fans and adapt components not \\noriginally intended for liquid submersion.  This can \\nlead to compatibility issues and increased reliance \\non external vendors.  In contrast, immersion-born \\nservers are designed from the ground up for immer-\\nsion environments.  They offer simplified deploy-\\nment, streamlined maintenance, and often superior \\nperformance due to their optimized design.\\nA recent study by Gartner estimates that air-cooled \\ndata centers experience a significant inefficiency of up \\nto 30% at rack densities exceeding 15kW. This translates \\nto wasted energy and limited performance potential.\\nImmersion cooling offers a compelling solution. It can \\nsupport 10x or greater server density increases while \\nconsistently maintaining optimal operating temper-\\natures. Advanced modular immersion systems are \\ncapable of handling configurations of up to 144 nodes, \\n288 CPUs, and 98kW per 48U tank – a level of capacity \\nunattainable with conventional air-based approaches.\\nA recent study by Gartner estimates \\nthat air-cooled data centers experience \\na significant inefficiency of up to 30% at \\nrack densities exceeding 15kW. \\nFEATURES\\nRETROFIT\\nADVANCED RETROFIT\\nIMMERSION-BORN DESIGN\\nAvailability\\nMultiple Vendors\\nSome vendors + Hypertec\\nHypertec Exclusive\\nBase\\nOTS Air-Cooled Servers\\nCTS Air-Coooled Servers\\nImmersion-Born\\nVanity-Free Design\\nEase of Installation and Serviceability\\nLow\\nMedium\\nHigh\\nPerformance Availability (OC or Turbo Lock)\\nHypertec Exclusive\\nSupports both AC & OCp Power\\nSustainable Composite Material\\nCustom Immersioon Cooling Heatsink\\nIntel Xeon and AMD EPYC 4th Gen Compatibility\\nIntel Xeon W (SP/ER) 5th Gen Ready\\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 8\\nPart 2 (con’t)\\n03\\nThe Performance and \\nEfficiency Imperative\\nPurpose-Built Immersion Designs \\nMaximizing efficiency is a universal goal for \\ndata centers, but optimizing for performance is \\nequally critical.  The early days of liquid cooling \\noften involved direct-to-chip solutions that \\ndemonstrated the superior heat transfer capa-\\nbilities of liquids compared to air. However, \\nmodern modular immersion systems elevate \\nthis concept to a new level of sophistication.\\nPurpose-built designs integrate all essential \\ncomponents – compute, network, and storage \\nresources – within a fully enclosed immersion \\nenvironment. This approach offers significant \\nefficiency and performance gains. Since the \\nentire system is optimized for immersion, power \\ndistribution, heat management, and fluid flow \\nare streamlined for maximum thermal efficiency. \\nAdditionally, the compact design of immersion \\ncooling eliminates the need for complex airflow \\nmanagement, translating into reduced energy \\nconsumption and overhead costs.\\nIt’s important to note that not all immersion solu-\\ntions are created equal. Advanced two-phase \\nimmersion systems with engineered, high- \\nperformance fluids can achieve PUEs (Power \\nUsage Effectiveness) as low as 1.03. This means \\nfor every 100 watts of power used by IT equip-\\nment, only an additional 3 watts are required for \\ncooling.  By contrast, traditionally air-cooled data \\ncenters typically have average PUEs of 1.5-1.8.\\n04\\nStrategic  \\nVendor Partnerships\\nBuilding Trust in Immersion Technology\\nThe immersion cooling landscape has evolved signifi-\\ncantly, offering more integrated and streamlined solu-\\ntions. This simplifies deployment but highlights the \\nimportance of careful vendor selection. Despite the \\nease of deploying these new systems, thoroughly eval-\\nuating vendors remains critical.  Choosing partners \\nthat align with your unique requirements ensures a \\nsuccessful long-term implementation.\\nConsider the following: \\na recent survey by AFCOM found that over 60% of data \\ncenter professionals cite vendor compatibility as a key \\nconcern when adopting new technologies.  During \\nyour selection process, scrutinize potential vendors \\njust as you would with any other major technology \\npurchase. Evaluate their designs, ensuring they inte-\\ngrate seamlessly with your existing architecture while \\nsupporting projected growth and business operations. \\nResearch their track record, industry reputation, and \\ncommitment to ongoing support during and after \\nsystem implementation.\\nStrategic vendor partnerships offer tangible benefits. \\nEstablished immersion cooling specialists have the \\nexpertise to guide you through system selection, \\nintegration, and maintenance, de-risking the imple-\\nmentation process.   Additionally, these partnerships \\ncan foster innovation and collaboration. According to \\na McKinsey study, companies that collaborate exten-\\nsively with external partners are more likely to report \\nabove-average revenue growth.\\n05\\nUnderstanding  \\nTrue Cost\\nThe Compelling Economics of Immersion Cooling\\nEscalating energy costs are a major concern for data center operators. This makes it critical to assess infrastructure \\ninvestments from a holistic cost perspective. While traditional air-cooling systems may appear more affordable \\nupfront, this perspective shifts as rack densities increase. Air-cooled solutions quickly reach their economic limita-\\ntions, leading to sprawling data center expansions, inefficient resource utilization, or underpowered hardware.\\nA recent study by Schneider Electric offers valuable insights into data center design costs. While the initial costs \\nof air-cooled and immersion-cooled solutions may be comparable at standard densities (10kW/rack), immersion \\ncooling demonstrates its true value at higher densities. Here’s the breakdown:\\n \\n2x Compaction (20kW/rack):  \\t Immersion cooling offers a 10% reduction in initial costs (CapEx) \\n\\t\\n\\t\\n\\t\\n\\t\\ncompared to a traditional air-cooled data center.\\n4x Compaction (40kW/rack):  \\t Savings rise to an impressive 14%.\\n \\nThe latest AFCOM State of the Data Center report (2023) highlights increasing operational expenditures (OpEx) \\nand capital expenditures (CapEx) as critical challenges. A majority of respondents (57%) reported rising OpEx in \\n2022, primarily driven by energy costs, equipment service, and personnel expenses. Similarly, 53% faced increased \\nCapEx due to supply chain issues, investment in existing facilities, IT refreshes, and new facility construction.\\nImmersion cooling delivers significant advantages in reducing both OpEx and CapEx.  Single-phase immersion \\nsystems can reduce cooling-related OpEx by up to 95%. With PUE values as low as 1.03, many operators see a \\nreturn on investment (ROI) in less than a year, based on energy savings alone.  Moreover, the streamlined design \\nof single-phase systems enables rapid, flexible deployment.  Since there’s no need for raised floors, cold aisles, and \\ncomplex retrofits, CapEx is reduced by up to 50% for new builds.\\nIt’s important to look beyond immediate costs. Immersion cooling’s optimized environment extends hardware \\nlifespan by up to 30%. The absence of moving parts, dust, and vibration minimizes wear and tear, leading to less \\nfrequent IT refresh cycles and further cost optimization.\\n$8\\n$7\\n$6\\n$5\\n$4\\n$3\\n$2\\n$1\\n$_\\nAir cooled \\n10kW/rack\\nLiquid cooled \\n10kW/rack\\nLiquid cooled \\n20kW/rack\\nLiquid cooled \\n40kW/rack\\nSource: Schneider Electric\\n  Core & Shell\\n  Racks & Rack PDUs\\n  Auxiliary\\n  Power\\n  Cooling\\nCost per watt\\n$ 7.02\\n$ 6.98\\n$ 6.33\\n$ 6.02\\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 10\\nPAGE 11\\nTOP FIVE  \\nIMMERSION COOLING MYTHS\\nMyth #1 \\nImmersion Cooling Requires Complex Water \\nTreatment and Harms the Environment\\nFACT: Modern immersion solutions integrate seamlessly with existing data \\ncenter cooling loops, eliminating the need for specialized water treatment. \\nAdditionally, the fluids used are fully biodegradable and non-toxic, ensuring \\nenvironmentally responsible operation.\\n1\\nMyth #2 \\nImmersion Cooling Fluids are Highly Flammable\\nFACT: Presently, many immersion liquids, including those used in closed-\\nloop systems, contain oils. If these oils are heated to the point of evapora-\\ntion or condensation, they can technically become flammable, as indicated \\nby the labels on these liquids. Enclosed loops and single-phase immersion \\nsolutions offer the highest level of safety regarding combustibility. Solutions \\nlike Hypertec utilize single-phase immersion cooling, where servers are \\nsubmerged in a thermally conductive dielectric liquid or coolant—a signifi-\\ncantly better heat conductor than air, water, or oil. Importantly, the coolant \\nremains in a stable state, and the liquid utilized in leading immersion cooling \\nsolutions does not pose a risk of combustion.\\n2\\nMyth #3 \\nImmersion Cooling Systems are  \\nComplex to Maintain\\nFACT: Next-generation immersion cooling solutions are designed for \\nsimplicity and streamlined maintenance. Even retrofit scenarios offer \\nimproved serviceability compared to legacy immersion approaches. Immer-\\nsion-born systems take this a step further, offering exceptional ease of access \\nto all critical components.\\n3\\nMyth #4 \\nImmersion Cooling is More Complex  \\nthan Traditional Air Cooling\\nFACT:  While air cooling may be more familiar currently, modern immer-\\nsion cooling systems are engineered for simplicity. They integrate seamlessly \\nwith standard data center infrastructure, including power, connectivity, and \\noften existing water loops. This compatibility, coupled with the superior effi-\\nciency of immersion cooling, can lead to a significantly lower PUE compared \\nto air-based systems.\\nWhile immersion cooling may be a newer concept within the data center \\nindustry, it’s crucial to recognize that liquid cooling is a well-established, \\nreliable technology widely used in various industries.  From nuclear reactors \\nand powerful car engines to large-scale manufacturing like paper mills, \\nliquid cooling has proven essential for managing high thermal loads.  This \\nunderscores the potential for immersion cooling to address the unique \\ndemands of modern data centers, where energy-intensive computing oper-\\nations necessitate innovative and efficient cooling solutions.\\n4\\nMyth #5 \\nImmersion Cooling Poses  \\nan Electrical Hazard\\nFACT:  Modern immersion cooling systems prioritize safety by utilizing \\nspecially engineered dielectric fluids. These fluids are non-conductive, \\nensuring that even in the unlikely event of a leak, critical IT components are \\nprotected. Furthermore, they are designed to leave no residue, minimizing \\nany potential long-term impact on hardware.\\n5\\nSPECIAL REPORT - HYPERTEC\\nPAGE 12\\nPAGE 13\\nSubmersion  \\ncompatibility\\nDirect Immersion Cooling\\nThe First Step in Your Immersion Journey \\nBefore embarking on any immersion cooling \\nproject, it’s crucial to assess which components \\nof your existing infrastructure are suitable for \\nimmersion.\\nWhile there are various immersion cooling approaches, let’s focus on direct immersion cooling, as it offers \\nseveral advantages for data center deployments. \\nDirect Liquid Cooling (Immersion)\\nIn this system, heat-producing IT components are fully submerged in a specialized, non-conductive dielectric \\nfluid.  The fluid absorbs heat directly from the components through convection, providing highly efficient \\ncooling. Heat removal in immersion cooling systems is achieved in a few ways.  Most commonly, the heated \\ncoolant is circulated through a heat exchanger, where it transfers thermal energy to a secondary cooling \\nmedium (often water) and is subsequently recirculated back to the immersion tank.  \\nAlternatively, in some systems, heat passively transfers from the liquid to the sealed enclosure, which then \\ndissipates the heat to the surrounding air through natural convection – a method similar to the cooling of \\npower transformers on utility power lines.\\nWe’ve explored how immersion cooling addresses critical data center challenges and \\ndispelled common myths holding back its wider adoption. Now, it’s time to determine if \\nimmersion cooling is the right solution for your specific needs.  \\nLet’s dive into a practical readiness checklist to help you evaluate your data center’s potential \\nfor successful immersion deployment.\\nSingle Phase \\nImmersion  \\nCooling\\nSingle-Phase Immersion Cooling\\nSingle-phase immersion cooling offers a compelling approach for data centers seeking efficient and reliable thermal \\nmanagement. In this method, IT components are fully submerged in a specially formulated dielectric fluid. This fluid \\ndirectly absorbs heat from the servers, similar to two-phase immersion. Unlike two-phase systems, the single-phase \\ncoolant doesn’t boil off or undergo a phase change. It remains in a liquid state throughout the cooling process. The \\nheated dielectric fluid is then circulated through a heat exchanger within a Cooling Distribution Unit (CDU). This heat \\nexchanger transfers the thermal energy to a separate cooling medium, typically a closed-loop water system. The \\ncooled dielectric fluid is then pumped back to the immersion tank, completing the cooling cycle.\\nImmersion Cooling Readiness Checklist\\nPart 3\\nWithin the category of immersion cooling,  \\nthere are two primary types to consider: \\nSingle phase and Two Phase immersion cooling\\nTwo-Phase  \\nImmersion \\nCooling\\nTwo-Phase Immersion Cooling: \\nIn a two-phase immersion cooled system, electronic components are submerged into a dielectric heat-transfer \\nliquid bath, a much better heat conductor than air, water, or oil. Selecting the right immersion cooling solution is \\na decision driven by a data center’s specific needs.  To optimize its technology roadmap, Hypertec conducted a \\ncomprehensive assessment of both single-phase and two-phase systems. This analysis revealed a strong case for \\nprioritizing single-phase immersion due to its balance of efficiency, reliability, and adaptability for a wide range of \\ndata center use cases.\\nAIR\\nIMMERSION\\nCRITERIA\\n✓\\n✓\\nFabric (Copper single mode fiber)\\n✓\\n✓\\nNetwork Switching\\n✓\\n✓\\nSolid State Storage and NVMe Drives\\n✓\\n✓\\nSpinning Media\\n✓\\nSpinning media if \\nsealed (HDD)\\nPower Supplies\\n✓\\n✓\\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 14\\nPAGE 15\\nThe following table summarizes key distinctions between the technologies, providing \\ninsights into the factors considered in this evaluation:\\nSummary\\nHOW TO BECOME  \\nREADY FOR IMMERSION? \\nUnderstanding your specific workloads and use cases is crucial for determining the potential benefits \\nof immersion cooling.   To reap the full advantages of this technology, business needs must align with \\nits technical capabilities. Immersion cooling excels in a variety of scenarios; however, the optimal server \\nconfigurations, tank designs, and overall system architecture can vary.\\nFor data centers focused on highly demanding workloads, immersion cooling offers significant \\nadvantages.  Here are use cases where it often provides substantial value:\\nCRYPTOCURRENCY \\nMINING\\nEfficient cooling supports \\nthe constant computational \\ndemands of crypto mining \\nhardware.\\nAI & ML \\nImmersion cooling effec-\\ntively supports the power \\nand cooling requirements \\nof artificial intelligence (AI) \\nand machine learning (ML) \\ntraining and inference.\\nDATA SCIENCE\\nEnables complex data \\nanalytics and computa-\\ntionally intensive data \\nscience workloads.\\nCRITERIA\\nSINGLE PHASE\\nTWO PHASE\\nPower Usage Effectiveness (PUE)\\n✓\\n✓\\nFluid Loss\\n✓\\nToxicity of Fluid\\n✓\\nBiodegradability of Fluid\\n✓\\nMaterial Compatibility\\n✓\\nCost\\n✓\\nMaintenance\\n✓\\nCLEVER POWER DESIGN\\nEASE OF SERVICE\\nEASE OF USE\\nCHASSIS DESIGN\\nOpen chassis concept \\nfor optimal liquid flow\\nStandard AC or OCP type\\nFixed PSU or 12/54V PDB\\nOpen chassis concept \\nfor optimal liquid flow\\nTool-less PCle Card cage\\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 16\\nPAGE 17\\nTheir capabilities come with a cost: \\na single Google search can power a 100w lightbulb for 11 seconds, while a ChatGPT \\nsession is estimated to be 50-100 times more energy-intensive. This massive increase \\nin computational power generates significant heat that must be efficiently managed. \\nImmersion cooling offers a compelling solution, with its superior efficiency and ability \\nto dissipate the intense thermal output associated with training LLM models and \\nrunning AI applications like ChatGPT.\\nHow to become ready for immersion\\nThe Environmental Impact\\nA critical step towards immersion readiness \\ninvolves careful calculations of your data center\\'s \\nworkload and its environmental impact.  Under-\\nstanding the total heat output (in BTUs) gener-\\nated by your IT hardware is crucial for selecting \\nthe right immersion tank.  Consider how a \\nspecific tank\\'s maximum heat load (in kW) and \\nthe number of supported physical nodes align \\nwith your needs. Additionally, the high densities \\nachievable with immersion cooling are game-\\nchanging.  \\n \\nFor example, you can fit 144 servers within a \\n48U rack at approximately 100kW, significantly \\nincreasing your computational capacity within \\nthe existing footprint.\\nImmersion \\ncooling \\ndelivers \\na \\ncompel-\\nling financial impact, a point we explored \\nfurther in Section 2.  Solutions like Hyper-\\ntec\\'s can slash cooling-related operational \\nexpenditure (OpEx) by up to 95% and boast  \\na certified PUE of 1.03, resulting in a rapid ROI \\noften achieved in less than one year. Addition-\\nally, when combined with immersion-born  \\n \\nservers like the Ciara Trident, you can potentially \\nreduce the total number of servers and storage \\ndevices required, lowering your total cost of \\nownership (TCO) by up to 33%.  This highlights \\nthe significant and multifaceted cost-saving \\npotential of immersion cooling for data centers.\\nLet\\'s put some real numbers behind the poten-\\ntial of immersion cooling. Using an Immer-\\nsion Cooling Savings Calculator, a 1MW data \\ncenter paying $0.10 per kWh could achieve \\nan impressive $500,000 in annual savings.  \\nThis is a direct result of the dramatic reduc-\\ntion in PUE achievable with immersion cooling \\ncompared to traditional air-cooling methods. \\nThe exceptional efficiency of immersion cooling \\nmeans that data centers of all sizes can experi-\\nence significant financial benefits.\\n01\\nThe emergence of ChatGPT and similar  \\nAI models underscores the growing demand  \\nfor powerful, resource-intensive computing\\nThe Facility’s Readiness\\nAn \\narchitectural \\nreview \\nis \\nessential \\nto  \\ndetermine your facility\\'s readiness for immer-\\nsion cooling.  \\nThis \\nwill \\nhelp \\nyou \\ndetermine \\nif \\nyour  \\nfacility can support immersion cooling or if \\nyou\\'ll need to make retrofits. \\nOwning the data center building simplifies \\nthis process. If you work with a colocation pro-\\nvider, collaborate closely to assess the feasibil-\\nity of immersion cooling. \\nTaking all of this into consideration,  \\nlet’s conclude by examining a few immersion  \\ncooling solutions and exploring how  \\nto begin the journey.\\nPartnering with an Expert\\nChoosing the right partner is crucial for a \\nsmooth and successful transition to immer-\\nsion cooling.  A leading partner eliminates \\nthe need for in-house expertise by offering a \\ncomprehensive white-glove service.   \\nThis includes site assessments, installation \\nplanning, tank design, factory assembly, \\nsoftware setup, onsite installation, and even \\ncabling and labeling.  \\nPartnering with an expert ensures a seamless \\ndeployment and alleviates the complexities \\nassociated with adopting immersion cooling.\\n02\\n03\\nWHITE PAPER\\nPAGE 18\\nPAGE 19\\nBefore you embrace immersion cooling, it’s crucial to comprehend your specific use cases and identify where \\nimmersion cooling can deliver significant benefits. Keep in mind that depending on the component and the \\nsolution, liquid cooling may only dissipate a portion of the heat. Consequently, not all equipment will or should \\nbe cooled through immersion methods. As previously mentioned, the fascinating aspect is that immersion \\ncooling has the potential to create a comprehensive solution where every component in your design receives \\nadequate cooling. Therefore, designing around your use case is of utmost importance.\\nFor those organizations looking to leverage immersion-born technology, it’s vital to look at immersion servers \\ncurrently leading the market. For example, The Ciara Trident line of servers was the first to be developed specif-\\nically for single-phase immersion cooling. These servers offer ultra-high-density compute performance, up to \\n288 CPUs per 48U (144 nodes/18,432 cores). They are engineered to accommodate next-generation CPU, GPU, \\nand FPGA cards, ensuring your data center is ready for a wide range of demanding workloads.\\nThe Ciara Trident server line exemplifies how immersion-born design goes hand-in-hand with sustainability. \\nOptimized for single-phase immersion cooling, these servers significantly reduce energy and water consump-\\ntion while maximizing heat capture efficiency. This results in a substantial reduction in carbon emissions, up to \\n40% compared to air-cooled setups.  Moreover, Hypertec’s commitment to using recycled materials in future \\nCiara Trident systems underscores their dedication to minimizing environmental impact, offering a compelling \\nchoice for data centers seeking powerful and eco-conscious solutions.\\nImmersion cooling isn’t a one-size-fits-all solution. \\nThe right partner can help you design a hybrid cooling ecosystem that leverages both immersion and air cooling \\nfor optimal efficiency.  This adaptability makes immersion cooling a powerful tool for a wide range of applications, \\nincluding HPC, rendering, edge computing analytics, AI, machine learning, and deep learning.  By working with \\nan expert, you can harness the benefits of immersion cooling to propel various business-critical workloads.\\nThe transformative potential of immersion cooling is reflected in the experiences of data center leaders.  Patrick \\nQuirk, CTO at Nautilus Data Technologies, highlights the strategic value of partnering with Hypertec:\\nThe Immersion Cooling Journey:  \\nInnovation in Infrastructure\\nPart 4\\nPartnering with Hypertec was a logical next step in offering our customers \\neven more sustainable, cost-saving energy reductions from immersion \\ncooling. The Hypertec immersion cooling solutions open the door for Nautilus \\ncustomers to push the limits of increased Kilowatts for unrivaled  \\npower without increased electricity consumption.\\nPatrick Quirk \\nCTO at Nautilus Data Technologies\\nAs data center technologies continue to evolve, there\\'s no doubt that data and the systems that process it will \\nbecome even more central to organizations across industries.  To successfully handle these increasing demands, \\ndata center leaders must proactively rethink their infrastructure strategies. Embracing innovation is essential to \\ndeliver the necessary capacity, density, and efficiency for a data-driven future.\\nThroughout this whitepaper, we\\'ve explored the compelling advantages of single-phase immersion cooling in \\naddressing critical data center challenges.  Let\\'s recap some of the essential points we\\'ve discussed:\\nEnergy Efficiency: \\nWe examined the alarming growth of data center energy consumption and how immersion cooling \\ncan dramatically reduce cooling-related costs and improve PUE.\\nDemanding Workloads: \\nImmersion cooling offers a solution for the increasing power and heat demands of  HPC, AI, machine \\nlearning, and deep learning applications.\\nRetrofitting: \\nWe discussed strategies for integrating immersion cooling with existing infrastructure and the consid-\\nerations for successful retrofits.\\nTwo-Phase vs. Single-Phase: \\nWe compared these technologies, highlighting the advantages of single-phase immersion in terms of \\nsimplicity, reliability, and scalability.\\nImmersion Readiness: \\nWe outlined steps for assessing your data center\\'s readiness and the importance of partnering with an \\nexpert for a smooth transition.\\nContinuous Innovation: \\nWe highlighted how immersion cooling drives innovation in data center infrastructure, paving the \\nway for greater efficiency and performance.\\nFINAL THOUGHTS \\nTo embark on your immersion cooling journey, start by asking critical questions about \\nyour current data center infrastructure and business goals.  If you prioritize high-den-\\nsity, data-driven workloads, immersion cooling offers significant technological and \\nbusiness advantages.  Its superior efficiency, density, and scalability unlock the full \\npotential of your IT investments. Furthermore,  the ability to access and process data \\nfaster empowers you to make more informed, timely business decisions. \\nSPECIAL REPORT - HYPERTEC\\nSPECIAL REPORT - HYPERTEC\\nPAGE 20\\nPAGE 21\\nSpecial Report \\nImmersion Cooling \\n2024\\nhypertec.com/immersion-cooling/\\n+ 1 514.745.4540\\nGet In Touch\\nH312_IC_SpecialReport_ENG (2024-04)\\n9300, Route Trans Canada\\nSt-Laurent, Québec, H4S 1K5 \\nCanada\\n', 'Immersion Cooling for Hyperscalers, enabling \\nbetter sustainability at high densities.\\nIntroduction\\nThe Wyoming Hyperscale \\nand Submer Partnership \\nCase Study\\nThis Use Case details how Immersion \\nCooling can be used to power Hyper\\xad\\nscalers and how the technology can \\nhelp future and existing facilities to \\nhave a positive impact on the environ\\xad\\nment, helping them to reduce their \\ncarbon footprint, energy consump\\xad\\ntion, and costs while also giving back \\nto the community. \\n  Immersion Cooling  |  Hyperscale \\nFounded in 2020, Wyoming Hyperscale is combining re\\xad\\nsources to sustainably satisfy the demand for hyperscale \\ndata center capacity while implementing best-in-class \\nsolutions to directly address global climate change and \\neliminate  the waste inherent in conventional datacenter \\ndesigns. The company has chosen Submer’s SmartPod \\nXL+ to help it achieve its goals.  \\nCollaboration between:\\nSustainability\\nEfficiency\\nCost-saving\\nInnovation\\nRating Index: \\nPage 2\\nMicroPod + Campus Genius \\nSoftware\\nServers heat will power an \\nindoor farm providing food to \\nlocal community\\nWyoming, USA \\nHyperscale , Cloud, HPC, \\nAI / ML\\nThe Case  \\nWhat is Immersion Cooling?\\nThe challenge \\nType of Solution\\nHighlight\\nAvailability\\nIndustry\\nWhile efficiency and sustainability are becoming increasingly im\\xad\\nportant for the industry, the Hyperscale industry is arguably lag\\xad\\nging behind. Wyoming Hyperscale is setting the standard for fu\\xad\\nture facilities by utilizing technologies that ensure it minimizes its \\nimpact on the environment. Using Submer’s Immersion Cooling \\nsolutions has allowed Wyoming Hyperscale to:\\n•\\t\\nProvide high-density compute while having a positive impact \\non the environment \\n•\\t\\nFuture-proof the project and ensure it meets the most strict \\nrules and regulations \\n•\\t\\nAchieve huge savings on power consumption \\n•\\t\\nConsume zero water in cooling operations \\n•\\t\\nReuse surplus heat, specifically to benefit the local commu\\xad\\nnity \\n•\\t\\nLower Scope 3 carbon emissions by selection of greener \\nbuilding materials  and attach a carbon-negative enterprise to \\nthe data center \\nAlso known as liquid submersion cooling, it is the practice of sub\\xad\\nmerging computer components (or full servers) in a thermally, but \\nnot electrically, conductive liquid (dielectric coolant).  \\nWyoming Hyperscale has a company philosophy \\nof enabling and building zero-waste datacenters \\nthat are able to handle the density and compute \\npower to meet the needs of today and in the fu\\xad\\nture, while maintaining the family’s multi-genera\\xad\\ntional stewardship traditions.  \\n \\nThe number of datacenters continues to in\\xad\\ncrease, with datacenter hotspots now part of \\nmany landscapes throughout the USA and the \\nrest of the world. At the same time, the number \\nof rules and regulations that Datacenters (new \\nand old) must adhere to is rising.  \\n \\nArguably the main driver for this is centered \\naround environmental and sustainability con\\xad\\ncerns. The green datacenter market is set to \\ngrow from $50 billion to $143 billion by 2026.  \\n \\n As both companies and governments pursue \\ntheir own sustainability goals Datacenters and by \\nand large the tech companies that use them will \\nbecome increasingly reliant on technologies that \\nhelp them to become a positive influence on so\\xad\\nciety rather than a burden.  \\n \\nConclusion? Wyoming Hyperscale wanted to ex\\xad\\nplore what options were available to help them \\nachieve their goal of offering a high-tech, for\\xad\\nward-thinking facility that would be compliant \\nwith government regulations and safe for the en\\xad\\nvironment and their employees.\\n \\nCase Study\\n  Immersion Cooling  |  Hyperscale\\nCollaboration between:\\nThe solution\\nThe actions\\nWyoming Hyperscale aims to accommodate this need in multiple \\nways: \\n \\nThe use of Submer’s SmartPod XL+ and SmartCoolant technology \\nwill offer customers a hyperscale facility that is low-risk and fu\\xad\\nture-proof.  \\n \\nThe heat generated by the datacenter will be used to power an \\nadjacent Indoor Farms operation, which will feed the local com\\xad\\nmunity and give them access to fresh produce.  \\n \\nPrior to the installation, the local community did not have access \\nto its own supply of fresh fruit and vegetables. It was reliant on im\\xad\\nports from California and Mexico which wasn’t of a high quality and \\noften didn’t last due to the length of supply chains and distribution \\ntransit times.   \\n \\nThe unique location of Wyoming Hyperscale puts them in a \\nunique position where they are able to provide private DWDM con\\xad\\nnections to strategic exchanges across the US  with cutting edge \\nfiber connectivity to the server, diversity of carriers, routes and \\nlong-haul fiber networks. \\n \\nThis ensures users are able to meet the needs of their custom\\xad\\ners without paying over price premiums for fiber cross connects, \\nspace, land, electricity and water.  \\n \\n \\nWyoming Hyperscale came to Submer to see if their Immer\\xad\\nsion Cooling solutions could help.  The company chose Submer’s \\nSmartPod XL+ to kit out its entire Hyperscale facility.  \\n \\nHow will the SmartPod XL+ help power high-performance computing? \\nOvercome power and density issues\\xa0\\nThe SmartPod XL+ can hold up to 100kW of \\ncompute density using commercially availa\\xad\\nble technology, allowing you to house more \\ncompute in a smaller space and meet the \\ndensity requirements often requested by \\nhigh-performance computing. Save space \\nwhile simulataneously increasing compute.  \\n \\n\\xa0Slash cooling costs\\nThanks to Immersion, reduce cooling costs \\nby up to 95% in comparison to air cooling \\ntechniques.  \\n \\nProtect the environment \\nThe setup of the SmartPod XL + allows users \\nto not only improve its Power Usage Effec\\xad\\ntiveness (PUE) inside the datacenter and \\nreduce overall energy consumption, users are \\nalso able to harness the benefit of zero waste \\nof water and heat reuse.  \\n \\nSimplify and cut building and hardware costs\\xa0\\nImmersion does not require a specific envi\\xad\\nronment in order to be successful. SmartPod \\nXL + is a fully modular solution, and unlike its \\nair counterpart, does not require any extra \\nequipment such as chillers or fans. Addition\\xad\\nally, the lifespan of hardware increases, and \\nhardware failures decrease, between 30-\\n60%.  \\n \\nBenefits\\nPage 3\\nCase Study\\n  Immersion Cooling  |  Hyperscale\\nCollaboration between:\\n“Submer has built a strong repu\\xad\\ntation for building innovative, for\\xad\\nward-thinking technology. Thanks to \\nSubmer, we have been able to over\\xad\\ncome challenges companies face \\nwhen wishing to build datacenters \\nhardened against all potential natu\\xad\\nral disasters in the region, while also \\nensuring we remain sustainable and \\nhighly cost efficient.” \\nTrenton Thornock\\nFounder of Wyoming Hyperscale\\nUpon completion of the project, Wyoming Hyperscale will stand \\nat 120 MW, with a phase II project of an additional 90 MW already \\nin the master plan for the site and in its Engineering Services \\nAgreement with Rocky Mountain Power/PacifiCorp, who recently \\nannounced that they will host TerraPower’s advanced nuclear \\nreactor and liquid sodium energy storage solution in Kemmerer, \\nWyoming, only 30 miles nort of the switchgear from which Wyo\\xad\\nming Hyperscale draws its 138kV primary power. \\n \\nWyoming Hyperscale will use the heat generated by the Smart\\xad\\nPodXL in Phase I’s 30MW to power roughly 30 acres of Indoor \\nFarms. This will benefit users of the datacenter by offering a \\ncompetitive effective net power cost around 4 cents per kWh.   \\n \\nWyoming Hyperscale will continue to work with Submer to utilize \\nImmersion cooling in its current and future projects around the \\nglobe.  \\n \\nThis partnership illustrates how Immersion Cooling can offer sus\\xad\\ntainable and efficient solutions at scale and empower its users .  \\n \\nWant to know more about how we can help? Visit: submer.com \\n \\nWant to know more about Wyoming Hyperscale White Box? \\nVisit: wyominghyperscalewhitebox.com \\n \\nFounded in 2015, Submer provides best-in-class technology that \\nenables data centers around the world to leverage the power of \\nimmersion cooling for HPC, hyperscale, data centers, Edge, AI, DL \\nand blockchain applications. Headquartered in Barcelona, with \\noffices in Virginia and Palo Alto, California, Submer consists of an \\ninternational team of engineering, technological and business ex\\xad\\nperts. For more information, visit submer.com. \\nWant to know more about how we can help? \\nVisit: submer.com \\nWant to know more about Wyoming Hyper\\xad\\nscale White Box? Visit: https://wyominghy\\xad\\nperscalewhitebox.com\\nFounded in 2020 by members of a 6th generation ranching family, \\nWyoming Hyperscale is combining resources and has assembled \\na team of preeminent design, engineering and facilities mainte\\xad\\nnance & operations teams to sustainably satisfy parabolic demand \\nfor hyperscale data center capacity while implementing best-in-\\nclass solutions to directly address global climate change and elim\\xad\\ninate  the waste inherent in conventional datacenter designs.  \\n \\nWyoming Hyperscale decided to change the industry with patent\\xad\\ned and patent pending technologies that are innovative, efficient, \\nsustainable, and significantly less costly to build and operate.\\nWhat next?\\nAbout Submer\\nKnow more\\n“\\n”\\nAbout Wyoming Hyperscale\\nPage 4\\nCase Study\\n  Immersion Cooling  |  Hyperscale\\n  Click here to watch testimonial \\nCollaboration between:\\n', \" \\n \\n \\n \\nNavigating Liquid Cooling  \\nArchitectures for Data Centers  \\nwith AI Workloads  \\nExecutive summary \\nMany AI servers with accelerators (e.g., \\nGPUs) used for training LLMs (large language \\nmodels) and inference workloads, generate \\nenough heat to necessitate liquid cooling. \\nThese servers are equipped with input and \\noutput piping and require an ecosystem of \\nmanifolds, CDUs (cooling distribution) and \\noutdoor heat rejection. There are six common \\nheat rejection architectures for liquid cooling \\nwhere we provide guidance on selecting the \\nbest one for your AI servers or cluster. \\nEnergy Management Research Center  \\nWhite Paper 133 \\nVersion 2 \\nby Paul Lin  \\n    Robert Bunger \\n    Victor Avelar  \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    2 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n \\nAI training and inference servers use accelerators and processors with high thermal \\ndesign power (TDP)1. Air-cooling these chips becomes less practical when consid-\\nering heat sink dimensions, server airflow and energy efficiency, forcing a transition \\nto liquid-cooling. Liquid cooling servers offer benefits including improved accelera-\\ntor reliability & performance, increased energy efficiency, reduced water usage, \\nand reduced sound level.2  \\n \\nThere are two main categories of liquid cooling for AI servers – direct-to-chip and \\nimmersion3. There are slight differences in the heat rejection ecosystem that we will \\ncover. Data center operators and IT Managers unfamiliar with deploying liquid-\\ncooled servers will need to answer a few questions: \\n \\n• How do I get cold water in and hot water out? \\n• What is a CDU, and do I need one? \\n• What steps do I take to select an appropriate liquid cooling heat rejection ar-\\nchitecture? \\n \\nThere are three elements (i.e., heat capture within the server, CDU type, and meth-\\nod of rejecting heat to the outdoors) in a liquid cooling ecosystem. A CDU is a sys-\\ntem used to isolate the IT fluid loop from the rest of the cooling system and is nec-\\nessary to provide five key functions (i.e., temperature control, flow control, pressure \\ncontrol, fluid treatment, heat exchange and isolation). There are six common liquid \\ncooling architectures each with advantages, disadvantages, and when to imple-\\nment as shown in Table 1. \\n \\nHeat rejection method \\nCDU type \\nUse existing facility heat \\nrejection system \\nReject heat into air in the  \\nIT space \\nusing rack-mount CDU \\nusing floor-mount CDU \\nReject heat into existing  \\nfacility water system \\nusing rack-mount CDU \\nusing floor-mount CDU \\nCreate dedicated facility \\nheat rejection system \\nReject heat into independ-\\nent water system \\nusing rack-mount CDU \\nusing floor-mount CDU \\n \\n \\nThe coolant distribution unit (CDU) is a key element in the architecture. The CDU is \\nused to isolate the IT cooling fluid from the rest of the cooling system. Figure 1 \\nshows a simplified view of a liquid cooling architecture from ASHRAE. It shows \\nthree loops including the technology cooling system (TCS), facility water system \\n(FWS), and condenser water system (CWS). The FWS loop is considered the pri-\\nmary loop while the TCS loop represents the secondary loop. This brief description \\nprovides a hint of how a liquid-cooling architecture discussion could become com-\\nplex without a logical framework.  \\n \\n \\n1 See White Paper 110, The AI Disruption: Challenges and Guidance for Data Center Design for more \\ninformation on this topic. \\n2 See White Paper 279, Five Reasons to Adopt Liquid Cooling for more information on the benefits of liq-\\nuid cooling. \\n3 See White Paper 265, Liquid Cooling Technologies for Data Centers and Edge Applications for more \\ninformation on liquid cooling methods. \\nIntroduction \\nDescribing  \\nliquid cooling  \\narchitectures \\nTable 1 \\nCommon liquid cooling  \\narchitectures are comprised \\nof a heat rejection method \\nand a CDU type. \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    3 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\nRack\\nRack\\nExternal CDU\\nITE\\nCDU\\nITE\\nChiller\\nCondenser \\nWater System \\n(CWS)\\nFacilities \\nWater System \\n(FWS)\\nTechnology \\nCooling System \\n(TCS)\\nCooling \\nTower\\nDatacom Equipment Center\\n1\\n2\\n1\\n3\\n2\\n \\n \\nWe propose that a liquid cooling architecture can be fundamentally described by: \\n \\n1. Heat capture within the server (out of the scope for this paper) \\n2. CDU type \\n3. Method of rejecting heat to the outdoors \\n \\nThese three elements are labelled in Figure 1 and briefly described below. \\n \\n1. Heat capture within the server \\nHeat is captured from the IT components using a liquid. This liquid can be a dielec-\\ntric (commonly oil) in direct contact with the components, or a refrigerant or water \\npumped through cold plates attached to the heat-generating components.  While \\nthis is an important part of the liquid cooling architecture, it is out of scope for this \\npaper. \\n \\n2. CDU type \\nAs we indicated, a CDU is a system that isolates the IT fluid loop (TCS) from the \\nrest of the cooling system. CDUs are typically a single enclosure with all parts inte-\\ngrated within it. They perform five key functions described below. It’s important to \\nunderstand these functions before we describe the types of CDUs. \\n \\n• Temperature control – CDUs will precisely control the fluid temperature in the \\nTCS loop. The TCS supply temperatures are dictated by IT vendors and are \\ngenerally determined by the maximum case temperatures of the accelerators \\n& processors and liquid cooling solution used. \\n• Flow control – To move the heat away from chips, CDUs must be able to pro-\\nvide sufficient flow through the rack manifolds, connectors, and cold plates \\nacross all the servers and racks supported. Immersion tanks also have flow \\nrequirements of the dielectric across the servers. \\n• Pressure control – There are two aspects of pressure a CDU manages. First \\nis the maximum pressure allowed in the system and the second is the differen-\\ntial pressure (delta P) needed to provide the required flow. TCSs are com-\\nmonly operated under a positive pressure, but alternative CDUs can pump \\nwater under a vacuum, or often called “negative pressure”4, eliminating the \\nrisk of leaks in the TCS. This feature is commonly called a leak-prevention-sys-\\ntem (LPS). \\n \\n4 Key feature of a negative pressure CDU is leak prevention but has other advantages of simplifying the \\nconnections & components of the TCS loop and the rack & servers, which can reduce the overall cost. \\nFigure 1 \\nSimplified view of a liquid \\ncooling architecture in a \\ndata center \\n \\nSource: ASHRAE, Water-\\nCooled Servers: Common \\nDesigns, Components, \\nand Processes, page 10 \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    4 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n• Fluid treatment – The filtration and chemistry requirements of the fluids in the \\nTCS are more stringent than facility systems. For water-based TCSs, propyl-\\nene glycol-based water is a common fluid used to prevent biologic growth \\nand maintain water quality. \\n• Heat exchange and isolation – Exchanging the heat out of the TCS is a fun-\\ndamental function of a CDU. It must also isolate the fluid in the TCS loop, from \\nthe rest of the cooling system5.  \\n \\nCDUs are fundamentally comprised of pump(s), heat exchanger, filtration system, \\nand controls to perform these functions. There are many detailed attributes involved \\nin ultimately specifying a CDU (e.g., controller, filter type, etc.). However, you don’t \\nneed to specify all these attributes to choose the appropriate liquid cooling archi-\\ntecture for your facility. We can simplify this process by identifying only the critical \\nattributes of a CDU. Critical attributes are those that, if you incorrectly choose, will \\nforce you to go back and change your fundamental architecture, wasting time and \\nwork spent on detailed designs.  \\n \\nWe believe the CDU type must be based on two critical attributes: \\n \\n• Type of heat exchange (liquid-to-air, liquid-to-liquid, etc.) \\n• CDU capacity and form factor (rack-mounted, floor-mounted) \\n \\nWe describe each attribute in detail below. \\n \\nType of heat exchange \\nThere are six types of heat exchange seen in the liquid cooling industry: \\n \\n• Liquid to air (L-A) – TCS liquid loop heat is pumped to a coil (i.e., radiator) \\nwhere the heat is rejected directly into the data center air6. \\n• Liquid to liquid (L-L) – TCS liquid loop heat is transferred to a facility water \\nsystem. \\n• Refrigerant to air (R-A) – Two-phase direct-to-chip system rejects heat di-\\nrectly to air via a radiator. This operates like an air-based condenser. \\n• Refrigerant to liquid (R-L) – Two-phase direct-to-chip system rejects heat to a \\nfacility water system. This operates like a water-based condenser. \\n• Liquid-to-refrigerant (L-R) – A TCS liquid loop rejects heat to a facility \\npumped refrigerant system. \\n• Refrigerant-to-refrigerant (R-R) – Not typical. \\n \\nCDU capacity and form factor \\nThe pump size, heat exchanger size, and fluid type define a CDU system’s overall \\ncapacity (kW). CDUs come in a wide range of capacities depending on form factor:  \\n \\n• Rack-mounted – CDU mounted within a rack provides a TCS loop for a single \\nrack and can be pre-integrated with servers. Heat exchange can be L-A or L-L \\nwith CDU capacities ranging from 20-40 kW or 40-80 kW respectively. Rack-\\nmounted refrigerant TCS loops will also have capacities in these ranges. \\n \\n5 Direct-to-chip liquid-cooled servers have stringent requirements in water temperature, flowrate, and \\nchemistry. This means that water from facility systems (e.g., a chiller) cannot run directly through a \\nchip's cold plate. Doing so can corrode metals and tiny fluid channels within the cold plate. \\n6 Some server designs use cold plates, liquid loop, and L-A heat exchanger all contained within the \\nserver chassis. This is not considered a CDU since the server is operated and cooled just like a tradi-\\ntional air-cooled server. \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    5 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n• Floor-mounted – CDU provides a TCS loop for several racks. Form factor may \\nbe similar to an IT rack or larger as capacities increase. These CDUs are usu-\\nally located nearby or adjacent to liquid-cooled IT racks and for immersion \\nsystems can be integrated into the tanks. Floor-mounted CDUs can be L-A \\nwith capacities up to about 60 kW. L-L floor-mounted CDU capacities can \\nrange from 300 kW to greater than 1 MW. Figure 2 illustrates some examples. \\n  \\n \\n \\n   (a) \\n \\n \\n \\n       (b) \\n \\nFor this paper, we focus on two predominant types of heat exchanges (L-A and L-\\nL). Table 2 describes four common CDU types: \\n \\nType of heat exchange \\nCDU capacity and form factor \\nLiquid to Air (L-A) \\nRack-mounted (20-40 kW) \\nFloor-mounted (Up to 60 kW)  \\nLiquid to Liquid (L-L) \\nRack-mounted (40-80 kW) \\nFloor-mounted (300 kW and over) \\n \\n \\n3. Method of rejecting heat to the outdoors \\nThis is the third and final element for describing a liquid cooling architecture. Once \\nheat from the IT equipment is captured by the TCS loop the question then be-\\ncomes, how do I transfer this heat energy to the outdoors? The answer lies in the \\nheat rejection system as described in Figure 3. There are three common methods: \\n \\n• Existing heat rejection system \\no \\nReject heat in TCS loop to air in the IT space via liquid-to-air heat \\nexchange (also known as closed-loop local heat rejection)  \\no \\nReject heat in TCS loop to water in the facility systems via liquid-to-\\nliquid heat exchange (tap into existing FWS or CWS loop) \\n• Dedicated heat rejection system – design a new independent heat rejection \\nsystem for liquid cooling. \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 2 \\nExamples of CDU form \\nfactors \\n \\n(a): Floor-mounted (L-A) \\n(b): Floor-mounted (L-L) \\nTable 2 \\nCommon CDU types \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    6 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n \\n \\nUsing combinations of the last two architecture elements (“CDU type” and “Method \\nof rejecting heat to the outdoors”) we can construct six common liquid cooling ar-\\nchitectures seen in the industry. Table 3 illustrates these combinations. We de-\\nscribe how to choose the appropriate liquid cooling architecture in the next section. \\n \\nHeat rejection method \\nCDU type7 \\nExisting heat rejection system – Reject heat to air in IT space  \\nL-A Rack-mounted \\nL-A Floor-mounted \\nExisting heat rejection system – Reject heat to facility water \\nsystems \\nL-L Rack-mounted \\nL-L Floor-mounted \\nDedicated heat rejection system – Reject heat to independent \\nwater systems \\nL-L Rack-mounted \\nL-L Floor-mounted \\n \\n \\nIn this section we simplify the process of choosing the most appropriate of the six \\ncommon architectures by breaking it down into two steps. \\n \\n• Step 1 - Choose the heat rejection method \\n• Step 2 - Choose CDU capacity and form factor \\n \\nNotice in Table 3 that the type of heat exchange (e.g., L-A) listed under “CDU type” \\nis dictated by the heat rejection method.8 The second CDU attribute, “CDU capacity \\nand form factor”, is independent of the heat rejection decision. Therefore, you can \\ndetermine the appropriate liquid cooling architecture in two independent steps. The \\nselection of a heat rejection method coupled with CDU capacity and form factor de-\\npends on many factors including the four key ones below:   \\n \\n \\n7 R-A and R-L will have same decision process and L-A and L-L and thus are not added to the table. \\n8 For example, the secondary air side of L-A heat exchanger can’t interface with facility water system. \\nIndoor heat\\nexchange\\nOutdoor heat\\nexchange\\nTransport\\nFluid\\nWithin the\\nserver\\nTransport\\nFluid\\nTransport\\nFluid\\nServer\\nCRAC/\\nCRAH\\nCDU\\nOutdoor\\nheat\\nrejection\\nsystem\\nCRAC – computer room air conditioner\\nCRAH – computer room air handler\\nRefrigerant\\nAir\\nAir\\nWater\\nRefrigerant\\nWater\\nRefrigerant\\nAir\\nDielectric\\nFigure 3 \\nSimplified view of heat \\nrejection in a liquid \\ncooling architecture \\nTable 3 \\nCommon liquid cooling  \\narchitectures are comprised \\nof a heat rejection method \\nand a CDU type. \\nChoosing the \\nappropriate \\narchitecture \\n \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    7 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n• Existing cooling infrastructure compatibility – indicates the ease the existing \\ncooling infrastructure can support new liquid-cooled servers.  \\n• Deployment size – indicates the number of racks that can be supported by \\nthe liquid cooling architecture. \\n• Speed of deployment – indicates how long it takes the facility to deploy the \\nliquid-cooled architecture from design to construction, to first operation. \\n• Energy efficiency – indicates the relative efficiency of the overall liquid cool-\\ning architecture. Note that all architectures will provide some improvement \\nover equivalent air-cooled architectures. \\n \\nYou likely won’t be able to maximize all these factors, but the idea is to make \\ntradeoffs between them based on your priorities. For example, you’re unlikely to get \\nthe highest efficiency with the liquid cooling architecture that is most compatible \\nwith your existing air-cooled system. This is because it’s less efficient to transfer \\nheat using air compared to water. The following sections comprehend these factors \\nby describing each architecture choice in terms of advantages, disadvantages, and \\nwhen to implement. Note that all the heat rejection system diagrams illustrate a \\nfloor-mounted CDU but could be replaced with rack-mounted CDU. \\n \\nStep 1 – Choose the heat rejection method  \\nExisting heat rejection system – Reject heat to air in IT space  \\nThis architecture allows you to design the TCS loop as a self-contained system \\nwithin the IT space. The L-A CDU may be rack-mounted or floor-mounted. In this ar-\\nchitecture, everything about the existing air-cooled facility infrastructure stays the \\nsame (as shown in Figure 4). This architecture is also known as closed-loop local \\nheat rejection. Finally, all the heat in the IT room is rejected to the outdoors by the \\nexisting cooling infrastructure.  \\n \\n \\n \\nAdvantages \\n• Compatible with most existing cooling infrastructure \\n• No need to modify existing cooling infrastructure \\n• Can be prefabricated for easier installation, standardization, etc. \\n• If there is a problem with the TCS loop, a small number of servers / racks \\nwould be affected \\n \\nLiquid-\\ncooled ITE CDU\\nAir-cooled IT Racks\\nCRAH\\nCRAH\\nAir-cooled Chiller\\nwith Free Cooling\\nAir-cooled IT Racks\\nCRAH – computer room air handler\\nCDU – coolant distribution unit\\nFigure 4 \\nDiagram of “Reject heat to \\nair in IT space” architecture \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    8 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\nDisadvantages \\n• Reduced efficiency driven by more heat exchanges and fans compared to L-L \\nCDU deployments \\n• Rack-mounted and floor-mounted L-A CDUs can take up rack or floor space \\n• Costly for large-scale deployments \\n• Many separate loops are required to monitor and maintain water quality \\n• More difficult to achieve full concurrent maintainability or full redundancy \\n• Small water based TCS loops have short thermal ride through, which means if \\nthe CDU fans fail, the volume of water in the loop won’t have much cooling ca-\\npacity to support the load compared to other architectures  \\n \\nWhen to implement \\n• Chilled or condenser water is not available or connecting to existing cooling \\ninfrastructure is not acceptable \\n• There is sufficient air-cooled capacity and airflow analysis or computational \\nfluid dynamics (CFD) indicates the room can support the high densities. \\n• Supporting small scale liquid-cooled server deployments, from a single server \\nto several racks \\n• Speed of deployment is a top priority \\n \\nExisting heat rejection system – Reject heat to facility water systems \\nIn this architecture, the TCS loop leverages an L-L CDU, to become an isolated \\nloop fed off a chilled or condenser water loop. The server heat is transferred from \\nthe TCS loop to the facility loop via the CDU’s L-L heat exchanger (as shown in Fig-\\nure 5). The heat is then rejected to the outdoors or reused for other purposes (e.g., \\ndistrict heating). 60% to 90% of a liquid-cooled server’s heat can be removed via \\nthe liquid, depending on the number of liquid-cooled components. The remaining \\nheat is removed by air cooling (e.g., CRAC, CRAH9, rear door heat exchanger). \\n \\n \\n \\n \\n \\n \\n \\n9 CRAC – computer room air conditioner, CRAH – computer room air handler \\nAir-cooled IT Racks\\nCDU\\nCRAH\\nLiquid-cooled IT Racks\\nFWS\\n(Primary\\nLoop)\\nTCS\\n(Secondary Loop)\\nAir-cooled Chiller\\nwith Free Cooling\\nCRAH – computer room air handler\\nCDU – coolant distribution unit\\nFigure 5 \\nDiagram of “Reject heat to \\nexisting chilled water loop” \\narchitecture \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    9 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\nAdvantages \\n• Reduced investment by using existing heat rejection system \\n• Higher liquid cooling heat rejection capacities, increased energy efficiency, \\nand reduced sound levels (lower airflow rate across servers) compared to liq-\\nuid-to-air “Reject heat to air in IT space” architecture \\n• For retrofits, the CDUs can recoup space previously occupied by CRAHs \\n \\nDisadvantages \\n• More site installation work compared to L-A CDUs, including CDU connection \\nto facility water systems and TCS piping to racks \\n \\nWhen to implement \\n• Mid to large scale liquid-cooled server deployments in data centers with \\nchiller plant \\n• When water loop connections or “tap-offs” already exist \\n• When energy efficiency is a higher consideration over speed of deployment \\n(e.g., L-A CDUs) \\n \\nDedicated heat rejection system – Reject heat to independent water systems \\nIn this architecture, a dedicated heat rejection system is designed for liquid cooling \\nusing a L-L CDU. This optimizes temperature and flow of the TCS and heat rejection \\nloops in the most efficient way without the constraints imposed by a shared air-\\ncooled heat rejection system. Figure 6 demonstrates an example of dedicated heat \\nrejection systems for liquid cooling and air cooling. A dry cooler with trim compres-\\nsor is used to provide a high supply water temperature (40°C/104°F) for liquid cool-\\ning, while a free-cooling chiller is used to provide a low chilled water temperature \\n(20°C/68°F) for air cooling.  \\n \\n \\n \\nAdvantages \\n• High energy efficiency due to increased free cooling hours (mechanical cool-\\ning is not required in most cases except on hottest days) \\n• Higher return water temperatures offer the opportunity for reuse in space heat-\\ning, preheating industrial process water, etc. \\n• Implementation does not disrupt existing cooling system \\n \\nFree Cooler with\\nCompressor Assist\\nFWS\\n(Primary Loop)\\nCDU\\nTCS\\n(Secondary Loop)\\nCRAH\\nAir-cooled Chiller\\nwith Free Cooling\\nAir-cooled IT Racks\\nLiquid-cooled IT Racks\\nCRAH – computer room air handler\\nCDU – coolant distribution unit\\nFigure 6 \\nDiagram of dedicated heat  \\nrejection system architecture \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    10 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\nDisadvantages \\n• Requires design of additional facility-level piping system \\n• Requires investment in dedicated heat rejection system \\n• Longer deployment time compared to other architectures \\n \\nWhen to implement \\n• Significant liquid-cooled server deployments are expected \\n• High energy efficiency is a priority \\n \\nStep 2 – Choose CDU capacity and form factor  \\nRack-mounted \\nThe CDU is dedicated to a single rack, which means each rack has its own TCS \\nloop. Generally mounted at the bottom of the rack, the CDU includes a pumping \\nunit, filtration, and controls. Heat is either transferred to the data center air via a fan-\\nassisted rear-door heat exchanger (L-A) or to a facility loop via a L-L heat ex-\\nchanger. \\n \\nAdvantages \\n• Can be pre-integrated and tested with the servers prior to install in the data \\ncenter. \\n• Limits potential failure modes to a single rack (e.g., TCS leak or contamina-\\ntion) \\n• Redundancy (e.g., 1N vs 2N pumps) can be targeted for each rack \\n• Simple solution for a traditional data center with a few liquid-cooled racks \\n \\nDisadvantages \\n• Cost per kW IT load becomes greater than floor-mounted CDUs as the num-\\nber of racks increase \\n• CDU occupies IT server space \\n• Limits maximum rack density to about 40 kW (L-A) and 80 kW (L-L) \\n• As number of racks increase, total installation time is longer compared to a \\nlarger floor-mounted CDU (i.e., commissioning CDU in each rack) \\n• As number of racks increase, efficiency degrades compared to a single larger \\nfloor-mounted CDU \\n \\nWhen to implement \\n• Speed of deployment is important (applies for few liquid-cooled racks) \\n• Limited liquid-cooled rack deployments are expected (1 to 10 racks) \\n \\nFloor-mounted \\nThe CDU is dedicated to a row or multiple rows of racks, meaning they all share the \\nsame TCS loop. It may be placed at the end of the row or further away from the AI \\ncluster. Heat is either transferred to the data center air via a fan-assisted heat ex-\\nchanger (L-A) or to a facility loop via L-L heat exchanger. \\n \\nAdvantages \\n• Lower cost per kW IT load for large deployments compared to rack-mounted \\nCDUs \\n• In retrofits, CDUs can be selected based on the location and capacity of the \\nCRAHs they replace, to minimize piping work \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    11 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n• No IT rack space occupied by CDU \\n• Higher rack densities can be achieved compared to rack-mounted CDU  \\n• As the number of racks increases, installation time can be compressed com-\\npared to multiple rack-mounted CDUs \\n• As number of racks increase, efficiency improves compared to many rack-\\nmounted CDUs running multiple pumps \\n• More thermal ride thru than rack-mounted CDUs \\n \\nDisadvantages \\n• All racks on single TCS loop become susceptible to common cause failures \\n(e.g., TCS leaks, contamination, controls, etc.) \\n• Takes up floor space \\n \\nWhen to implement \\n• More than 10 liquid-cooled racks are expected \\n• Workloads (e.g., AI training cluster) can tolerate a common cause failure (e.g., \\nloss of fluid flow) for all racks served by a single CDU \\n• There’s no vertical space in IT rack to install rack-mounted CDUs \\n \\n \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    12 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n \\nAn increasing number of servers require liquid cooling systems to support AI \\nworkloads. Depending on the scale of liquid-cooled server deployments, a data \\ncenter can be cooled through exsiting or decidicated heat rejection systems. The \\nterminology, architectures, and factors in choosing dicussed in this paper provide a \\nstarting point for data center operators to build out the ecosystem. We provide the \\nfollowing answers to common liquid cooling questions raised by data center opera-\\ntors: \\n \\n• How do I get cold water in and hot water out? – Use three elements (i.e., \\nheat capture within the server, CDU type, and method of rejecting heat to the \\noutdoors). \\n• What is a CDU, and do I need one? –  A CDU is a system used to isolate the \\nIT fluid loop (TCS) from the rest of the cooling system, and is necessary to \\nprovide five key functions (i.e., temperature control, flow control, pressure \\ncontrol, fluid treatment, heat exchange and isolation). \\n• What steps do I take to select an appropriate heat rejection architecture? – \\nThis paper describes six liquid cooling architecture each with advantages, \\ndisadvantages, and when to implement. \\n \\n \\n \\n \\n \\nAbout the authors \\nPaul Lin is the Research Director and Edison Expert at Schneider Electric's En-\\nergy Management Research Center. He is responsible for data center design \\nand operation research and consults with clients on risk assessment and de-\\nsign practices to optimize the availability and sustainability of their data center \\nenvironment. He is a recognized expert, and a frequent speaker and panelist at \\ndata center industry events. Before joining Schneider Electric, Paul worked as \\nan R&D Project Leader in LG Electronics for several years. He is also a regis-\\ntered professional engineer and holds over 10 patents. Paul holds both a Bach-\\nelor’s and Master’s of Science degree in mechanical engineering from Jilin Uni-\\nversity. He also holds a certificate in Transforming Schneider Leadership Pro-\\ngramme from INSEAD. \\nRobert Bunger is the Innovation Product Owner within the CTO office at \\nSchneider Electric. In his 26 years at Schneider Electric, Robert has held man-\\nagement positions in customer service, technical sales, offer management, \\nbusiness development & industry associations. While with APC / Schneider \\nElectric, he has lived and worked in the United States, Europe, and China. Prior \\nto joining APC, he was a commissioned officer in the US Navy Submarine force. \\nRobert has a BS in Computer Science from the US Naval Academy and MS EE \\nfrom Rensselaer Polytechnic Institute. \\nVictor Avelar is a Senior Research Analyst at Schneider Electric’s Energy Man-\\nagement Research Center. He is responsible for data center design and opera-\\ntions research, and consults with clients on risk assessment and design prac-\\ntices to optimize the availability and efficiency of their data center environ-\\nments. Victor holds a bachelor’s degree in mechanical engineering from Rens-\\nselaer Polytechnic Institute and an MBA from Babson College. He is a member \\nof AFCOM. \\n \\nConclusion \\n \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric – Energy Management Research Center  White Paper 133  Version 2  \\n \\n \\n    13 \\nNavigating Liquid Cooling Architectures for Data Centers with AI Workloads \\n \\nThe AI Disruption: Challenges and Guidance for Data Center Design   \\nWhite Paper 110 \\n \\nLiquid Cooling Technologies for Data Centers and Edge Applications \\nWhite Paper 265 \\n \\nFive Reasons to Adopt Liquid Cooling   \\nWhite Paper 279 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNote: Internet links can become obsolete over time. The referenced links were available at the time this  \\npaper was written but may no longer be available now. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nContact us \\nFor feedback and comments about the content of this white paper: \\nSchneider Electric Energy Management Research Center \\ndcsc@schneider-electric.com \\nIf you are a customer and have questions specific to your data center project: \\nContact your Schneider Electric representative at \\nwww.apc.com/support/contact/index.cfm \\n \\n \\nResources \\nBrowse all  \\nwhite papers  \\nwhitepapers.apc.com  \\n \\ntools.apc.com  \\nBrowse all  \\nTradeOff Tools™ \\n© 2024 Schneider Electric. All rights reserved. \\n\", '©2021 Shell Corporation. All rights reserved.\\nINTEGRATED IMMERSION \\nCOOLING SOLUTIONS \\nENABLING GAME-\\nCHANGING ENERGY  \\nUSE REDUCTION AT  \\nDATA CENTERS\\nwww.shell.us/immersion\\nThese account for an estimated 1% of global electricity\\nconsumption,¹ more than a third of which is for cooling\\nelectronic components, and have half the carbon dioxide\\nemissions of global air travel. To compound matters, data \\ncenter infrastructure is expanding by more than 20% every \\nyear,² so, without game-changing action, data center \\nemissions will increase exponentially. At the same time, \\nsociety needs to cut carbon dioxide emissions by 50% to \\nreduce the impact of global warming.³\\nIn tackling climate change, the focus is increasingly on \\nlimiting the global temperature rise to 1.5°C. To achieve \\nthis, society needs to stop adding to atmospheric \\ngreenhouse gases – a state known as net-zero emissions – \\nby around 2060. More advanced parts of the world need \\nto reach that point sooner.\\nThat is why Shell has set itself an ambition to become, by \\n2050 or sooner, a net-zero-emissions energy business.⁴\\nWe are working towards this in many ways, including our\\nShell Immersion Cooling Fluid to tackle data center energy \\nuse as part of a wider integrated offer to help center \\noperators meet their climate action ambitions.\\nAir cooling equipment for electronic \\ncomponents accounts for 38% of the  \\nelectricity use in data centers,⁵ takes up \\nconsiderable space and is being put under \\nincreasing demand.\\nTraditional air cooling, with its low cooling capacity and \\npractical limitations (noise, footprint and build complexity), \\ncannot meet these growing demands without higher \\nenergy and space requirements, but there are cost-\\neffective alternatives.\\nTHE INTERNET IS A FUNDAMENTAL \\nPART OF MODERN LIFE. THE CLOUD \\nENABLES US TO WORK ANYWHERE, \\nWATCH ON-DEMAND MOVIES AND \\nMUCH MORE. THE CLOUD MAY \\nSOUND TRANSIENT, BUT IT HAS A \\nPHYSICAL, ENERGY-HUNGRY HOME – \\nA NETWORK OF DATA CENTERS.\\nShell’s Ambition Is to Be a Net-\\nZero Emissions Energy Business\\nMore Data and Higher Processing \\nPower Are Creating a New  \\nCooling Era\\nFull immersion in a thermally conductive, electrically non-conductive (dielectric) coolant is a highly  \\nefficient way to keep data center hardware and computer components cool. It can cut energy consumption \\nand reduce carbon dioxide emissions while reducing costs and increasing location flexibility. On a larger \\nscale, immersion cooling offers standardized efficiency that is independent of location, facility and \\nhardware requirements.\\nMore data. Estimates show that there will \\nbe more than 50 billion connected devices.4\\nExtreme processing speeds that \\ngenerate more heat. High-performance \\ncomputing is required for machine learning, \\nspeech and facial recognition, cryptocurrency \\nmining, blockchains, artificial intelligence, etc.\\nIncreased demand for online \\nservices. Examples include near-real-time \\nanalysis for autonomous vehicle route finding, \\ncontent distribution and fintech.\\nEdge computing. Data centers are \\nbeing built on the network edge to reduce \\ndata transfer time and increase availability; \\nthese facilities often have space and energy \\ninfrastructure constraints.\\nImmersion cooling is a way of cooling IT components, including whole servers, by submerging them in a \\ndielectric (electrically non-conducting) fluid. The fluid has more than a thousand times the thermal capacity \\nof air (by volume), which makes immersion cooling a highly efficient alternative to air cooling. The liquid \\ncirculates by natural convection or is pumped to remove heat from the components. An added benefit is \\nthat this heat can be recovered by water-cooled heat exchangers for reuse in district heating projects. In \\nsingle-phase immersion cooling, the coolant stays as a liquid without phase changing, whereas the coolant \\nboils to form a gas and then condenses back to a liquid in two-phase immersion cooling.\\nWhat Is Immersion Cooling?\\nUsing water-filled plates can upgrade the cooling \\ncapacity of some air-cooled facilities, but there is a high \\nleakage potential with the risk of water damage to the \\nsystem, and air cooling is still necessary. Plus, this method \\nis inflexible and expensive.\\nTwo-phase immersion cooling offers a high cooling \\ncapacity without needing circulation pumps and thus \\nhas a lower energy requirement. However, these systems \\nhave very high capital costs and use expensive fluids with \\nhigh global warming potential. Although these systems \\nare sealed, fluid may be lost through evaporation. The \\neffect of coolant vapor on people’s health is unclear and \\nthe fluid needs regular maintenance to replace losses. \\nAccess for maintenance is also more complicated with \\nsealed systems.\\nCompared with two-phase systems, single-phase \\nimmersion cooling systems have a simple architecture \\nfor significantly lower capital costs. Since there is no \\nevaporation of the dielectric coolant, open tanks can be \\nused for easy maintenance. Compared with traditional \\nair cooling, single-phase immersion cooling technology \\nas an integrated solution offers6,7:\\nWhat Is the Best\\nEnergy-Saving Solution?\\nUp to a 33% lower total cost of ownership\\nUp to a 48% reduction in energy footprint\\nUp to potentially 30% less carbon dioxide emissions\\nUp to 80% less floor space\\nUp to 40% more CPU performance\\n30–40% less operating and capital expenditure\\nHigh reliability with no moving parts\\nIndependence from climate. It works in \\nchallenging environments, including high ambient \\ntemperatures (up to 45°C) (130°F), humid \\nenvironments and industrial settings\\n■\\n■\\n■\\n■\\n■\\n■\\n■\\n■\\nSINGLE-PHASE IMMERSION\\nCOOLING TECHNOLOGY AS AN\\nINTEGRATED SOLUTION OFFERS\\nUP TO 48% REDUCTION IN \\nENERGY FOOTPRINT⁷\\nSingle-phase immersion cooling solutions are at the heart of our integrated energy solution for \\ndata centers.\\nFluid Solutions. We have developed Shell Immersion Cooling Fluid S5 X to help you get the \\nmost from your natural convection or pumped/forced data server liquid cooling systems. This synthetic fluid \\nmade from natural gas is inherently stable to provide superior performance and material compatibility with \\nserver components. It is clear, odorless, non-toxic and so safe that it is approved for use in applications such \\nas cosmetics.\\nHeat Reuse. Immersion cooling gives you the opportunity to reuse the heat generated by your servers. \\nFor example, a system can be optimized for heat reuse scenarios to recover up to 99% of the heat \\ngenerated as 55°C water for use in district heating projects or directly by industrial users.9\\nRenewable Energy. Immersion cooling technology as an integrated energy solution can cut data center \\nenergy use by up to 48%.7 We can help you reduce your carbon footprint further through on-site solar \\npower and/or renewable energy supply.\\nCarbon Credits. Once you have reduced your energy needs, we can deliver environmental solutions that \\nenable you to manage your local and regional carbon dioxide compliance obligations and gain access to \\nglobal markets. If you have made a strategic decision to reduce or eliminate your net carbon footprint, we \\noffer a variety of high-quality voluntary carbon credit solutions. Most of the projects in our global portfolio \\nare nature-based and focused on protecting, transforming or restoring land.\\nImmersion Cooling – The Heart of an Integrated Energy Solution\\nAir cooling (traditional  \\ndata centers)a\\nCooling capacity\\nHardware reliability\\nHardware performance\\nHeat recovery\\nInitial capital expenditure\\nOperating expenditure\\nHarware integration\\n(For example, space needed)\\nAIR COOLING\\nLIQUID COOLING\\nSingle-phase cooling:\\ncold plate (water)b\\nThe liquid coolant never changes state\\n(no boiling/freezing, etc.), for example, it is\\nused in high-performance computing\\nMainly for high heat\\nrange applications\\nsuch as crypto mining\\nSingle-phase cooling:\\nimmersionab\\nTwo-phase cooling:\\n(High heat range \\napplications)b\\na. Infrastructure Services Group, reported in Global immersion cooling market in data centers – Growth, trends, forecast (2019–2024), Mordor Intelligence (2019) \\nb. Customer feedback and internal evaluation\\nWhich Type of Cooling System Is the Best Fit for Your Data Center?\\nCloud providers and \\nhyperscale data\\ncenters can optimize \\ntheir efficiency and \\nachieve their sustainability \\ngoals while standardizing \\nfacilities for a variety of \\nhardware requirements.\\nEnterprises can \\nsimplify their on-premise \\ndata centers for high \\nefficiency and decreased \\ndependency on the\\npublic cloud by adopting \\nnext-generation hardware.\\nTelecom providers \\ncan operate edge  \\ndata centers anywhere. \\nThey can also use existing \\nbuildings within power \\nand cooling availability \\nconstraints.\\nResearch institutes \\ncan facilitate on-campus \\nHPC environments without \\nthe need for advanced \\ndata centers with their \\nassociated energy and \\ncost demands.\\nColocation providers \\ncan facilitate high density \\nand performance compute \\nusers in a simple and\\nscalable manner.\\nIMMERSED COMPUTING \\nTECHNOLOGY CAN  \\nCUT ENERGY USE BY 48%, \\nBOOST CPU PERFORMANCE \\nBY UP TO 40% AND REDUCE \\nCAPITAL AND OPERATING \\nEXPENDITURE BY 30-40%.6,7\\nBenefits by User\\nThe Shell Immersion Cooling Fluid solution is one of a \\nhandful of innovations featured in a World Economic\\nForum white paper showcasing disruptive \\ninnovations in the energy sector judged to be \\nNOVEL, BENEFICIAL TO SOCIETY AND \\nACCELERATING THE ENERGY TRANSITION.8\\nImmersed Computing is a trademark of Aecorsis BV.\\nReduce energy costs and emissions through its \\nhigh cooling efficiency, flow behavior and excellent\\nthermodynamic properties\\nCost less to manufacture than fluorocarbon and \\nincisively engineered synthetic fluids\\nImprove product safety through its high \\ncompatibility with computer components\\nBe safe and easy to handle\\nBetter heat capacity\\nHigher heat transfer efficiency\\nImprove product safety through its high \\ncompatibility with computer components\\nLower volatility means that the tanks can be operated\\nsafely for the life of the data center without being \\nsealed or having the fluid replaced\\nLower fluid density for less floor loading and possible\\nreinforcement\\n■\\n■\\n■\\n■\\n■\\n■\\n■\\n■\\n■\\nShell Immersion Cooling Fluid S5 X, is a synthetic,\\nsingle-phase immersion cooling fluid made from natural\\ngas using Shell’s gas-to-liquids technology.\\nIt is designed to⁹:\\nCompared with the fluorocarbons typically\\nused in two-phase systems, Shell Immersion\\nCooling Fluid S5 X has⁹:\\nFluid Development\\nSHELL IMMERSION COOLING \\nFLUID S5 X IS A SYNTHETIC, \\nSINGLE-PHASE IMMERSION\\nCOOLING FLUID MADE FROM \\nNATURAL GAS USING SHELL’S \\nGAS-TO-LIQUIDS TECHNOLOGY.\\nShell Immersion Cooling Fluid S5 X has been developed to achieve high levels of safety, \\nperformance and reliability. It is optimized for natural convection-driven immersion cooling solutions but can \\nalso be used in pumped/forced circulation systems.\\nSafety and purity. Being made from natural gas gives Shell Immersion Cooling Fluid S5 X \\noutstanding safety and purity. It meets EU and US pharmaceutical purity requirements, is a non-\\nhalogenated, food-grade product that is free from allergens and has extremely low volatility.10\\nPerformance. Shell Immersion Cooling Fluid S5 X has excellent thermodynamic properties, a low \\ndensity and a high flash-point. It is non-evaporating.10\\nProtection and reliability. The fluid has high compositional consistency and very high oxidation \\nand thermal stability. It contains virtually no sulphur, nitrogen or aromatics, and is non-corrosive.10\\n■\\n■\\n■\\n \\nFOOTNOTES\\n[1] Kamiya, G., &amp; Kvarnström, O. (2019, December 20). Data centres and energy – from global headlines to local headaches? Retrieved April 27, 2021, \\nfrom https://www.iea.org/commentaries/data-centres-and-energy-from-global-headlines-to-local-headaches\\n[2] Data center infrastructure management Solutions market by application and geography - forecast and Analysis 2020-2024. (2020, August). Retrieved \\nApril 27, 2021, from https://www.technavio.com/report/data-center-infrastructure-management-solutions-market-industry-analysis\\n[3] Our climate target: Frequently asked questions. (n.d.). Retrieved April 27, 2021, from https://www.shell.com/energy-and-innovation/the-energy-future/\\nwhat-is-shells-net-carbon-footprint-ambition/faq.html\\n[4] Behind the numbers: Growth in the Internet of things. (2015, March 20). Retrieved April 27, 2021, from https://www.ncta.com/whats-new/behind-the-\\nnumbers-growth-in-the-internet-of-things\\n[5] Centres using traditional air-based cooling technologies. 3M report referenced in Global Immersion Cooling Technology, Mordor Intelligence (2019).\\n[6] Immersion cooling market in data centers: Growth, forecast (2019-2024). (2020). Retrieved April 27, 2021, from https:/www.mordorintelligence.com/\\nindustry-reports/immersion-cooling-market-in-data-centers\\n[7] De Azevedo, E., Wang, L., Veeralinga Shivaprasad, P., &; Wei, T. A NEW IMMERSION COOLING FLUID to enable low-carbon data centres (No. 30 \\ned., Vol. 09, pp. 50-55, Tech.). Shell TechXplorer. These figures are based on Asperitas’ test results. The benefits achieved will vary according to the actual site \\ndeployment.\\n[8] Global innovations from the energy sector: World Economic Forum white paper. (2020, May 27). Retrieved April 27, 2021, from https://www.weforum.\\norg/whitepapers/global-innovations-from-the-energy-sector\\n[9] Asperitas’ & Shell. Integrated Immersion Cooling Solutions - Enabling Game Changing Energy USE Reduction at Data Centers [Brochure]. Author. \\nRetrieved May 03, 2021, from https://www.shell.us/businesscustomers/lubricants-for-business/process-oils/immersion-cooling-fluids\\n[10] Based on Shell’s internal tests and evaluations. Technical Data Sheet: Immersion Cooling Fluid S5 X. (2020, September 27).\\n', ' \\n \\n \\n \\n \\n \\n \\nSite Readiness Checklist \\n \\nDocument Number: IN-001-03375 \\nCPP Document Number: GRC-CPP-F-01-20 \\n \\nCustomer Contact Information \\nCompany \\n \\nInstallation Location Address \\n \\nPrimary Contact Name \\n \\nPrimary Contact Phone \\n \\nPrimary Contact Email \\n \\n \\nPROPRIETARY NOTE \\nTHIS DOCUMENT CONTAINS INFORMATION CONFIDENTIAL AND \\nPROPRIETARY TO GREEN REVOLUTION COOLING AND SHALL NOT BE \\nREPRODUCED OR TRANSFERRED TO OTHER DOCUMENTS OR \\nDISCLOSED TO OTHERS OR USED FOR ANY PURPOSE OTHER THAN \\nTHAT FOR WHICH IT WAS OBTAINED WITHOUT THE EXPRESSED \\nWRITTEN CONSENT OF GREEN REVOLUTION COOLING. \\nCopyright © 2023 Green Revolution Cooling, all rights reserved. \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n1 \\n \\nTABLE OF CONTENTS \\n1.0 \\nIntroduction ..................................................................................................................................... 2 \\n1.1. \\nPurpose ...................................................................................................................................... 2 \\n1.2. \\nScope.......................................................................................................................................... 2 \\n1.3. \\nContact Information ..................................................................................................................... 2 \\n2.0 \\nFacility Requirements ..................................................................................................................... 3 \\n3.0 \\nCoolant ........................................................................................................................................... 3 \\n4.0 \\nFacility Water .................................................................................................................................. 4 \\n5.0 \\nFacility Power ................................................................................................................................. 5 \\n6.0 \\nIT Requirements ............................................................................................................................. 5 \\n7.0 \\nCustomer Responsibility for Delays ................................................................................................ 6 \\n8.0 \\nAcceptance ..................................................................................................................................... 7 \\n \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n2 \\n \\n \\n1.0 Introduction \\n1.1. \\nPURPOSE \\nThis Site Readiness Checklist is to be completed and validated by GRC’s \\nCustomer (the “Customer”) in preparation for the installation of GRC equipment \\n(the “GRC Solution”). which may include:  \\n(a) Site readiness for the GRC Solution components and liquid coolant to be \\nshipped and stored at the Customer’s installation location; and  \\n(b) Site readiness for the GRC Installation Team to perform its installation \\nactivity, including equipment in place, adequate space, chilled water \\ninfrastructure, electrical infrastructure, network access and related items. \\n1.2. \\nSCOPE \\nThe installation scope includes all components identified as part of the GRC \\nSolution, which may include ICEraQs, Micros, HashRaqs, and related equipment. \\n1.3. \\nCONTACT INFORMATION \\nFor questions or clarification of the contents of this document please contact \\nsupport@grcooling.com. \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n3 \\n \\n2.0 Facility Requirements \\nRequirement \\nResponse \\nWere you provided with the GRC New Customer Installation Preparation \\nGuide? If not, contact your Sales representative as it includes information \\nimportant to the success of your implementation. \\n \\nAll permits and approvals must be in place prior to the arrival of the \\nequipment.\\n \\nThe customer is responsible for ensuring that the floor on which the \\nsystem(s) will be installed is level <3/16” (5 mm) rise over 10’ (3 m) travel \\n(raised floor compatible), <0.1” (<2.54 mm) average height change over \\n84” (2.13 m). One method to ensure a level floor is using an 8-foot (2.44 \\nm) box level. \\n \\nConfirm that the floor in the installation location meets required floor-\\nloading requirements stated in the Engineering Submittal. \\n \\nWho are the onsite personnel to assist in the installation and receive \\ntraining listed in the GRC New Customer Installation Preparation Guide? \\nProvide the names, phone numbers, and email addresses of each. \\n \\nWhat are the specific facility access requirements? (Sign-in, badges, \\nkeys, pass codes, escorts, parking) \\n \\nIs there a hardware store nearby? \\n \\nIs there a convenience store or restaurant nearby? \\n \\n \\n3.0 Coolant \\nRequirement \\nResponse \\nThe Customer is responsible for obtaining and reviewing the Coolant \\nSafety Data Sheet (SDS) and identifying any risks to the Installation \\nTeam. \\n \\nAre there facility specific requirements related to the coolant deployment? \\n \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n4 \\n \\n4.0 Facility Water \\nRequirement \\nResponse \\nWater connections: Underfloor or ceiling? \\n \\nConfirm that water source matches the specification in the Engineering \\nSubmittal. \\n \\nConfirm that flow rate matches the specification in the Engineering \\nSubmittal. \\n \\nCustomer is responsible for providing a qualified/licensed plumber for all \\nfacility water routing, delivery, and connections. \\n \\nAre the necessary bypass loops in place on the water lines? \\n \\nWill the GRC Solution be installed in a Data Center? If not, specify the \\nlocation (storage, facility, test facility, storage room). \\n \\nIs there a chilled water source available for the specified installation \\nlocation? \\n \\nHas the chilled water source been treated with the proper inhibitors in \\naccordance with the Engineering Submittal? \\n \\nAre water pipes located and functional per drawing with a manual \\nisolation valve nearby? Is water flowing, pipes flushed, is there adequate \\nwater circulation or is an additional pump required for your installation? \\n(Reference Engineering Submittal for specific requirements or consult \\nyour Solution Architect). \\n \\n \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n5 \\n \\n \\n5.0 Facility Power \\nRequirement \\nResponse \\nElectrical connections: Underfloor or ceiling? \\n \\nCustomer is responsible for providing a qualified/licensed electrician \\navailable for all facility power routing, delivery, and connections. \\n \\nConfirm that electrical power for the GRC Solution be available on the day \\nof installation. \\n \\nIs there a standard power outlet within 50 feet of the point of installation \\nfor power tools, vacuum, etc.? \\n \\n \\n6.0 IT Requirements \\nRequirement \\nResponse \\nWill access be available for remote diagnostics and system alerts? \\n \\nWhere will the Systems Manager be installed? \\n \\nThe Customer is responsible for providing three static IP addresses; one \\nfor the PLC and two for the Systems Manager. \\n \\nThe Customer is responsible for providing and installing networking \\nhardware and cabling. \\n \\nAn IT expert must be available onsite during and after installation to \\nsupport hardware and network setup. \\n \\n \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n6 \\n \\n7.0 Customer Responsibility for Delays \\nDepending on the GRC Solution being installed, the time required can vary \\nfrom a single day for Micros to several days for an ICEraQ Flex, Duo, or \\nQuad. For an ICEraQ Duo, the installation may follow the timeline similar to \\nthe example below. The actual timeline will be coordinated with the GRC \\nInstallation Team: \\n\\uf0b7 Day 1 – Solution inventory and major component placement  \\n\\uf0b7 Day 2 – Installation and coolant fill \\n\\uf0b7 Day 3 – Testing and commissioning \\nFor any delays in the agreed to timeline caused by the customer that \\nextends the installation beyond the mutually agreed to timeline, the \\nCustomer will be billed $1000 per day per resource while onsite, $100 per \\nhour per resource for travel, plus associated travel expenses such as \\nchange fees, etc. \\n \\n \\n \\n \\n \\nGRC Site Readiness Checklist \\n7 \\n \\n8.0 Acceptance \\nThe client signature below verifies that the terms of this Site Readiness Checklist are complete \\nand accurate. GRC reserves the right to charge a reasonable and customary fee for any costs \\nincurred due insufficient or erroneous information supplied on this checklist. \\nThe parties hereto are each acting with proper authority by their respective organizations. \\n \\n \\nGreen Revolution Cooling, Inc. \\nCustomer Name \\n \\n \\n \\n \\n \\n \\n \\n \\nFull name \\n \\nFull name \\n \\n \\n \\n \\n \\n \\n \\nTitle \\n \\nTitle \\n \\n \\n \\n \\n \\nAuthorized Signature \\n \\nAuthorized Signature \\n \\n \\n \\n \\n \\n \\n \\nDate \\n \\nDate \\n \\n \\n \\nGRC Site Readiness Checklist \\n8 \\n \\nREVISION HISTORY \\nOwner\\nGeneral Statement of Change\\nChecked \\nApproved Date \\nVersion\\nDavid \\nRobinson \\nInitial release \\nKevin \\nGoad \\nBill Kribbs \\n06/01/23 \\nA00 \\nDavid \\nRobinson\\nAdded coolant section \\nKevin \\nGoad\\nBill Kribbs\\n12/04/23 \\nA01 \\n \\n', \"The Convection Principle:\\nWhat it is and how it applies to\\nSubmer’s Immersion Cooling Technology \\nWhite paper\\nThermodynamics\\n2  The Convection Principle /  About the author\\nMSc in Aerospace Engineering and Specialist’s Degree in Fluid Mechanic, Jaime \\nstarted his professional career developing aero-engines for ITP, one of the \\nleading companies in the aeronautics sector in Spain.\\nAfter specialising in CFD, he started working for Indra, improving his knowledge \\nof cooling for electronics thanks to the development of air-cooled solutions for \\nradar and electronic countermeasures for fighter aircraft.\\nIn the UK, he worked for the environmental protection team in Rolls-\\nRoyce plc, analysing inclement weather phenomena and its effects in the \\nthermal performance on aero-engines, then as a Team Lead for aero- and \\nthermodynamics in Bombardier Transportation.\\nJaime Pita\\nSenior Thermal Engineer at Submer\\nJaime’s LinkedIn\\nAbout the author\\n3 SECTION // Power Density and Space Efficiency / The 5 pillars to build a Smart Datacenter!\\nAbout the autor\\n1. Introduction\\n2. Operational Driver\\n\\t\\n2.1 Gas, Liquid or Solid?\\t\\n3. Submer’s SmartCoolant\\n4. The Convection Principle\\n\\t\\n4.1 How the Convection Principle Works\\n\\t\\n4.2 Maximizing Convection\\n5. Conclusions\\n6. About Submer\\n2\\n4\\n5\\n5\\n6\\n7\\n8\\n10\\n12\\n12\\nTable of Contents\\n1. Introduction\\nHigh temperatures in electronics have always been a problem for their performance and \\nlifespan. This problem has been dealt with in two ways:\\n•\\t\\nBy means of cooling with the surrounding air by accelerating it (using fans).\\n•\\t\\nAnd by adding some features like channels, heatsinks, thermal paste (and some \\nother elements that have been developed in the past years in order to achieve the \\nmaximum energy dissipation from the components of every motherboard).\\nBut excessive cooling effort has three immediate consequences:\\n1.\\t\\nIt wastes energy.\\n2.\\t It increases costs.\\n3.\\t\\nIt adds the maintenance variable to the equation.\\nStill, it is not a bad approach. \\n“\\nHigh temperatures in electronics have always \\nbeen a problem for their performance and \\nlifespan.\\nUsing the accelerated air to cool down objects is a common experience known to anybody. \\nThink about, for example, when you blow air to cool a spoon of soup that looks definitely \\ntoo hot. You also feel colder on a windy day than on a calm day. In summer, you open up the \\nwindows to create some air currents to make the room more comfortable. The principle is the \\nsame, and we apply it every day.\\n“\\n[...] excessive cooling effort wastes energy, \\nincreases costs and adds the maintenance \\nvariable to the equation.\\nHistorically, for electronics, fans and heatsinks were good enough, but in the present times, \\nmore computation density is needed. The chips are required to operate at their maximum \\nlimit, and one of the known problems is that temperature affects their performance. Those \\nchips can withstand high temperatures, but would not it be great to have them operating in a \\nvery comfortable environment?\\nThis is one of the missions Submer chose to achieve since the Company started the \\nImmersion Cooling venture.\\n4 The Convection Principle /  Introduction\\n2. Operational Driver\\nHow to improve the energy extraction from a starting point that is already pushed to its \\nlimits? The air is well known for having excellent insulating properties and examples are \\npresent in lots of common technologies: double-glazed windows, clothing, food containers, \\ncanteens... The list of objects that use the air for thermal insulation is infinite. So, how can we \\nthink about using an insulating material for cooling down objects when its own properties \\nare not optimal enough?\\n2.1 Gas, Liquid or Solid?\\t\\nThe air is a gas. All substances can be classified as gases, liquids and solids. But also gases \\nand liquids are both fluids, that is, they are made out of molecules that are in constant \\nand random motion colliding with each other. So, when talking about improving thermal \\nproperties the first idea that comes to mind is to substitute air with another fluid capable \\nof extracting the energy more efficiently. Thermally speaking, water is one of the better \\nsubstances we have on Earth for heat dissipation. The problem is that water is not compatible \\nwith electronic devices, leaving that option as an impossible solution.\\n5 The Convection Principle /  Operational Driver\\nSolid\\nLiquid\\nGas\\nFigure 1: Molecules pattern in solid, liquid and gas substances.\\n6 The Convection Principle /  Submer's SmartCoolant\\n3. Submer’s SmartCoolant\\nIn order to solve that problem, Submer has developed a proprietary, synthetic, dielectric \\nfluid, called SmartCoolant. Since the SmartCoolant is a liquid (and not a gas), there is a base \\nimprovement due to the density of the substance (the SmartCoolant has more particles per \\ncubic meter than air to absorb and transport energy). Moreover, the SmartCoolant formula \\nimproves thermal properties compared to the air, and combining it with its dielectric \\nproperties makes it the best solution for cooling electronic devices.\\nThe SmartCoolant has a specific heat higher than air, so it absorbs the thermal energy better \\nthan air. The mechanism of absorbing energy and driving it away from the heat source is \\ncalled convection.\\n“\\nThe mechanism of absorbing energy and \\ndriving it away from the heat source is called \\nconvection.\\nFigure 2: Conduction, con vection and radiation.\\n7 The Convection Principle /  The Convenction Principle\\n4. The Convenction Principle\\nConvection is one of the three mechanisms of heat transfer existing in nature (conduction, \\nconvection and radiation - see figure 2), or means of transporting energy from one zone \\nto another on a specific domain. The convection principle relies on the movement of the \\nparticles of the substance to transport the energy, being this possible only on fluids, where \\nthe particles are able to flow inside the fluid domain. On the other hand, conduction is the \\ntransfer of energy between fixed particles (solids) and radiation is the transmission of energy \\nalong electromagnetic waves or subatomic particles along with vacuum or substances. A \\nheatsink on top of a chip is a good example of:\\n•\\t\\nConduction heat transfer, where the heat is driven away from the chip.\\n•\\t\\nAnd convection, where the heat is transferred to the fluid particles surrounding the \\nheatsink and then transported away from it.\\nAs the fluid is made of particles that touch each other, some conduction will occur but \\nthe predominant effect will be the convection. Radiation can be considered negligible and \\ntherefore applicable to different types of problems.\\n“\\nConvection is one of the three mechanisms of heat \\ntransfer existing in nature (conduction, convection \\nand radiation) or means of transporting energy \\nfrom one zone to another on a specific domain.\\nFigure 3: The conduction and convection principles in a heatsink.\\n4.1 How the Convenction Principle Works\\nAs specified before, the convection principle is related to the capacity of the substance \\nto absorb the energy and hence store it in its particles. Those particles then will move to \\nanother part of the domain and the heat can then be transferred to another system. The \\ncapacity of the substance particles to store a certain amount of energy is called specific heat. \\nThis constant value measures the amount of energy that is needed to increase the substance \\ntemperature by 1 Kelvin of a certain substance quantity (usually 1 kg). The greater the specific \\nheat, the higher capacity a substance - in these cases, a fluid - has to store energy. This \\nmeans that a certain amount of SmartCoolant needs more energy to increase in temperature \\nthan the same amount of air, for example.\\nA good way to visualize this energy storing behaviour is to think of these fluid particles as \\nwaiters of a restaurant carrying a tray filled up with hot drinks. The bigger the tray (specific \\nheat), the better, as this waiter - particle - will be able to carry more energy to different parts \\nof the domain. Liquids are normally better when talking about specific heat, due to the fact \\nthat they are denser than gases. This means that the “tray” used by the SmartCoolant for \\nstoring energy is bigger than the “tray” used by the air. The energy - imagined as hot drinks \\n- is picked up from a table (the solid) by the fluid particles. That is, in essence, heat transfer \\nbetween solids and fluids. In other words, convection.\\n“\\nThe capacity of the substance particles to store a \\ncertain amount of energy is called specific heat.\\n8  The Convection Principle / How the Convenction Principle Works \\nFigure 4: CFD simulations of a Raspberry Pi hardware in air (left) and in SmartCoolant (right) under the same conditions.\\n9  The Convection Principle / How the Convenction Principle Works\\nThe convection principle is the main heat transfer method between solids and fluids. This \\nprinciple is modelled normally following Newton's Law of cooling. This Law is a mathematical \\napproximation and uses a certain parameter called heat transfer coefficient to approximate \\nthe behaviour of the thermal exchange between solid and liquid, along with the area of \\nexchange of thermal energy. The heat transfer coefficient is the parameter that defines the \\namount of energy transferred from the solid to the fluid per a certain area. Newton’s law of \\ncooling can be expressed as:\\nWhere he is the heat transfer coefficient expressed in W/m2K, S is the surface where the heat \\ntransfer occurs and (T-T∞) is the temperature difference between the solid and the fluid. The \\nresulting sign of this difference will give the resulting sign of the heat transfer, meaning that if \\nthe temperature difference is higher than zero the heat transfer will be positive from the solid \\nto the fluid and hence the fluid is extracting heat from the solid. If the opposite occurs, then \\nthe fluid temperature is higher than the solid temperature and therefore the fluid is applying \\nheat to the solid.\\n“\\nThe heat transfer coefficient is the parameter \\nthat defines the amount of energy transferred \\nfrom the solid to the fluid per a certain area.\\n10  The Convection Principle / Maximizing Convection\\n4.2 Maximizing Convection\\nThere are then three ways of maximizing convection:\\n1.\\t\\nThe easiest way is to act on this temperature difference and that is by using the fluid \\nin a cooler condition - if you want to cool down a system - supposing that the solid \\ntemperature is fixed. On the other hand, if the thermal power of the solid is fixed and \\nthe only intention is to cool it down, the resulting temperature will become lower if the \\nfluid temperature is colder. The temperature of the fluid then plays a critical role.\\n2.\\t The second way of maximizing convection is to physically act on the surface for \\nthe thermal exchange. Maximizing the area by modifying the morphology of a solid \\ncomponent is a good way of improving thermal exchange with the fluid. In some \\ncases, this will be impossible and additional highly conductive solids with large areas \\nof exchange need to be used. That is what we call heatsinks. \\nHeatsinks are devices where the convection is maximized by increasing the area of \\nexchange with fins or pins. Adding heatsinks does not act solely on the convection \\nwith the fluid, it also adds a conduction effect so the resultant heat transfer is a \\ncombination of conduction and convection. Acting purely on modifying convection \\nwill imply to modify the geometry of the solid that transfers the energy with the fluid.\\n3.\\t The third way of maximizing the convection mechanism is to change the \\nmorphology of the fluid, that is, selecting another fluid with a greater heat transfer. \\nGases like air usually have very low values of this parameter, whereas liquids present \\nvalues several orders of magnitude greater than gases. As a guideline, the typical \\nvalues for the heat transfer coefficient for different substances are:\\n•\\t\\nFree Convection - air, gases and dry vapours:\\t                \\t he=0.5-1000 W/m2K\\n•\\t\\nFree Convection - water and liquids: \\t \\t\\n              \\t he=50-3000 W/m2K\\n•\\t\\nForced Convection - air, gases and dry vapours: \\t\\nhe=10-1000 W/m2K\\n•\\t\\nForced Convection - water and liquids: \\t\\n\\t\\nhe=50-10000 W/m2K\\n•\\t\\nForced Convection - liquid metals: \\t\\n\\t\\n              \\t he=5000-40000 W/m2K\\n•\\t\\nBoiling Water: \\t\\n\\t\\n\\t\\n\\t\\n\\t\\nhe=3000-100000 W/m2K\\n•\\t\\nCondensing Water Vapor:\\t\\n\\t\\n\\t\\n\\t\\nhe=5000-100000 W/m2K\\nThe SmartCoolant that Submer has developed relies on this concept of substituting the fluid \\nwhere the hot electronic components are. From a base point, liquids have better behaviour \\non convection than gases, so why not use them? Studies over flat plate systems with the \\nSmartCoolant fluid gave some excellent properties, as the base heat transfer coefficient for \\nthe SmartCoolant is around 500 W/m2K (natural convection).\\n11  The Convection Principle / Maximizing Convection\\nFigure 5 shows the typical behaviour for the convection coefficient of the SmartCoolant at \\nlower velocities (left end of the graph) or higher velocities (right end). Compared to air, where \\nthe natural convection heat transfer coefficient seats in values around 0.5, it is obvious that \\nchoosing a liquid solution is way better than a gas solution. \\nOther ways of increasing the heat transfer coefficient is by accelerating the fluid in order to \\nincrease the heat exchange. In figure 5, this behaviour will be located in the right end of the \\nchart, where the fluid has been accelerated and its capability for extracting heat increases. A \\ngood example of this behaviour is the historical mechanism of cooling IT hardware with fans \\nand narrow channels to accelerate the air.\\nFigure 5: SmartCoolant heat transfer coefficient.\\n12  The Convection Principle / Conclusions\\n5. Conclusions\\nUsing the SmartCoolant for thermal management truly represents a game-changing approach \\nfor the electronics industry. There are a lot of different solutions in the electronics industry for \\ncooling down microchips and other components, but some of them are being pushed beyond \\ntheir limits while the market keeps demanding for more efficient solutions. Plus, the design \\nof heatsinks to optimise air heat transfer is expensive and in some cases, inefficient, due to \\nthe insulating nature of the air itself. The SmartCoolant gives the opportunity to improve this \\nscenario by avoiding the use of accelerated cooling flow - no fans are needed - since the base \\nheat transfer coefficient of the fluid is way higher than air. Also, a wide range of possibilities \\nare available for the user if more cooling is needed, since the SmartCoolant gains thermal \\npower extraction when accelerated, opening new scenarios to be explored.\\n6. About Submer\\nIn 2015, a team of industry forward-thinkers and innovation visionaries felt that it was time for \\nthe datacenter industry to turn over a new leaf. The idea was to challenge the way datacenters \\nare perceived and understood today and how this can create deep change in the way \\ntechnology and humans behave.\\nSubmer (https://submer.com) was born to pave the way towards next generation datacenters. \\nWe design, build and install solutions for HPC, hyperscaler, datacenters, Edge, AI, DL and \\nblockchain applications. \\nAt Submer, we believe that innovation can and must be sustainable. Every day we work to find \\nthe best solutions to make operating and constructing datacenters and supercomputers as \\nefficient as possible and to have little or positive impact on the environment around them \\n(reducing their footprint and their consumption of precious resources such as water). \\n13 SECTION // Power Density and Space Efficiency / The 5 pillars to build a Smart Datacenter!\\nAre you planning to make your \\ndatacenter smarter? \\nLet’s do it together!\\n Book a 30 minutes call with us and \\nwe’ll analyze and draw together \\nyour SmartDC strategy!\\nhttps://submer.com/book-a-call\\nBOOK A CALL\\n\", 'Castrol ON DC 15\\nSAFETY DATA SHEET\\nProduct name\\n1.1 Product identifier\\n1.3 Details of the supplier of the safety data sheet\\nLiquid.\\nProduct type\\nE-mail address\\nMSDSadvice@bp.com\\n1.2 Relevant identified uses of the substance or mixture and uses advised against\\nSECTION 1: Identification of the substance/mixture and of the company/undertaking\\nProduct code\\nVP1-5000-06-01..\\n1.4 Emergency telephone number\\nEMERGENCY \\nTELEPHONE NUMBER\\n+44 (0)118 9843311\\nSupplier\\nCastrol International\\nTechnology Centre\\nWhitchurch Hill\\nPangbourne\\nReading\\nBerkshire RG8 7QR\\nUnited Kingdom\\nTel.:  +44 (0)118 9843311\\nFax.:  +44 (0)118 9845254\\nSDS #\\nVP1-5000-06-01..\\nUse of the substance/\\nmixture\\nThermal Management Fluid\\nFor R&D use only.\\nThis product is supplied for trial/research purposes only.  Although the necessary hazard \\nassessment has been carried out prior to the release of the experimental formulation, the \\nassessment may be subject to change during the course of further evaluation. The product \\nshould therefore be handled with care and exposure kept to a minimum.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSee sections 11 and 12 for more detailed information on health effects and symptoms and environmental hazards.\\nClassification according to Regulation (EC) No. 1272/2008 [CLP/GHS]\\nSECTION 2: Hazards identification\\n2.1 Classification of the substance or mixture\\nProduct definition\\nMixture\\nSee Section 16 for the full text of the H statements declared above.\\n2.2 Label elements\\nHazard pictograms\\nSignal word\\nHazard statements\\nPrevention\\nPrecautionary statements\\nResponse\\nDanger\\nH304 - May be fatal if swallowed and enters airways.\\nNot applicable.\\nP301 + P310, P331 - IF SWALLOWED: Immediately call a POISON CENTER or physician.  Do \\nNOT induce vomiting.\\nAsp. Tox. 1, H304\\nGeneral\\nP102 - Keep out of reach of children.\\nP101 - If medical advice is needed, have product container or label at hand.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 1/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 2: Hazards identification\\nOther hazards which do \\nnot result in classification\\nDefatting to the skin.\\nContact with hot product may cause burns.\\nStorage\\nDisposal\\nP405 - Store locked up.\\nP501 - Dispose of contents and container in accordance with all local, regional, national and \\ninternational regulations.\\nSupplemental label \\nelements\\nContainers to be fitted \\nwith child-resistant \\nfastenings\\nYes, applicable.\\nTactile warning of danger\\nYes, applicable.\\nNot applicable.\\nSpecial packaging requirements\\nHazardous ingredients\\nLubricating oils (petroleum), C15-30, hydrotreated neutral oil-based\\n2.3 Other hazards\\nEU Regulation (EC) No. 1907/2006 (REACH)\\nAnnex XVII - Restrictions \\non the manufacture,\\nplacing on the market \\nand use of certain \\ndangerous substances,\\nmixtures and articles\\nNot applicable.\\nResults of PBT and vPvB \\nassessment\\nProduct does not meet the criteria for PBT or vPvB according to Regulation (EC) No. 1907/2006,\\nAnnex XIII.\\nProduct meets the criteria \\nfor PBT or vPvB according \\nto Regulation (EC) No.\\n1907/2006, Annex XIII\\nThis mixture does not contain any substances that are assessed to be a PBT or a vPvB.\\nSECTION 3: Composition/information on ingredients\\nType\\nSynthetic lubricant Proprietary performance additives.\\nOccupational exposure limits, if available, are listed in Section 8.\\nSee Section 16 for the full text of the H statements declared above.\\nMixture\\n3.2 Mixtures\\nProduct definition\\nLubricating oils (petroleum),\\nC15-30, hydrotreated neutral \\noil-based\\nREACH #:\\n01-2119474878-16\\nEC: 276-737-9\\nCAS: 72623-86-0\\nIndex: 649-482-00-X\\n≥90\\nAsp. Tox. 1, H304\\n-\\n[1]\\n[1] Substance classified with a health or environmental hazard\\nProduct/ingredient name\\nIdentifiers\\n%\\nClassification\\nSpecific Conc.\\nLimits, M-factors \\nand ATEs\\nType\\nDo not induce vomiting.  Never give anything by mouth to an unconscious person.  If \\nunconscious, place in recovery position and get medical attention immediately.  Aspiration \\nhazard if swallowed.  Can enter lungs and cause damage.  Get medical attention immediately.\\nHot product - Flood with water to dissipate heat.  In the event of any product remaining, do not \\ntry to remove it other than by continued irrigation with water.  Obtain medical attention \\nimmediately.\\nCold product - Wash eye thoroughly with copious quantities of water, ensuring eyelids are held \\nopen.  Obtain medical advice if any pain or redness develops or persists.\\n4.1 Description of first aid measures\\nIf inhaled, remove to fresh air.  Get medical attention if symptoms occur.\\nIngestion\\nInhalation\\nEye contact\\nSECTION 4: First aid measures\\nSkin contact\\nHot Product - Flood skin with cold water to dissipate heat, cover with clean cotton or gauze,\\nobtain medical advice immediately.\\nCold Product - Wash contaminated skin with soap and water.  Remove contaminated clothing \\nand wash underlying skin as soon as reasonably practicable.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 2/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 4: First aid measures\\nNotes to physician\\nTreatment should in general be symptomatic and directed to relieving any effects.\\nProduct can be aspirated on swallowing or following regurgitation of stomach contents, and can \\ncause severe and potentially fatal chemical pneumonitis, which will require urgent treatment.\\nBecause of the risk of aspiration, induction of vomiting and gastric lavage should be avoided.\\nGastric lavage should be undertaken only after endotracheal intubation.  Monitor for cardiac \\ndysrhythmias.\\nProtection of first-aiders\\nNo action shall be taken involving any personal risk or without suitable training.  It may be \\ndangerous to the person providing aid to give mouth-to-mouth resuscitation.\\n4.2 Most important symptoms and effects, both acute and delayed\\n4.3 Indication of any immediate medical attention and special treatment needed\\nSee Section 11 for more detailed information on health effects and symptoms.\\nPotential acute health effects\\nInhalation\\nVapour inhalation under ambient conditions is not normally a problem due to low vapour \\npressure.\\nAspiration hazard if swallowed -- harmful or fatal if liquid is aspirated into lungs.\\nIngestion\\nSkin contact\\nDefatting to the skin.  May cause skin dryness and irritation.\\nNo known significant effects or critical hazards.\\nEye contact\\nDelayed and immediate effects as well as chronic effects from short and long-term exposure\\nInhalation\\nIngestion\\nSkin contact\\nEye contact\\nOverexposure to the inhalation of airborne droplets or aerosols may cause irritation of the \\nrespiratory tract.\\nIngestion of large quantities may cause nausea and diarrhoea.\\nProlonged or repeated contact can defat the skin and lead to irritation and/or dermatitis.\\nPotential risk of transient stinging or redness if accidental eye contact occurs.\\nNo action shall be taken involving any personal risk or without suitable training.  Promptly \\nisolate the scene by removing all persons from the vicinity of the incident if there is a fire.\\nHazardous combustion \\nproducts\\nHazards from the \\nsubstance or mixture\\nCombustion products may include the following:\\ncarbon oxides (CO, CO2) (carbon monoxide, carbon dioxide)\\nDuring use heat transfer oils may be thermally degraded leading to the formation of volatile \\nhydrocarbons with flash points considerably lower than the original product. It is therefore \\nessential that the system is not drained while hot unless an inert gas system is used to displace \\nflammable gaseous residues.  Adequate ventilation is essential during draining operations as \\nhot oil will fume.\\nThe temperature at which spent product is drained is a compromise between the need to have \\nthe oil sufficiently hot to facilitate drainage, the need to avoid fuming and the dangers of fire \\nfrom degraded oil with a low flash point.  It is recommended therefore that spent oil is drained \\nat a temperature of less than 100°C.  During system filling and venting, care should be taken to \\nensure that hot oil is not pumped through the expansion tank.  A failure to prevent this could,\\nunder certain conditions, lead to the creation of a flammable atmosphere in the expansion tank.\\nAs the expansion tank is being filled it is essential that the gases and vapours formed should be \\nfree to vent to an open atmosphere where they can quickly disperse.  Oil soaked lagging may \\nspontaneously ignite and should be replaced by fresh lagging as soon as possible.  Product \\ncontaminated rags, paper or material used to absorb spillages, represent a fire hazard, and \\nshould not be allowed to accumulate.  Dispose of safely immediately after use.  In a fire or if \\nheated, a pressure increase will occur and the container may burst.\\nFire-fighters should wear appropriate protective equipment and self-contained breathing \\napparatus (SCBA) with a full face-piece operated in positive pressure mode.  Clothing for fire-\\nfighters (including helmets, protective boots and gloves) conforming to European standard EN \\n469 will provide a basic level of protection for chemical incidents.\\nSpecial protective \\nequipment for fire-fighters\\nIn case of fire, use foam, dry chemical or carbon dioxide extinguisher or spray.\\n5.1 Extinguishing media\\nDo not use water jet.  The use of a water jet may cause the fire to spread by splashing the \\nburning product.\\nSuitable extinguishing \\nmedia\\nUnsuitable extinguishing \\nmedia\\nSECTION 5: Firefighting measures\\n5.2 Special hazards arising from the substance or mixture\\n5.3 Advice for firefighters\\nSpecial precautions for \\nfire-fighters\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 3/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\n6.2 Environmental \\nprecautions\\nStop leak if without risk.  Move containers from spill area.  Approach the release from upwind.\\nPrevent entry into sewers, water courses, basements or confined areas.  Contain and collect \\nspillage with non-combustible, absorbent material e.g. sand, earth, vermiculite or diatomaceous \\nearth and place in container for disposal according to local regulations.  Contaminated \\nabsorbent material may pose the same hazard as the spilt product.  Dispose of via a licensed \\nwaste disposal contractor.\\nAvoid dispersal of spilt material and runoff and contact with soil, waterways, drains and sewers.\\nInform the relevant authorities if the product has caused environmental pollution (sewers,\\nwaterways, soil or air).\\nLarge spill\\nStop leak if without risk.  Move containers from spill area.  Absorb with an inert material and \\nplace in an appropriate waste disposal container.  Dispose of via a licensed waste disposal \\ncontractor.\\nSmall spill\\n6.3 Methods and material for containment and cleaning up\\nSECTION 6: Accidental release measures\\n6.1 Personal precautions, protective equipment and emergency procedures\\nFor non-emergency \\npersonnel\\nFor emergency responders\\n6.4 Reference to other \\nsections\\nSee Section 1 for emergency contact information.\\nSee Section 5 for firefighting measures.\\nSee Section 8 for information on appropriate personal protective equipment.\\nSee Section 12 for environmental precautions.\\nSee Section 13 for additional waste treatment information.\\nContact emergency personnel.  No action shall be taken involving any personal risk or without \\nsuitable training.  Evacuate surrounding areas.  Keep unnecessary and unprotected personnel \\nfrom entering.  Do not touch or walk through spilt material.  Floors may be slippery; use care to \\navoid falling.  Avoid breathing vapour or mist.  Provide adequate ventilation.  Put on appropriate \\npersonal protective equipment.\\nEntry into a confined space or poorly ventilated area contaminated with vapour, mist or fume is \\nextremely hazardous without the correct respiratory protective equipment and a safe system of \\nwork.  Wear self-contained breathing apparatus.  Wear a suitable chemical protective suit.\\nChemical resistant boots.  See also the information in \"For non-emergency personnel\".\\nStore in accordance with local regulations.  Store in a dry, cool and well-ventilated area, away \\nfrom incompatible materials (see Section 10).  Store locked up.  Keep away from heat and \\ndirect sunlight.  Keep container tightly closed and sealed until ready for use.  Containers that \\nhave been opened must be carefully resealed and kept upright to prevent leakage.  Store and \\nuse only in equipment/containers designed for use with this product.  Do not store in unlabelled \\ncontainers.\\nSECTION 7: Handling and storage\\n7.1 Precautions for safe handling\\nProtective measures\\nAdvice on general \\noccupational hygiene\\n7.2 Conditions for safe \\nstorage, including any \\nincompatibilities\\n7.3 Specific end use(s)\\nRecommendations\\nPut on appropriate personal protective equipment.  Do not swallow.  Aspiration hazard if \\nswallowed. Can enter lungs and cause damage.  Never siphon by mouth.  Avoid contact with \\neyes, skin and clothing.  Avoid breathing vapour or mist.  Keep in the original container or an \\napproved alternative made from a compatible material, kept tightly closed when not in use.  Do \\nnot reuse container.  Empty containers retain product residue and can be hazardous.\\nEating, drinking and smoking should be prohibited in areas where this material is handled,\\nstored and processed.  Wash thoroughly after handling.  Remove contaminated clothing and \\nprotective equipment before entering eating areas.  See also Section 8 for additional \\ninformation on hygiene measures.\\nNot suitable\\nAvoid significant changes in temperature to prevent humidity ingress.\\nSee section 1.2 and Exposure scenarios in annex, if applicable.\\nSECTION 8: Exposure controls/personal protection\\n8.1 Control parameters\\nOccupational exposure limits\\nNo exposure limit value known.\\nWhilst specific OELs for certain components may be shown in this section, other components may be present in any mist,\\nvapour or dust produced. Therefore, the specific OELs may not be applicable to the product as a whole and are provided for \\nguidance only.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 4/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 8: Exposure controls/personal protection\\nHand protection\\nRespiratory protective equipment is not normally required where there is adequate natural or \\nlocal exhaust ventilation to control exposure.\\nRespiratory protective equipment must be checked to ensure it fits correctly each time it is worn.\\nIn case of insufficient ventilation, wear suitable respiratory equipment.\\nProvided an air-filtering/air-purifying respirator is suitable, a filter for particulates can be used.\\nUse filter type P or comparable standard.\\nAir-filtering respirators, also called air-purifying respirators, will not be adequate under \\nconditions of oxygen deficiency (i.e. low oxygen concentration), and would not be considered \\nsuitable where airborne concentrations of chemicals with a significant hazard are present.  In \\nthese cases air-supplied breathing apparatus will be required.\\nA combination filter for particles, organic gases and vapours (boiling point >65°C) may be \\nrequired if mist or fume is present as well as vapour. Use filter type AP or comparable standard.\\nApproved air-supplied breathing apparatus must be worn where there is a risk of exceeding the \\nexposure limit of carbon monoxide\\nApproved air-supplied breathing apparatus must be worn where there is a risk of exposure to \\nhazardous combustion and thermal decomposition products.\\nEntry into a confined space or poorly ventilated area contaminated with vapour, mist or fume is \\nextremely hazardous without the correct respiratory protective equipment and a safe system of \\nwork.\\nThe correct choice of respiratory protection depends upon the chemicals being handled, the \\nconditions of work and use, and the condition of the respiratory equipment. Safety procedures \\nshould be developed for each intended application. Respiratory protection equipment should \\ntherefore be chosen in consultation with the supplier/manufacturer and with a full assessment \\nof the working conditions.\\nGeneral Information:\\nBecause specific work environments and material handling practices vary, safety procedures \\nshould be developed for each intended application. The correct choice of protective gloves \\ndepends upon the chemicals being handled, and the conditions of work and use. Most gloves \\nprovide protection for only a limited time before they must be discarded and replaced (even the \\nbest chemically resistant gloves will break down after repeated chemical exposures).\\nGloves should be chosen in consultation with the supplier / manufacturer and taking account of \\nHot material: to prevent thermal burns wear a helmet, full face visor and heat resistant neck \\nflap / apron.\\nCold material: wear safety glasses with side shields.\\nEye/face protection\\nRespiratory protection\\nSkin protection\\nAppropriate engineering \\ncontrols\\nProvide exhaust ventilation or other engineering controls to keep the relevant airborne \\nconcentrations below their respective occupational exposure limits.\\nAll activities involving chemicals should be assessed for their risks to health, to ensure \\nexposures are adequately controlled. Personal protective equipment should only be considered \\nafter other forms of control measures (e.g. engineering controls) have been suitably evaluated.\\nPersonal protective equipment should conform to appropriate standards, be suitable for use, be \\nkept in good condition and properly maintained.\\nYour supplier of personal protective equipment should be consulted for advice on selection and \\nappropriate standards.  For further information contact your national organisation for standards.\\nThe final choice of protective equipment will depend upon a risk assessment. It is important to \\nensure that all items of personal protective equipment are compatible.\\nWash hands, forearms and face thoroughly after handling chemical products, before eating,\\nsmoking and using the lavatory and at the end of the working period.  Ensure that eyewash \\nstations and safety showers are close to the workstation location.\\n8.2 Exposure controls\\nHygiene measures\\nNo DNELs/DMELs available.\\nPredicted No Effect Concentration\\nNo PNECs available\\nDerived No Effect Level\\nIndividual protection measures\\nRecommended monitoring \\nprocedures\\nReference should be made to monitoring standards, such as the following:  European Standard \\nEN 689 (Workplace atmospheres - Guidance for the assessment of exposure by inhalation to \\nchemical agents for comparison with limit values and measurement strategy)  European \\nStandard EN 14042 (Workplace atmospheres - Guide for the application and use of procedures \\nfor the assessment of exposure to chemical and biological agents)  European Standard EN 482 \\n(Workplace atmospheres - General requirements for the performance of procedures for the \\nmeasurement of chemical agents)  Reference to national guidance documents for methods for \\nthe determination of hazardous substances will also be required.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 5/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 8: Exposure controls/personal protection\\na full assessment of the working conditions.\\nHot material: to prevent thermal burns wear heat resistant and impervious gauntlets/gloves.\\nCold material: Wear chemical resistant gloves.  Recommended: nitrile gloves.\\nBreakthrough time:\\nBreakthrough time data are generated by glove manufacturers under laboratory test conditions \\nand represent how long a glove can be expected to provide effective permeation resistance. It \\nis important when following breakthrough time recommendations that actual workplace \\nconditions are taken into account. Always consult with your glove supplier for up-to-date \\ntechnical information on breakthrough times for the recommended glove type.\\nOur recommendations on the selection of gloves are as follows:\\nContinuous contact:\\nGloves with a minimum breakthrough time of 240 minutes, or >480 minutes if suitable gloves \\ncan be obtained.\\nIf suitable gloves are not available to offer that level of protection, gloves with shorter \\nbreakthrough times may be acceptable as long as appropriate glove maintenance and \\nreplacement regimes are determined and adhered to.\\nShort-term / splash protection:\\nRecommended breakthrough times as above.\\nIt is recognised that for short-term, transient exposures, gloves with shorter breakthrough times \\nmay commonly be used. Therefore, appropriate maintenance and replacement regimes must \\nbe determined and rigorously followed.\\nGlove Thickness:\\nFor general applications, we recommend gloves with a thickness typically greater than 0.35 mm.\\nIt should be emphasised that glove thickness is not necessarily a good predictor of glove \\nresistance to a specific chemical, as the permeation efficiency of the glove will be dependent \\non the exact composition of the glove material. Therefore, glove selection should also be based \\non consideration of the task requirements and knowledge of breakthrough times.\\nGlove thickness may also vary depending on the glove manufacturer, the glove type and the \\nglove model. Therefore, the manufacturers’ technical data should always be taken into account \\nto ensure selection of the most appropriate glove for the task.\\nNote: Depending on the activity being conducted, gloves of varying thickness may be required \\nfor specific tasks. For example:\\n  • Thinner gloves (down to 0.1 mm or less) may be required where a high degree of manual \\ndexterity is needed. However, these gloves are only likely to give short duration protection and \\nwould normally be just for single use applications, then disposed of.\\n  • Thicker gloves (up to 3 mm or more) may be required where there is a mechanical (as well \\nas a chemical) risk i.e. where there is abrasion or puncture potential.\\nUse of protective clothing is good industrial practice.\\nPersonal protective equipment for the body should be selected based on the task being \\nperformed and the risks involved and should be approved by a specialist before handling this \\nproduct.\\nCotton or polyester/cotton overalls will only provide protection against light superficial \\ncontamination that will not soak through to the skin.  Overalls should be laundered on a regular \\nbasis.  When the risk of skin exposure is high (e.g. when cleaning up spillages or if there is a \\nrisk of splashing) then chemical resistant aprons and/or impervious chemical suits and boots \\nwill be required.\\nSkin and body\\nThermal hazards\\nWear impervious and heat resistant coveralls covering the full body and limbs.  Cotton or \\npolyester/cotton overalls will only provide protection against light superficial contamination that \\nwill not soak through to the skin.  Overalls should be laundered on a regular basis.  When the \\nrisk of skin exposure is high (e.g. when cleaning up spillages or if there is a risk of splashing)\\nthen chemical resistant aprons and/or impervious chemical suits and boots will be required.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 6/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 8: Exposure controls/personal protection\\nEnvironmental exposure \\ncontrols\\nEmissions from ventilation or work process equipment should be checked to ensure they \\ncomply with the requirements of environmental protection legislation.  In some cases, fume \\nscrubbers, filters or engineering modifications to the process equipment will be necessary to \\nreduce emissions to acceptable levels.\\nRefer to standards:\\nRespiratory protection: EN 529\\nGloves: EN 420, EN 374\\nEye protection: EN 166\\nFiltering half-mask: EN 149\\nFiltering half-mask with valve: EN 405\\nHalf-mask: EN 140 plus filter\\nFull-face mask: EN 136 plus filter\\nParticulate filters: EN 143\\nGas/combined filters: EN 14387\\nPhysical state\\nMelting point/freezing point\\nInitial boiling point and boiling \\nrange\\nVapour pressure\\nRelative density\\nRelative vapour density\\nLiquid.\\nNot available.\\nNot available.\\nNot available.\\nNot available.\\nOdour\\npH\\nColourless.\\nColour\\nEvaporation rate\\nNot available.\\nAuto-ignition temperature\\nFlash point\\nNot available.\\nNot available.\\nNot applicable.\\nNot available.\\nViscosity\\nKinematic: 7.6 mm2/s (7.6 cSt) at 40°C\\nKinematic: 2.2 mm2/s (2.2 cSt) at 100°C \\nNot available.\\nOdour threshold\\nPartition coefficient: n-octanol/\\nwater\\nExplosive properties\\nNot available.\\nOxidising properties\\n9.1 Information on basic physical and chemical properties\\nAppearance\\n9.2 Other information\\nDecomposition temperature\\nNot available.\\nSECTION 9: Physical and chemical properties\\nFlammability (solid, gas)\\nNot available.\\nNo additional information.\\nDensity\\n836 kg/m³ (0.836 g/cm³) at 15°C\\nParticle characteristics\\nMedian particle size\\nNot applicable.\\nThe conditions of measurement of all properties are at standard temperature and pressure unless otherwise indicated.\\nIngredient name\\nVapour Pressure at 20˚C\\nVapour pressure at 50˚C\\nmm Hg kPa\\nMethod\\nmm \\nHg\\nkPa\\nMethod\\nLubricating oils \\n(petroleum), C15-30,\\nhydrotreated neutral oil-\\nbased\\n<0.08\\n<0.011\\nASTM D 5191\\nNot available.\\nOpen cup: >180°C (>356°F) [Cleveland]\\nNot available.\\nSolubility(ies)\\nNot available.\\nLower and upper explosion \\nlimit\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 7/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\n10.6 Hazardous \\ndecomposition products\\n10.4 Conditions to avoid\\nAvoid all possible sources of ignition (spark or flame).\\nUnder normal conditions of storage and use, hazardous decomposition products should not be \\nproduced.\\nThe product is stable.\\n10.2 Chemical stability\\n10.5 Incompatible materials\\n10.3 Possibility of \\nhazardous reactions\\nUnder normal conditions of storage and use, hazardous reactions will not occur.\\nUnder normal conditions of storage and use, hazardous polymerisation will not occur.\\nSECTION 10: Stability and reactivity\\n10.1 Reactivity\\nNo specific test data available for this product.  Refer to Conditions to avoid and Incompatible \\nmaterials for additional information.\\nReactive or incompatible with the following materials: oxidising materials.\\nPotential chronic health effects\\nPotential acute health effects\\nInhalation\\nVapour inhalation under ambient conditions is not normally a problem due to low vapour \\npressure.\\nAspiration hazard if swallowed -- harmful or fatal if liquid is aspirated into lungs.\\nIngestion\\nSkin contact\\nDefatting to the skin.  May cause skin dryness and irritation.\\nNo known significant effects or critical hazards.\\nEye contact\\nNo known significant effects or critical hazards.\\nGeneral\\nNo known significant effects or critical hazards.\\nCarcinogenicity\\nNo known significant effects or critical hazards.\\nMutagenicity\\nDevelopmental effects\\nNo known significant effects or critical hazards.\\nFertility effects\\nNo known significant effects or critical hazards.\\nSymptoms related to the physical, chemical and toxicological characteristics\\nSkin contact\\nIngestion\\nInhalation\\nNo specific data.\\nAdverse symptoms may include the following:\\nnausea or vomiting\\nAdverse symptoms may include the following:\\nirritation\\ndryness\\ncracking\\nEye contact\\nNo specific data.\\nRoutes of entry anticipated: Dermal, Inhalation, Eyes.\\nSECTION 11: Toxicological information\\nInformation on likely \\nroutes of exposure\\nDelayed and immediate effects as well as chronic effects from short and long-term exposure\\nInhalation\\nIngestion\\nSkin contact\\nEye contact\\nOverexposure to the inhalation of airborne droplets or aerosols may cause irritation of the \\nrespiratory tract.\\nIngestion of large quantities may cause nausea and diarrhoea.\\nProlonged or repeated contact can defat the skin and lead to irritation and/or dermatitis.\\nPotential risk of transient stinging or redness if accidental eye contact occurs.\\nAcute toxicity estimates\\nNot available.\\n11.1  Information on hazard classes as defined in Regulation (EC) No 1272/2008\\n11.2.2 Other information\\nNot available.\\n11.2 Information on other hazards\\n11.2.1 Endocrine disrupting properties\\nNot available.\\nRemarks - Endocrine \\ndisruptor - Health\\nNot available.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 8/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nMobility\\nSpillages may penetrate the soil causing ground water contamination.\\n12.3 Bioaccumulative potential\\n12.1 Toxicity\\n12.2 Persistence and degradability\\nSECTION 12: Ecological information\\n12.4 Mobility in soil\\nSoil/water partition \\ncoefficient (KOC)\\nNot available.\\n12.5 Results of PBT and vPvB assessment\\nExpected to be biodegradable.\\nThis product is not expected to bioaccumulate through food chains in the environment.\\nEnvironmental hazards\\nNot classified as dangerous\\nProduct does not meet the criteria for PBT or vPvB according to Regulation (EC) No. 1907/2006, Annex XIII.\\nSpills may form a film on water surfaces causing physical damage to organisms. Oxygen \\ntransfer could also be impaired.\\nOther ecological information\\nNot available.\\nRemarks - Endocrine \\ndisruptor - Environment\\nNot available.\\n12.7 Other adverse effects\\nNo known significant effects or critical hazards.\\n12.6 Endocrine disrupting \\nproperties\\nEuropean waste catalogue (EWC)\\nYes.\\nHazardous waste\\nWhere possible, arrange for product to be recycled.  Dispose of via an authorised person/\\nlicensed waste disposal contractor in accordance with local regulations.\\nMethods of disposal\\nSECTION 13: Disposal considerations\\n13.1 Waste treatment methods\\nProduct\\nPackaging\\nWaste code\\nWaste designation\\nMethods of disposal\\nSpecial precautions\\n13 02 06*\\nsynthetic engine, gear and lubricating oils\\nWhere possible, arrange for product to be recycled.  Dispose of via an authorised person/\\nlicensed waste disposal contractor in accordance with local regulations.\\nThis material and its container must be disposed of in a safe way.  Care should be taken when \\nhandling emptied containers that have not been cleaned or rinsed out.  Empty containers or \\nliners may retain some product residues.  Empty containers represent a fire hazard as they may \\ncontain flammable product residues and vapour. Never weld, solder or braze empty containers.\\nAvoid dispersal of spilt material and runoff and contact with soil, waterways, drains and sewers.\\nHowever, deviation from the intended use and/or the presence of any potential contaminants may require an alternative waste \\ndisposal code to be assigned by the end user.\\nReferences\\nCommission 2014/955/EU \\nDirective 2008/98/EC\\n-\\n-\\n-\\n-\\n-\\n-\\nNot regulated.\\nNot regulated.\\nNot regulated.\\nSECTION 14: Transport information\\nADR/RID\\nIMDG\\nIATA\\n14.2 UN proper \\nshipping name\\n14.3 Transport \\nhazard class(es)\\nADN\\nNot regulated.\\n-\\n-\\n14.1  UN number \\nor ID number\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 9/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 14: Transport information\\n-\\n-\\n-\\n-\\n-\\n-\\n14.4 Packing \\ngroup\\nAdditional \\ninformation\\n14.5 \\nEnvironmental \\nhazards\\nNo.\\nNo.\\nNo.\\nNot available.\\n14.6 Special precautions for \\nuser\\nNot available.\\n-\\nNo.\\n-\\n14.7 Maritime transport in \\nbulk according to IMO \\ninstruments\\nOther regulations\\nREACH Status\\nFor the REACH status of this product please consult your company contact, as identified in \\nSection 1.\\nSECTION 15: Regulatory information\\n15.1 Safety, health and environmental regulations/legislation specific for the substance or mixture\\nEU Regulation (EC) No. 1907/2006 (REACH)\\nAnnex XIV - List of substances subject to authorisation\\nSubstances of very high concern\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nFor R&D use only.\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nUnited States inventory \\n(TSCA 8b)\\nAustralia inventory (AIIC)\\nCanada inventory\\nChina inventory (IECSC)\\nJapan inventory (CSCL)\\nKorea inventory (KECI)\\nPhilippines inventory \\n(PICCS)\\nTaiwan Chemical \\nSubstances Inventory \\n(TCSI)\\nAll components are listed or exempted.\\nOzone depleting substances (1005/2009/EU)\\nNot listed.\\nPrior Informed Consent (PIC) (649/2012/EU)\\nNot listed.\\nSeveso Directive\\nThis product is not controlled under the Seveso Directive.\\nNone of the components are listed.\\nAnnex XIV\\nEU - Water framework directive - Priority substances\\nNone of the components are listed.\\nEU Regulation (EC) No. 1907/2006 (REACH)\\nAnnex XVII - Restrictions \\non the manufacture,\\nplacing on the market \\nand use of certain \\ndangerous substances,\\nmixtures and articles\\nNot applicable.\\nPersistent Organic Pollutants\\nNot listed.\\nNone of the components are listed.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 10/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 15: Regulatory information\\n15.2 Chemical safety \\nassessment\\nA Chemical Safety Assessment has been carried out for one or more of the substances within \\nthis mixture. A Chemical Safety Assessment has not been carried out for the mixture itself.\\nSECTION 16: Other information\\nAbbreviations and acronyms\\nADN = European Provisions concerning the International Carriage of Dangerous Goods by \\nInland Waterway\\nADR = The European Agreement concerning the International Carriage of Dangerous Goods by \\nRoad\\nATE = Acute Toxicity Estimate\\nBCF = Bioconcentration Factor\\nCAS = Chemical Abstracts Service\\nCLP = Classification, Labelling and Packaging Regulation [Regulation (EC) No. 1272/2008]\\nCSA = Chemical Safety Assessment\\nCSR = Chemical Safety Report\\nDMEL = Derived Minimal Effect Level\\nDNEL = Derived No Effect Level\\nEINECS = European Inventory of Existing Commercial chemical Substances\\nES = Exposure Scenario\\nEUH statement = CLP-specific Hazard statement\\nEWC = European Waste Catalogue\\nGHS = Globally Harmonized System of Classification and Labelling of Chemicals\\nIATA = International Air Transport Association\\nIBC = Intermediate Bulk Container\\nIMDG = International Maritime Dangerous Goods\\nLogPow = logarithm of the octanol/water partition coefficient\\nMARPOL = International Convention for the Prevention of Pollution From Ships, 1973 as \\nmodified by the Protocol of 1978. (\"Marpol\" = marine pollution)\\nOECD = Organisation for Economic Co-operation and Development\\nPBT = Persistent, Bioaccumulative and Toxic\\nPNEC = Predicted No Effect Concentration\\nREACH = Registration, Evaluation, Authorisation and Restriction of Chemicals Regulation \\n[Regulation (EC) No. 1907/2006]\\nRID = The Regulations concerning the International Carriage of Dangerous Goods by Rail\\nRRN = REACH Registration Number\\nSADT = Self-Accelerating Decomposition Temperature\\nSVHC = Substances of Very High Concern\\nSTOT-RE = Specific Target Organ Toxicity - Repeated Exposure\\nSTOT-SE = Specific Target Organ Toxicity - Single Exposure\\nTWA = Time weighted average\\nUN = United Nations\\nUVCB = Complex hydrocarbon substance\\nVOC = Volatile Organic Compound\\nvPvB = Very Persistent and Very Bioaccumulative\\nVaries = may contain one or more of the following 64741-88-4 / RRN 01-2119488706-23,\\n64741-89-5 / RRN 01-2119487067-30, 64741-95-3 / RRN 01-2119487081-40, 64741-96-4/ RRN \\n01-2119483621-38, 64742-01-4 / RRN 01-2119488707-21, 64742-44-5 / RRN \\n01-2119985177-24, 64742-45-6, 64742-52-5 / RRN 01-2119467170-45, 64742-53-6 / RRN \\n01-2119480375-34, 64742-54-7 / RRN 01-2119484627-25, 64742-55-8 / RRN \\n01-2119487077-29, 64742-56-9 / RRN 01-2119480132-48, 64742-57-0 / RRN \\n01-2119489287-22, 64742-58-1, 64742-62-7 / RRN 01-2119480472-38, 64742-63-8,\\n64742-65-0 / RRN 01-2119471299-27, 64742-70-7 / RRN 01-2119487080-42, 72623-85-9 /\\nRRN 01-2119555262-43, 72623-86-0 / RRN 01-2119474878-16, 72623-87-1 / RRN \\n01-2119474889-13\\nProcedure used to derive the classification according to Regulation (EC) No. 1272/2008 [CLP/GHS]\\nClassification\\nJustification\\nAsp. Tox. 1, H304\\nCalculation method\\nFull text of abbreviated H \\nstatements\\nH225\\nHighly flammable liquid and vapour.\\nH301\\nToxic if swallowed.\\nH304\\nMay be fatal if swallowed and enters airways.\\nH311\\nToxic in contact with skin.\\nH331\\nToxic if inhaled.\\nH370\\nCauses damage to organs.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 11/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 16: Other information\\nDate of issue/ Date of \\nrevision\\nNotice to reader\\nDate of previous issue\\nAll reasonably practicable steps have been taken to ensure this data sheet and the health, safety and environmental information \\ncontained in it is accurate as of the date specified below. No warranty or representation, express or implied is made as to the \\naccuracy or completeness of the data and information in this data sheet.\\nThe data and advice given apply when the product is sold for the stated application or applications. You should not use the \\nproduct other than for the stated application or applications without seeking advice from BP Group.\\nIt is the user’s obligation to evaluate and use this product safely and to comply with all applicable laws and regulations. The BP \\nGroup shall not be responsible for any damage or injury resulting from use, other than the stated product use of the material,\\nfrom any failure to adhere to recommendations, or from any hazards inherent in the nature of the material. Purchasers of the \\nproduct for supply to a third party for use at work, have a duty to take all necessary steps to ensure that any person handling or \\nusing the product is provided with the information in this sheet. Employers have a duty to tell employees and others who may be \\naffected of any hazards described in this sheet and of any precautions that should be taken.  You can contact the BP Group to \\nensure that this document is the most current available.  Alteration of this document is strictly prohibited.\\nHistory\\nProduct Stewardship\\nPrepared by\\nIndicates information that has changed from previously issued version.\\n19/04/2023.\\n18/10/2022.\\nFull text of classifications \\n[CLP/GHS]\\nAcute Tox. 3\\nACUTE TOXICITY - Category 3\\nAsp. Tox. 1\\nASPIRATION HAZARD - Category 1\\nFlam. Liq. 2\\nFLAMMABLE LIQUIDS - Category 2\\nSTOT SE 1\\nSPECIFIC TARGET ORGAN TOXICITY - SINGLE EXPOSURE -\\nCategory 1\\nExposure Scenario \\ninformation\\nAspiration hazard  :  Relevant safety measures have been included into the applicable sections \\nof this safety data sheet, in place of appending an exposure scenario.\\nProduct name\\nVersion 1\\nCastrol ON DC 15\\nPage: 12/12\\nVP1-5000-06-01..\\nProduct code\\nDate of issue 19 April 2023\\nFormat United \\nKingdom \\n(UK)\\nLanguage\\nENGLISH\\n(United Kingdom)\\nDate of previous issue\\n18 October 2022.\\n', \"Deploying liquid cooling \\nin the data center\\nA guide to high-density cooling\\nVertiv White Paper\\n2\\n3\\n5\\n21\\n20\\n23\\n22\\nAdopting Liquid Cooling  \\nat Existing Data Centers\\nGetting Ready To Deploy Liquid Cooling\\nMoving Forward With Hybrid Cooling\\nAppendix B: Glossary\\nAppendix A: Creating the  \\nOptimal System Design\\nEvaluating Liquid Cooling Options To Cool 1 MW IT Loads \\n \\nImplementing RDHx \\n \\nBest Practices for CDU Deployment and Management \\n \\nDrilling Down on New Power Requirements for 1 MW  \\nIT Workloads \\n \\nRestructuring AC Power To Support a Liquid  \\nCooling Installation \\n \\nChoosing the Right Racks for AI Computing Workloads \\n \\nLeveraging Opportunities to Reduce Costs \\n \\nDesigning Mechanical Space \\n \\nDesigning Technical Space \\n \\nDetermining Piping Size and Placement \\n \\nSelecting Manifolds and Couplings \\n \\nSourcing and Using Compatible Materials \\n \\nProtecting IT and Liquid Cooling Systems Against Failure \\nTable of Contents\\nLeveraging Project Services To Get Ready \\nfor New Capacity Turn-on\\nConducting Ongoing Maintenance\\n8\\n9\\n9\\n11\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n18\\n3\\nThe future of IT thermal management has arrived, and it’s a \\nhybrid of air and liquid cooling technologies. Enterprises are \\nadopting high-performance computing (HPC) for artificial \\nintelligence (AI) and machine learning (ML) model training \\nand inference, causing a fast rise in chip, server, and rack \\ndensities, power consumption, and heat levels. Air cooling \\nalone can’t abate hot-running equipment effectively. As a \\nresult, many data center teams are strategizing how best to \\nmake their cooling strategy future-ready in support of \\nevolving business requirements. Nearly one in five data \\ncenters (17%) already use liquid cooling, whereas another 61% \\nof operations teams are considering it for their facilities.1 \\nWhile some new facilities will be specifically designed for AI \\nworkloads and liquid cooling, most deployments will occur in \\nexisting facilities. Multi-tenant data center (MTDC) owners \\nand operators are driven to increase their competitiveness. \\nThey know that offering HPC capabilities for AI and other \\nworkloads will quickly become an industry standard. As a \\nresult, many are developing a business case to incrementally \\nadd liquid cooling across racks and rooms, evaluating \\ndifferent options, and creating a roadmap to phase in the \\nadoption of new solutions.\\nAdopting Liquid Cooling \\nat Existing Data Centers\\n1 Daniel Bizo and Lenny Simon, The coming era of direct liquid cooling: it’s when, not if, Uptime Institute, July 2022, page 13, https://uptimeinstitute.com/ \\nIT, facility, and power teams will need to work together closely \\nto deploy liquid cooling at existing facilities, as they will \\nredesign buildings around rack, power, and cooling \\nrequirements. Readers of this technical guide are likely \\nseeking insight into how to deploy liquid cooling to support \\nrack densities up to, and in some cases exceeding 50 \\nkilowatts (kW) per rack. \\nThis guide discusses how to take a 1 MW IT load that is \\ncurrently air cooled and add the incremental liquid cooling \\ninfrastructure to create a hybrid system (hereafter called \\nhybrid cooling infrastructure). The guide covers evaluation of \\ncooling, power, and rack requirements, strategies for cost \\nreduction, designing the physical space, fluid network sizing, \\nmonitoring requirements, and services. In addition, it provides \\nan appendix with recommended equipment and tips on which \\nequipment should be purchased together. Teams can use this \\ninformation to convert 1 MW IT loads to hybrid cooling \\nsystems and scale with business growth. \\nThermal Management Capabilities At Various Rack Densities\\nLiquid-cooled systems are often used with air-cooling systems to cool racks at higher densities.\\nPerimeter CRAC or AHU\\nPassive Rear Door\\nActive Rear Door\\nLiquid Cooling\\nIndex\\nLower boundaries by\\ndesign for eﬀiciency\\nor use case\\nUpper boundaries for\\nextreme densities; rack\\nsize typically increases in\\nheight and width\\nMultiple technologies \\ncan be combined for\\nincreased performance\\nBoundary for hybrid\\nand liquid cooling solutions\\n3kW\\n5kW\\n10kW\\n15kW\\n20kW\\n30kW\\n50kW\\n75kW\\n100kW\\n150kW\\n200kW\\n200+\\nRow-based Cooling With Containment\\nPerimeter CRAC, Raised Floor,\\nor Containment\\n4\\nTop Three Takeaways on Deploying Liquid Cooling\\n1\\n3\\n2\\nHere’s high-level guidance any team can use to navigate the process of developing a hybrid air-liquid cooling infrastructure. \\nCreate a team to oversee the addition of liquid cooling  \\nto cool high-density applications\\nGet ready to deploy liquid cooling technology\\n•\\t Assemble a team of subject matter experts who will provide input on hybrid \\ncooling infrastructure design, selection, installation, and maintenance. \\n•\\t This group will include internal experts (IT, facilities, cooling, and power), \\nconsultants, manufacturers, and other vendors. \\n•\\t Gather application and workload requirements to determine  \\nfuture-state needs.\\n•\\t Develop a target state infrastructure to support the new requirements  \\nby using design and budget guidance.\\nUse project services to deploy and maintain  \\nnew systems \\n•\\t Commission systems in line with their design specifications.\\x03\\n•\\t Start up services, train teams on new systems, and ensure an effective \\nhandover to operations teams.\\x03\\n•\\t Begin scheduled maintenance.\\n5\\n1\\nDetermine current and future workload requirements\\nDeploying liquid cooling is a significant initiative that requires careful planning and consideration of the existing facility’s footprint, \\ncurrent thermal management strategy, workloads, and budget, among other considerations. Here is a roadmap for getting started. \\nAt MTDCs and enterprise data centers, rack densities are growing, spurred by the latest x86 and AI-capable graphics processing \\nunit (GPU) chipsets, surpassing thermal design power (TDP) of 300 watts (W) and 800 W, respectively, and fast approaching 1,000 \\nW or more. These chipsets are used for cloud and enterprise applications including deep learning, natural language processing, AI \\nchat generation, training, and inferencing. With this trend, the newest AI servers are now approaching a TDP of 6 kW to 10 kW \\nper server.\\nIT and facility teams must decide how much space to allocate to new AI/HPC workloads to support current demand and growth \\nover the next one to two years. Some will convert a few racks at a time, while others will allocate entire rooms for these workloads \\nand supporting the addition of liquid cooling systems. \\nBefore developing a business case, teams need to know if \\nretrofitting a facility with liquid cooling systems is technically \\nand economically feasible. \\nThe IT and facility team should work with partners to conduct \\na thorough site audit. A partner should perform a \\ncomputational fluid dynamics (CFD) study of existing airflows \\nin the facility. This expert will analyze existing air-cooling \\nequipment to see if it provides enough capacity to be \\nleveraged in the new hybrid cooling infrastructure and if \\nexisting piping can be reused. They will also want to perform a \\nflow network modeling (FNM) analysis to select the correct \\ncoolant distribution units (CDUs), size piping appropriately, \\nchoose manifolds, and evaluate the ability of the liquid cooling \\nsystem to support server liquid cooling requirements\\nTeams will want to execute water and power usage \\neffectiveness (WUE and PUE) analyses to determine how \\nefficiently they are using water and power resources to identify \\nareas for improvement. A total cost of ownership (TCO) study \\nis also recommended to optimize operations by replacing old \\nor inefficient equipment to lower operational costs. Multiple \\nonline tools are available to help calculate TCO specifically for \\nliquid cooling applications. \\n2\\nConduct a site audit\\nThe power team should also analyze infrastructure to see if \\nit can be adapted for use with more power-intensive \\nworkloads, such as AI. The larger group — IT, facility, and \\npower — should review physical space to see if raised floors \\ncan support the combined weight of new power and hybrid \\ncooling systems and determine access routes for piping. A \\ncheck of the facility for potential required maintenance of \\nexisting infrastructure should also be conducted, as there \\ncould be contamination or degradation in the quality of \\nexisting pipes or equipment that could lead to inefficiencies or \\nfailure. The joint team should review the on-site water supply \\nand determine if it is suitable for use in planned liquid cooling \\nsystems. Finally, any safety regulation compliance concerns \\nshould be addressed to ensure the new solution will be up to \\nstandards and safe for use.\\nAll of this information will inform the business case, so that \\nsenior stakeholders can make critical decisions about which \\napproach to use to develop a hybrid cooling infrastructure \\nand provide the investment and resources needed to \\ncreate it. \\nGetting Ready \\nTo Deploy Liquid Cooling\\n6\\nSince liquid cooling removes heat at the source, it can be more \\nefficient than air cooling alone and lowers facilities’ PUE \\nmetrics. It also uses water or fluid to cool systems and allows \\nteams to recapture and reuse heat, reducing WUE. After \\nauditing systems and benchmarking data, teams can regularly \\ncapture metrics, demonstrating progress in reducing PUE and \\nWUE. These gains can reduce indirect or energy-regulated \\nemissions (Scope 2) for enterprises. As a result, liquid cooling \\ncan be an essential part of enterprises’ sustainability programs. \\nWith this information, the IT and facility team can work with \\nthe design consultant and partners to design a new solution \\ncustomized for site requirements. The joint team can then use \\nthat information to create a bill of materials and services \\n(BOMS), request quotes, and select the manufacturers to build \\nand integrate the liquid cooling system.\\n5\\n6\\nFactor in efficiency and sustainability gains\\nDesigning the new solution\\nThe audit and modeling exercise provides the IT and facility \\nteam with insight into how extensive the liquid cooling \\ndeployment will be to develop a business case for \\nexecutive consideration. \\nThe IT and facility team will also want to consider how on-site \\nconstruction will disrupt current operations and what impact \\nadding extra heat loads on site will have on current workloads \\nand service-level agreements (SLAs). For example, colocation \\nand managed services firm teams will want to review proposed \\nnew heat loads and ensure that they can maintain temperature \\nand humidity for existing customers. \\n4\\nConsider budget and site impacts\\nWith this data and partner support, IT and facility teams can \\nmodel the desired hybrid cooling infrastructure in the data \\ncenter and identify obstacles to overcome. These obstacles \\ncan include weight restrictions, a lack of on-site water, the \\nneed to install new piping, access route concerns, and \\nother issues. \\nThe team can model a greenfield build if space is available at \\nthe data center. If the team wishes to replace existing servers \\nwith higher-density alternatives, they must optimize technology \\nplacement, including busbars, connectivity, and racks. Once all \\nissues have been addressed, it is a good idea to contract with \\na vendor to construct a digital twin replica of the new design \\nto explore new systems and processes in 3D. The tool can also \\nprovide a methodology for modeling different scenarios to \\nimplement additional IT loads, all at once or in a phased model. \\nWith this information, the team can finalize their system design, \\nmaking any needed enhancements. \\n3\\nModel new infrastructure in desired space\\n7\\nPlanning Liquid Cooling Deployments: Equipment and Timeframes \\nWhat type of equipment does it take to support a liquid cooling solution at an existing site? The product types below can vary \\nbased on the cooling approach, site requirements, and other factors, but they provide a starting point for making the first steps. \\nTo select vendors, suppliers, and contractors; perform CFD, PUE, WUE, and TCO \\nanalyses; conduct site planning; obtain a design quote; and create a BOMS.\\nManufacture and transport of liquid-cooled infrastructure, including pipework,  \\nrack manifolds, CDUs, heat rejection, power components  \\nwhere required, and racks.\\nConduct site planning and deliver and integrate the new liquid cooling system.\\nIntegrate IT equipment with the new cooling solution and check for  \\nany discrepancies. Conduct regular maintenance as needed.\\nUtilizing liquid-to-air CDUs could speed up deployment times. \\nWant help navigating \\noptions, designing a \\ncustom system for  \\nyour site, and \\ncreating a budget? \\nContact Vertiv for \\nmore information.\\nAllow up to a year to \\x03deploy new systems \\nAt least 2 months\\nAt least 3 months\\n2 to 3 months\\n2 to 3 months\\nCategory\\nEquipment Type\\nLiquid Cooling\\nRack manifold, CDU,  \\ndistribution manifold, RDHx\\nHeat Rejection\\nFreecooling or indoor split chiller; drycooler; \\nDX condensing unit\\nPower Distribution\\nHigh-amperage busway,  \\nrack power distribution units\\n8\\nCustomer: I need to deploy \\nhigh density racks that my \\nexisting cooling cannot handle\\nCDU\\n(Liquid-to-Air)\\nor Split Indoor Chiller\\nSplit \\nIndoor Chiller\\nCDU\\n(Liquid-to-Liquid)\\nRDHx\\nIs the IT Equipment \\nair-cooled or liquid-cooled?\\nWhat is the heat \\nrejection method?\\nIs chilled water \\navailable on site?\\nProduct Solution\\nDX\\nNo\\nAir-cooled\\nLiquid-cooled\\nCW\\nYes\\nRequires chilled water on site\\nDetermining Which Liquid Cooling Solution To Add To Air Cooled Facilities\\nLiquid Cooling Solution Tree\\nUse this decision tree to select the right liquid cooling solution. \\nNo single liquid cooling solution exists for 1 MW IT loads, as \\ncompany requirements, site conditions, and budgets vary \\nconsiderably. Some options teams can consider include  \\nthe following:\\n \\ny\\nLeveraging existing chilled water: If teams have access \\nto chilled water on site, they can deploy a CDU that \\nprovides a liquid-to-liquid heat exchanger. To do this, the \\nchilled water system must be a 24x7 design versus a \\ntraditional comfort cooling system which is often shut off \\nafter hours, on weekends, and during off-seasons. Available \\nas in-rack, in-row, and perimeter designs, these CDUs \\nisolate IT equipment from the facility’s chilled water loop by \\nproviding a separate liquid loop to and from the racks. This \\nsolution enables teams to determine which fluid and flow \\nrate to supply the racks, such as treated water from a \\nwater-glycol mix. Water quality may also be a concern when \\ndeploying CDUs and utilizing existing chilled water systems. \\nEnsuring availability of filtration, or the capability to deploy \\nfiltration if needed, should be considered when leveraging \\nthe on-site chilled water.\\nEvaluating Liquid Cooling Options To Cool 1 MW IT Loads \\n \\ny\\nLeveraging existing DX condensers: If teams have \\nalready deployed a direct expansion system, they can \\nconsider utilizing split indoor chillers as a solution. This \\nsystem can reuse existing rooftop condensing units or \\nadditional units can be deployed to support incremental \\nloads. Split systems offer the flexibility to be easily \\ndeployed as demand increases incrementally. When \\nconsidering a split system, technologies such as pumped \\nrefrigerant economization should be leveraged to drive \\noverall system efficiency. If the existing rooftop condensers \\nare not enough in terms of quantity or capacity, or if there \\nis not sufficient roof or ground space available for \\nadditional units, then a split indoor chiller may not be \\napplicable.\\n \\ny\\nOvercoming a lack of access to reject heat from the \\nliquid: Teams can consider deploying a standalone CDU \\nthat provides liquid-to-air heat exchange, cooling IT \\nequipment and blowing hot air into the data center, where \\ncomputer room air conditioning (CRAC) units capture and \\nremove it. \\n9\\nImplementing RDHx\\nMany organizations use RDHx as a first step on their path to \\nincrementally adding liquid cooling to develop a hybrid cooling \\nsystem, as they can be used with existing air-cooling \\ntechnology and don’t require significant structural changes to \\nwhite space. Rear doors do extend the depth of the cabinet \\nand require a minimum distance between the door and other \\nobjects to ensure unobstructed airflow.\\nRDHx use air to cool racks but capture heat via a liquid-to-air \\nheat exchanger mounted to the racks. The air leaving the heat \\nexchanger is then expelled to the room at ambient \\ntemperature. Rear doors can be configured as either passive or \\nactive units. Passive heat exchangers use server fans to expel \\nhot air through a liquid-filled coil, which absorbs the heat \\nbefore the air is returned to the data center. Since these do not \\nhave fans or other moving parts requiring power, passive units \\nhave no annual usage cost to the doors themselves. These \\npassive doors work well for loads from 5-25 kW. With no \\nmoving parts, these passive units will also reduce the amount \\nof noise in the facility.\\nActive units assist the servers with fans attached to the rear \\nof the door that can help pull hot air through the coil and \\nreject room-neutral air to the IT space. The fans can modulate \\nbetween 0-100% based on either the differential pressure \\nacross the coil or the measured temperature difference.  \\nActive rear doors are capable of cooling 50-70 kW based on  \\napplication parameters.\\nFluid flow control for passive or active doors can be adjusted \\nbased on the return water temperature using a modulating \\nvalve to make sure the door is operating efficiently. Additional \\nmonitoring and control capabilities are available with these \\nunits, providing remote access, alarms, and system  \\nstatus updates.\\nWhen properly sized, RDHx can eliminate the need for \\ncontainment strategies, such as hot and cold aisles. However, \\nit is still recommended that rack doors face each other in a  \\nhot aisle/cold aisle pattern. \\nBest Practices for CDU Deployment and Management \\nCDUs create an isolated secondary loop separate from the \\nchilled water supply, enabling teams to contain and manage \\nfluids used in liquid cooling systems and precisely control their \\npressure, temperature, flow rate, and even filtration quality. \\nCDUs are equipped with supply-side filters that are typically 50 \\nmicrons in size. The filters keep the fluid supply free from \\ncontaminants and debris, protecting the integrity of the server \\ncold plate and maintaining its performance. The cold plate fin \\npitch is the space between the raised fins on the surface of the \\ncold plate. When teams deploy cold plates with a fin pitch of \\n100 microns, it is advised to use filters no greater than 50 \\nmicrons to reduce the possibility of debris gathering within the \\nfin pitch gap on the cold plate and negatively affecting system \\nperformance or life-span.\\nTo determine CDU requirements, the application engineer will \\nevaluate the number of racks to support, the heat-to-liquid ratio \\nfor liquid-cooled IT equipment in each rack, the flow rate \\nrequired for each rack, and the facility layout. For example, \\nCDUs that support systems with high required flow rates will \\nrun out of pumping capacity before the heat exceeds its \\nthermal capacity. As a result, these applications require greater \\nredundancy. After establishing requirements, the engineer will:\\n \\ny\\nConnect CDUs to key equipment: Liquid-to-liquid CDUs \\nmust be connected to the facility's chilled water supply or \\nother heat rejection source to provide coolant needed to \\nremove heat from liquids. Additionally, the CDU should be \\nconnected via inbound and outbound pipes to the liquid \\nthat needs cooling. The CDU contains a heat exchanger \\nwhich transfers heat into the chilled water supply, \\ncompleting the liquid-to-liquid cooling process.  \\n \\nLiquid-to-air CDUs do not require a chilled water supply to \\nprovide liquid cooling to the rack, but rather provide an \\nindependent secondary fluid loop to the rack and reject \\nheat to the data center. The secondary fluid loop is an \\nisolated system, delivering chilled fluid to liquid cooled \\nservers in an otherwise air cooled environment. By rejecting \\nheat directly into the data center, the liquid-to-air CDUs \\ncan rely on existing air cooling systems to capture and \\nreject the heat outdoors.\\n \\ny\\nDetermine CDU placement: CDUs can be placed in \\nmultiple locations. With in-rack or in-row placement, they \\nfeed individual racks or rows, respectively. They can also be \\nplaced at the end of a row, feeding the racks on that row.  \\n \\n10\\nFinally, they can be grouped at the end of a data center, \\nfeeding a common fluid circuit that supplies all racks in the \\nrooms. Teams will make this determination based on the \\nlevel of redundancy they want to achieve, how efficient they \\nwant the liquid cooling to be, and the capital expenditure \\n(CapEx) funds they have for new technology.\\n \\ny\\nDecide on the level of redundancy desired: To achieve \\nN+1 redundancy at the row level requires two CDUs: one to \\nfeed the racks and the second to back it up. This takes up \\nvaluable floor space and increases costs. However, MTDCs \\nmay choose this layout to provide redundancy for individual \\ncustomer technology deployments. \\n \\nEnterprise IT and facility teams will likely group all CDUs \\ncentrally to provide adequate fluid flow, achieve redundancy \\nwith fewer units, and scale more quickly. \\n \\ny\\nDetermine liquid cooling requirements: The amount \\nrequired to support 1 MW of IT load varies based on \\nworkload temperatures and flow rates. \\n \\nThe IT and facility teams determine rack power and \\nhydraulic requirements. The table below shows an example \\nof a direct-to-chip liquid cooling system that cools 20 \\n50 kW racks. The heat-to-liquid specification is for the cold \\nplate liquid cooling system, where cold plates are placed on \\nIT equipment components that generate the most heat. \\nOther components rely on air systems for cooling.  \\n \\nThe liquid flow rate is approximately 1.5 liters per \\nminute/kW, which depends on cold plate heat transfer \\neffectiveness and the heat generated by IT components \\ncooled with liquid. The facility specifying engineer will \\nconsult with the IT equipment vendor to determine upper \\nand lower flow rate requirements to set the desired flow \\nrate. The facility team should review flow rates regularly \\nand increase allowable ranges when IT equipment  \\nis refreshed. \\n \\nThe IT and facility teams and contractors should also \\ninspect the liquid loop to ensure that the piping can \\nsupport desired flow rate requirements. This group will also \\nwant to review the pipework, secondary inlet, and return \\nheader dimensions, ensuring they match connections to \\nthe CDU. The system will use pipework reducers to \\nconnect hydraulic loops to the CDU.\\nRepresentative Hybrid Cooling Deployment for a 1 MW IT Load \\nQuantity of liquid-cooled racks\\n20\\nTotal power per rack \\n50 kW\\nTotal cluster power\\n1 MW\\nHeat to liquid\\n37.5 kW\\nTotal cluster heat to liquid\\n750 kW\\nHeat to air\\n12.5 kW\\nTotal cluster heat to air\\n250 kW\\nLiquid flow rate per rack\\n56.25 lpm\\nTotal cluster liquid flow rate\\n1125 lpm\\nAirflow rate per rack\\n1875 CFM \\nTotal cluster air flow rate\\n37500 CFM \\nFor illustrative purposes only. Actual site conditions may vary. \\n11\\nDrilling Down on New Power Requirements for 1 MW IT Workloads \\nHow do power requirements change with liquid cooling?  \\nThe IT power load may remain the same as data center teams \\nenable new AI and other 1 MW IT workloads. However, rack \\ndensities will increase, consolidating power consumption into \\nfewer racks.\\nIn other scenarios, data center teams may want to increase \\noverall IT power to expand computing operations. \\nEither way, the IT and facility team needs to ensure that power \\ndistribution provides the higher amperage that AI computing \\nhardware requires. Teams can accomplish this goal by:\\n \\ny\\nInstalling a new busway: A high-amperage busway can \\nprovide the overhead feed that new computing hardware \\nrequires. Tap-off boxes that draw off power should be \\nappropriately sized to protect current flow. \\n \\ny\\nConsidering redundancy: Teams may want to deploy A+B \\npower feeds, each with automatic transfer switches, \\nuninterruptible power supply (UPS) units, and power \\ndistribution units (PDUs), to provide redundancy to \\nduplicate critical components and ensure power continuity  \\nfor workloads.  \\nRestructuring AC Power To Support a Liquid Cooling Installation \\nRestructuring AC power to support the addition of liquid cooling \\nrequires an evaluation of the full power train from server loads \\nto utility feed. By so doing, the site electrical team can ensure \\nsufficient critical power continuity for new HPC applications.\\nCurrently, standard AC power systems support most  \\nhigh-density liquid cooling applications. However, some liquid \\ncooling sites will require reconfiguration, redeployment, or full \\nreplacement of the power infrastructure. \\nTo get ready for liquid cooling, IT and facility teams should work \\nwith a design consultant to design an integrated thermal-power \\nsolution, paying close attention to the physical layout of the \\nspace and potential mechanical interferences. This process \\nallows teams to potentially upgrade critical power to support \\ncontinued business growth or improve redundancy to ensure \\npower continuity to liquid-cooled racks and rooms.\\nThe AC power restructuring process includes:\\n \\ny\\nAssessing the current state: The electrical team will \\ndefine the current state of power supporting IT workloads \\nand infrastructure versus the target power distribution and \\ninfrastructure required to support HPC workloads.  \\nThe greater the redundancy, the greater the protection \\nagainst downtime. However, additional systems may \\nincrease the overall budget. Multiple redundant models can \\nbe deployed, including N, N+1, N+2, and 2N. \\n \\ny\\nDetermining how to route electricity: Teams can \\ncommission an electrician to hardwire electrical \\nconnections to the rack or combine the higher-amperage \\nbusway with higher-amperage rack PDUs. If teams choose \\nthe rack PDUs, they should decide whether to have an \\nelectrician hardwire them or use a traditional rack PDU \\nwith a plug.  \\n \\nWhile the traditional rack PDU is simpler to install, teams \\nmay be able to decrease the number of PDUs within each \\ncabinet by hardwiring them. Fewer rack PDUs mean \\ncabinets have more space for liquid cooling equipment, \\nsuch as direct-to-chip liquid cooling pipes, manifolds, CDUs  \\nand RDHx.\\nThe team will:\\n•\\t Create a detailed power inventory, associated electrical \\none-lines, and target states to frame restructuring \\nrequirements.\\n•\\t Capture available power backup, conditioning, and \\ndistribution capacity (such as rating and connectivity) \\nto maximize the utilization of power restructuring and \\ndefine any required supplemental support.\\n•\\t Document power cabling sizing and layout, especially \\nfrom the PDU, remote power panel (RPP), and busway \\nto equipment where load concentration change is  \\nmost acute. \\n•\\t Evaluate overall rack and discrete internal rack loads for \\nrack PDU deployment, considering redundancy \\nstrategies, phase balancing, and connection type  \\nand count.\\n12\\n \\ny\\nPlan and implement restructuring: The electrical team \\nwill determine the requirements and process to convert the \\npower infrastructure to accommodate new liquid cooling \\nsystems and their buildout, evaluating potential \\nimprovements. The team should consider physical \\nconstraints associated with concentrated rack power loads \\n(such as connections and wiring path) and how best to \\nintegrate seamlessly with the liquid cooling infrastructure.\\nChoosing the Right Racks for AI Computing Workloads \\nTo deploy new AI workloads, teams must change their \\napproach to selecting racks. Racks will support dense \\ncomputing environments up to 50 kW — and ultimately \\nbeyond. Thus, when selecting racks, bigger is better. Racks \\nbetween 42U and 52U provide room for servers populated \\nwith accelerators, networking infrastructure, power distribution \\nequipment, and cables. When selecting taller racks, ceiling \\nheight needs to be accounted for as some sites only have a  \\n2 meter clearance height in the data center or through \\ndoorways. Teams should plan on having each rack support up \\nto 3,000 pounds (1.36 kilograms) of static weight to handle \\nincreased gear density. As a result, it is highly unlikely teams \\nwill be able to reuse existing racks to support new AI/HPC \\ncomputing equipment. They should also invest in cable \\nmanagement to reduce clutter and avoid the risk of \\nunplugging essential components. Other guidance includes:\\nThe most common rack in an air-cooled data center is 600 \\nmillimeters (24 inches) wide, which aligns with raised floor tiles. \\nWhen deploying liquid cooling, teams can benefit by selecting \\nwider racks, with 800 mm (31.5 inches) being the most \\ncommon size. This size provides an extra 100 mm of space \\n(4 inches) on each side of the centrally located server \\nmounting rails. \\nThis extra space provides room for the coolant distribution \\npiping and high-power cabling at the rear of a liquid-cooled \\nrack. It also offers additional free, unobstructed space directly \\nbehind the IT gear that helps preserve proper airflow through \\nthe equipment and out the rear of the rack. Crowding this area \\nby selecting a narrower rack can impede airflow, increase \\ntemperatures, reduce cooling efficiency, and degrade IT \\nperformance as temperatures rise. \\n1\\nSelect wide racks\\n \\ny\\nManage new systems: The electrical team should \\nconsider working with power partners to help define and \\nprovide ongoing critical power monitoring, analysis, and \\nservice management to ensure critical power continuity. \\nThe facilities team should establish and monitor power \\nperformance and alarm parameters, as well as confirm \\npreparedness to manage new power systems on an  \\nongoing basis.\\nPiping that’s run under the floor to rack rows must also feed \\neach rack individually. These pipes can be large in diameter \\nand may change the underfloor airflow pattern that data center \\ndesigners originally intended for power or data cabling. With a \\ngreenfield build, designers may use the raised floor for piping, \\nrunning power and data cabling overhead. With a brownfield \\nretrofit, designers may need to move existing racks and air \\ncooling, which could interfere with pipe runs. This can be \\ncostly and disruptive to operational performance. \\nOne alternative is to mimic greenfield designs, decommissioning \\nunderfloor space and using it solely for pipework, moving power \\nand data cables overhead. Separating cabling from piping has \\nmultiple advantages, which includes alleviating the risk of a fluid \\nleak affecting cabling and reducing “working interference” where \\nworkers accidentally push or nudge cables out of the way while \\nworking on a system, even if intentional and temporary. \\nWorking interference can negatively impact other systems.\\nIn addition to demanding greater width, liquid cooling systems \\nbenefit from deeper racks. New piping occupies space that \\npreviously wasn’t required for air-cooled racks. In addition, high-\\npower IT gear requires thicker power cables to provide \\nincreased power, while larger PDUs are needed to handle higher \\nampacity. While 1000 or 1100 mm (40 or 43.5 inches) deep \\nracks have worked for some air-cooling cases, racks for liquid \\ncooling should be 1200 mm (48 inches) deep. This is especially \\ntrue when using a RDHx, as the extra depth allows for better \\nairflow (static regain). Similarly, direct-to-chip cooling systems \\nrequire space to mount rack distribution manifolds, cabling,  \\nand rack PDUs.\\n3\\n2\\nConsider where to place piping\\nChoose deeper racks\\n13\\nLeveraging Opportunities to Reduce Costs\\nRegardless of the technology type or required updates needed \\nto implement liquid cooling, deploying a hybrid solution is a \\nmajor investment toward the future. While this can be a point of \\nconcern for some, data center IT and facility teams have many \\nopportunities to reduce upfront CapEx costs to implement \\nliquid cooling infrastructure. There are many ways to go about \\nthis, including: \\n \\ny\\nPerform a thorough site assessment: As mentioned \\npreviously, a site audit should be one of the first activities \\ndata center teams perform when looking to deploy a high \\ndensity system. Review the existing infrastructure \\nconditions to identify areas that may need modification, \\nrenovation, or new equipment to support this higher density \\nIT workload. Work with internal and external experts to \\nensure the full scope is understood.\\n \\ny\\nReuse existing infrastructure: Much of the installed \\ninfrastructure, such as cabinets, PDUs, UPS, and cooling, \\nhas the potential to be reused for high density applications. \\nRather than unnecessarily investing in equipment that \\nrenders existing product obsolete, look to use as much as \\npossible.\\n•\\t Utilize existing cooling infrastructure: A major benefit \\nto hybrid cooling solutions is the ability to take \\nadvantage of existing cooling infrastructure. By deploying \\na split indoor chiller unit, customers can make use of \\nexisting outdoor condensers for heat rejection.  \\nInter-row clearance may be affected if liquid cooling \\nequipment, such as a RDHx, adds depth to a rack. For \\nexample, consider two racks placed rear to rear, which \\ncommonly occurs in hot aisles. Each rack gains up to 330 mm \\n(13 inches) in depth when adding a RDHx, while the aisle width \\nis decreased by 400 mm (16 inches). This may result in an \\naisle that is too narrow for effective cooling and doesn’t enable \\nfunctional work between racks.\\nHigh-density racks take up less white space, but the \\nproportion of mechanical to white space in high-density, liquid-\\ncooled data centers varies. For example, replacing 100 10 kW \\nracks with 20 50 kW racks decreases the white space \\nfootprint. However, teams must deploy new equipment needed \\nfor liquid cooling, such as CDUs, indoor chiller units, piping, \\nstorage tanks, treatment devices, and leak detection and \\nmonitoring systems. Conversely, teams may be able to \\ndecommission some air-cooling equipment, potentially freeing \\nup space. Regardless of an increase or decrease in available \\nspace, teams should also take into consideration the benefits \\nthat deploying an aisle containment system can bring to their \\nexisting white space.\\n4\\n5\\nPlan for inter-row clearance changes\\nPrepare for different space use\\nTeams at data centers with slab floors will likely run power and \\npiping overhead and need to consider issues such as system \\nweight and available space. See “Designing Technical Space”  \\nfor more information. \\nIf liquid-to-air CDUs are deployed for direct-to-chip \\napplications, existing room cooling units are still required \\nparts of the hybrid infrastructure.\\n•\\t Repurpose the power grid: While some AI/HPC \\nworkloads increase power requirements, others will only \\nconsolidate power into denser rack deployments. In \\nthese instances, existing UPS units, busways, and PDUs \\ncan be rearranged to meet the new demands or be used \\nto supplement new power requirements, limiting the \\ninitial investment.\\n•\\t Leverage existing IT racks: Not all AI/HPC \\ndeployments will require additional U-space within the \\nracks to support the increased workload. Whenever \\npossible, teams should maximize usage of the existing \\nracks to limit the up-front investment. Liquid-cooled \\nservers take up less space compared to air-cooled \\nheat sinks.\\n \\ny\\nCarefully select fluids and coolants: With the plethora \\nof options available on the market at various price points, \\ndata center teams can be very selective about which fluid \\nor coolant will be the best fit from a material compatibility, \\navailability, and economic standpoint. Selecting one that \\nis more cost-effective than others will help reduce \\nupfront expenses.\\n14\\n \\ny\\nFocus on energy-efficient solutions: Energy efficiency is a top priority for investors and operators. Selecting equipment that \\nhas been designed to maximize energy efficiency will help reduce ongoing operational costs. There are many liquid cooling \\noptions that provide significant efficiency gains, which leads to overall long-term savings.\\n \\ny\\nImplement the changes incrementally: The new high density deployment and supporting hybrid cooling infrastructure doesn't \\nneed to be installed in one large system upgrade. Start with critical areas or equipment, and gradually expand to the entire data \\ncenter to spread out the capital costs over time. Work with technology partners to decide which products need to be deployed \\nfirst to support future expansion. Consider using modular systems that can be added incrementally to facilitate the addition of \\nliquid in the data center.\\nCRAC Units\\nSeparating Wall\\nData Center Floor\\nMechanical Gallery\\nGrey Space\\nWhite Space\\nDesigning Mechanical Space \\nWhere should teams place cooling systems? In some \\ninstances, equipment may be located in the white space. \\nHowever, most MTDC or enterprise facility operations teams \\nprefer to locate liquid cooling equipment in grey space, which \\nmay be shared with electrical equipment.\\nA typical scenario is to place equipment in a hallway or gallery \\nsurrounding the perimeter of white space. This gallery may \\nhave a raised floor that’s level with the white space raised floor \\nor have a sunken floor. \\nAnother less common scenario is to run a deeper concrete \\ntrench around the gallery with the raised-floor level above \\nthe trench — level with the white space raised floor. In this \\nscenario, large pipes are typically located in the trench floor \\nor its walls. The goal is to create adequate space for \\nongoing service and maintenance.\\n15\\nAir-cooling systems that serve IT systems in the white space \\nmay penetrate the gallery via ducting or wall cut-outs. \\nDepending on their design, these areas, which enable free air \\npass-through, may require fire dampers.\\nWhen designing mechanical space, teams and contractors \\nshould ensure that it provides adequate room to install, remove, \\nand replace any items located therein without disrupting other \\nequipment. For this reason, aisle space may need to exceed \\nrequired fire, electrical, and accessibility code requirements. \\nIn addition to space considerations, an initial calculation is \\nrecommended to determine if the existing floor can support \\nthe load of the new equipment and if any bolt down brackets \\nwill be required.\\nSince pipework is run overhead, IT and facility teams should \\nensure valves and other items are easy to access  \\nfor maintenance. \\nDesigning Technical Space \\nNext, the IT, facility, and power team must work with the design \\nconsultant to design technical space, considering where racks, \\nair cooling distribution, fluid piping, power distribution systems, \\nand monitoring equipment will be placed. A key factor guiding \\ndecision-making will be whether the data center has a raised  \\nor non-raised floor design.\\nFor a non-raised floor, the team should consider a technical \\ndesign that places power and pipework conduits above the \\nracks with threaded rods hung from ceiling joists, providing \\nceiling support.  \\nHow Threaded Rods Support Overhead Infrastructure \\nThis illustration depicts an overhead liquid supply and return manifold and power bus.\\nThe design should consider the weight of supporting cable \\ntrays and pipework to ensure overhead joists and ceiling \\nstructures can support them. \\nThe team should also verify sufficient space to accommodate \\npipework and equipment servicing. To continue the 1 MW IT \\nload example, a team would need 20 50 kW rack positions, \\neach with a 1.5 lpm (0.4 gallons per minute/kW)f low rate. \\nCeiling Support\\nSupply\\nReturn\\n16\\nRunning Liquid Cooling Equipment Overhead in Raised-Floor Facilities \\nThis illustration shows how running  \\na rear-door heat exchanger adds depth \\nto racks. \\nThis illustration depicts the clearance required to accommodate underground \\npiping in a facility with a raised-floor design. Dimensions will vary from site \\nto site. \\nUnderfloor designs for piping should leave adequate space for piping, insulation, and pipe crossover. Dimension will vary from site to site. \\nDetermining Piping Size and Placement \\nPipe flows, or capacity, are limited by velocity and pressure. High-velocity fluid flows can cause erosion at pipe turns and create \\nnoise. Pressure loss is driven by flow rate, the length of the run, and the use of devices such as valves and quick-connects.  \\nAllocating Extra Space Underfloor  \\nfor Piping Requirements \\nCeiling Height\\nto Hard Deck\\nor Dropped Ceiling\\nRDHx\\nAdditional\\nDepth\\nClear\\nHeight\\nMax Width\\nUnderfloor\\nClear Width\\nHeader Piping (typical)\\n150-300 mm [3-6 in] nominal\\nBranch Piping (typical)\\n25-50 mm [1-2 in] nominal\\n17\\nPipes should also provide a loop to isolate valves from racks, reducing the possibility of leaks. \\nWhen piping is deployed under raised floors, it must fit in the available space. The design consultant should consider piping size and \\nthe insulation wrapped around it. If the team adds 25 mm (or one inch) of insulation to a pipe, the overall diameter of the wrapped \\npipe is increased by 50 mm (or two inches). The design consultant should also consider that pipes may cross over each other and \\naccount for extra height requirements when finalizing the underfloor height and piping system.\\nSelecting Manifolds and Couplings \\nLike piping, an in-rack manifold is a relatively simple set-and-forget product. However, IT and facility teams still have some unique \\ndesign requirements to consider, as manifolds are placed in racks and are the last touchpoint of the secondary fluid network \\nsupplying the server. When they select manifolds, teams should consider material compatibility, flow rate, pressure, flow distribution \\nuniformity, and pressure drop.\\nEnsuring material compatibility is especially important as teams want to prevent galvanic corrosion, which could cause catastrophic \\nfailure of the manifold or server chassis. Galvanic corrosion occurs when two different metals are in contact with each other in the \\npresence of an electrolyte. The metal that is less resistant to corrosion will oxidize and flake. \\nIn addition, components used to seal each junction, hoses used for \\nfluids, and couplings must be compatible with the selected wetted \\nmaterial. If they’re incompatible, seals could begin degrading, with \\nsmall pieces breaking off, entering the flow stream, and damaging \\nthe downstream heat sinks. \\nDirect-to-chip cooling uses coolant to remove heat from cold plates \\nand direct it to the CDU for removal from the data center. For this \\nprocess to work, there needs to be an adequate flow rate. As a result, \\nteams must ensure that the manifolds and couplings are sized \\nappropriately and have enough pressure to supply an adequate flow \\nrate through the cooling system. \\nEach system component must also withstand pressure drop and \\nwithstand high pressure, including manifolds and couplings. The \\nmanifold system must be able to isolate racks and individual servers \\nto protect IT equipment from leaks and enable both planned and \\nunplanned maintenance. \\nTo prevent fluid from escaping the liquid cooling system when couplings are connected or disconnected, server and manifold \\ncouplings should have a non-spill quick disconnection with an internal valve and seal. The fluid should not flow until a complete \\nconnection has been made, and the valve should immediately reseal itself after initiating a disconnection. \\nIn-rack manifolds that are being plumbed from below also require air bleeders, which control how much the valve opens to allow air \\nrelease, including fluid pressure. In addition, teams may need to deploy circuit setters to relieve pressure from the secondary \\nmanifold as it connects to the in-rack manifold. \\nFinally, the secondary fluid network piping system connection to the in-rack manifold must also have an intermediary isolation valve. \\nTypically, manufacturers use an isolation valve (such as a ball valve) placed between the secondary fluid network piping and the \\nin-rack manifold's inlet to enable isolation of the secondary system. While it’s possible to add controls, they significantly increase \\nsolution cost.\\n18\\nSourcing and Using Compatible Materials\\nProtecting IT and Liquid Cooling Systems Against Failure \\nKey factors that drive the selection of fluids for liquid cooling \\ninclude the presence or absence of chilled water on site and \\nthe need to ensure compatibility with all systems and \\ncomponents in the secondary liquid circuit. \\nDirect-to-chip manufacturers can guide what fluids are best for \\nuse with cold plate loops. Vendors are increasingly using \\npropylene glycol to enhance and extend fluid quality \\nthroughout its lifecycle. However, propylene glycol provides \\nlower thermal performance when compared to pure water. \\nIt is thus often treated with biocides and inhibitors to maintain \\nthermal performance, increase reliability, and strengthen \\ncorrosion protections. Best practices include:\\n \\ny\\nEnsuring material compatibility: The selected fluid \\nshould be compatible with all systems that encounter it, \\nincluding the CDU pump, heat exchanger, valves, couplings, \\nsensor fittings, and pipework. Pipework includes hoses, \\ncouplings, seals, gaskets, and brazing flux. The manifold in \\nthe liquid-cooled rack includes couplings, hoses, and \\nconnectors for the cold plate loop. The cold-plate array \\nincludes couplings, connectors, hoses, and the cold plate \\nitself. Since most cold plates are manufactured from copper, \\nwet materials should not include aluminum, which would \\ncause galvanic corrosion of the cold plate.  \\nIT and facility teams can use multiple strategies to reduce the \\nrisks of damage to IT from fluid leaks and proactively mitigate \\nleaks when they occur. These strategies include:\\n \\ny\\nIsolating IT infrastructure: By keeping fluid piping \\nseparate from IT and power equipment or cabling, teams \\ncan reduce the risks of leaks. Each rack should also be able \\nto be isolated from the system through the use of valves, \\nand each server should be able to be isolated from the \\nin-rack manifold to enable routine and emergency \\nmaintenance. By designing systems that allow users to shut \\noff fluid flow to individual racks and individual servers, \\nteams can continue to perform maintenance on specific \\nsections of the data center without causing disruptions to \\ncritical operations. Tubing from the manifold to the server \\nshould have interconnections with dripless quick \\ndisconnects to avoid any damage caused when servicing \\nthe servers.\\nFor more information on material compatibility \\nrequirements, please reference Liquid Cooling Guidelines \\nfor Datacom Equipment Centers, Section 6.1.5 – Wetted \\nMaterial Requirements from the American Society of \\nHeating, Refrigerating and Air-Conditioning Engineers \\n(ASHRAE). \\n \\nTeams should also consider the impact of fluid velocity as \\nwell. Degrading wet surfaces due to galvanic corrosion or \\nerosion will contaminate the flow stream and cause \\ncatastrophic failure downstream at delicate and critical \\ncomponents such as the cold plate.\\n \\ny\\nConducting regular testing: Teams should contract with a \\npartner for regular chemistry testing to ensure fluids \\nmaintain their desired properties, typically recommended to \\nbe done twice a year. The same fluid composition should \\nbe used throughout the system lifecycle to avoid unwanted \\nchemical interactions caused by a different material. \\n \\nFluid testing provides two main benefits: maintaining fluid \\nhealth by managing contamination and the impact of \\ncontamination on fluid life and ensuring that the fluid \\nmeets the desired lifecycle targets without requiring \\nflushing or additives.\\n \\ny\\nSystem visibility through monitoring software: \\nMonitoring environmental conditions around liquid cooled \\nsystems is pivotal to ensuring protection of the IT \\nequipment. Liquid cooling is inherently different than air \\ncooling when it comes to rapid system response time when \\nfailure scenarios occur due to the higher heat densities that \\nexist. To ensure real-time visibility to system performance \\nand IT equipment safety, additional data should be \\ncollected and acted upon quickly. Integrating intelligent \\nflow monitoring and advanced leak detection into the \\ncooling system monitoring solution will enable more \\nproactive visibility for identifying harmful conditions. These \\nscenarios could include system fluid leaks, humidity levels \\nallowing condensation, and elevated fluid temperatures \\nbeing supplied directly to IT gear. \\n19\\nWith liquid cooling, another important concern is one of \\ncoolant fluid quality. Identifying fluid parameters that cause \\ncorrosion, blockage or limitation of fluid heat transfer \\ncapacity should all be considered in selecting a sufficient \\nmonitoring software package. \\n \\nLiquid-cooled and air-cooled systems should both provide \\nimportant data points upstream to higher level monitoring \\nsolutions through standard industrial protocols for easier \\nintegration into common existing interfaces to provide a \\nwholistic overview of system status. This enables MTDC \\nand enterprise facility operations teams to easily report on \\nthis information to meet customer and regulatory \\nrequirements. \\n \\ny\\nLeveraging protective system controls: For liquid-cooled \\nsystems, robust controls should provide not only exemplary \\ncooling unit component logic, but multi-device awareness \\nand system-level reaction capabilities. As heat densities \\nincrease, reaction time of the system in responding to \\nevents of a larger data scope (outside of the single unit \\ncontroller) become more important. Utilizing advanced \\nembedded controllers enable flexible and programmatic \\ninterfaces to collect non-traditional data sets for better IT \\ngear protection and full system energy efficiency.  \\nCross-device awareness through these interfaces enables \\nsystem-wide operation through setpoint sharing and \\nparallel protection of system flow and temperatures. \\nAutomated failover of redundant components and units \\nensure this protection while specific liquid cooling control \\nparameters allow easy implementation of energy efficiency \\ninitiatives.\\n \\ny\\nUsing extensive leak detection systems: Leak detection \\nsystems should be placed across system components \\nreporting into or off of a CDU and at critical locations \\nacross the piping system.  \\n \\nThey provide proactive alarms when fluid pressure drops at \\nnode, rack, and data center levels, indicating a leak. These \\nsystems may also identify where a leak occurs, enabling \\nteams to do controlled shutdowns of the CDU, the servers, \\nand its cooling systems, streamlining equipment \\ntroubleshooting and repair processes. Deploying systems \\nthat can manage temperature below the dew point is vital to \\nlimit condensation formation and avoid potential leaks.\\n \\ny\\nDeploying systems with integrated cybersecurity \\nmeasures: One of the top priorities of data center managers \\nis cybersecurity, according to Uptime Institute reports. \\nBased on survey responses, 43% of respondents have \\ninitiated or increased cybersecurity initiatives in response to \\nregulation changes, and 64% of respondents cited data \\nsecurity concerns as a main reason for not hosting mission-\\ncritical workloads in cloud data centers.2 This extends past \\nthe IT equipment and into the supporting infrastructure. \\nCDUs, RDHx, supporting power equipment and more all \\nrequire protection from cyber threats. Selecting equipment \\nfrom vendors that integrate protection into their units \\nremoves strain from the system managers and speeds \\ndeployment times.\\nOne caution: If the IT and facility team adopts non- or \\nlow-conductive fluids in the future, they will want to review and \\nenhance existing systems to ensure they can continue \\ndetecting any changes in fluid conditions.\\n2 Douglas Donnellan, et al., Uptime Institute Global Data Center Survey 2023, Uptime Institute, July 2023, pages 21-22,  \\nhttps://uptimeinstitute.com/resources/research-and-reports/uptime-institute-global-data-center-survey-results-2023\\n20\\nThe IT and facility team can leverage partner project services to prepare for on-site liquid cooling system installation, commissioning, \\nand startup. As a major infrastructure project, this requires careful planning and coordination. Project phases and services include: \\n \\ny\\nNavigating application design: The IT and facility team \\nwill leverage available services and work with key partners \\nto determine the scope of work that needs to be done on \\nsite by performing a thermal and electrical evaluation. This \\nprocess includes providing written instructions for electrical \\ncontractors on where to install or upgrade busbars and \\nPDU circuit breakers in remote power panels and \\ndistribution boards. For cooling, it might involve taking \\ndown walls, adding or redirecting piping, directing water \\nthrough cold water loops, and deploying manifolds. \\n \\ny\\nConducting off-site testing: The system integrator will \\ntest all systems off site to ensure they work properly before \\ninstalling them at the data center. The IT and facility team \\nshould pre-inspect equipment on arrival to ensure it \\nfunctions properly. \\n \\ny\\nProviding service project management: The IT and \\nfacility team should develop a document detailing who is \\nresponsible, accountable, consulted, and informed (RACI). \\nThis expanded group will include the IT, facility, health, and \\nsafety teams, the design consultant, equipment \\nmanufacturers, and mechanical, electrical, and  \\nIT consultants.  \\n \\nAfter creating the RACI document, the IT and facility team \\nshould work with on-site experts to create a health and \\nsafety plan that discusses tasks, methods, risks, and \\nmitigation strategies. Then, the IT and facility team can \\ncreate a site plan with the design consultant, including all \\nhydraulic, mechanical, and electrical designs for the  \\ntarget space.  \\n \\nFinally, the IT and facility team will want to create \\noperational guides that teams and partners will use for \\nservicing, troubleshooting, and emergency maintenance. By \\ncreating defined processes, implementing controls, and \\nmaintaining strict governance, the group responsible for \\ndeploying the air-liquid hybrid cooling system can mitigate \\nrisks and ensure a safe and effective implementation and  \\nhandover process. \\n \\ny\\nInstalling the new system: The external integration team \\nwill then implement the new hybrid cooling infrastructure, \\nleveraging all guidance documents and preparation work. \\nThe IT and facility team should ensure that any external \\nfirms working with the liquid cooling project deploy only \\ncertified engineers to the site.  \\n \\nOne caution is to keep the space and equipment clean to \\nbe ready for commissioning and startup. The integration \\nteam should install end caps in all open pipework, \\nmanifolds, and connectors.\\n \\ny\\nStarting up systems: To ensure that the new hybrid \\ncooling infrastructure is ready for use, contractors will \\nconduct a series of tests, including L3 (startup and leak \\ndetection testing), L4 (site acceptance testing, which may \\nrequire specialist load bank testing), and L5 (integration \\ntesting). The integration testing will determine that cooling, \\npower, room, and monitoring systems are all working \\neffectively together to manage target heat loads that \\nrepresent the daily operating conditions systems will be \\nrequired to support. Contractors will also provide site data \\nand closeout reports. \\n \\ny\\nHanding over the new system: Equipment manufacturers \\ncan train the IT and facility team on new systems to \\nunderstand how to operate and maintain systems \\neffectively and what to do when they detect \\nperformance anomalies. \\nConducting Ongoing Maintenance \\nLiquid cooling systems need to be constantly monitored and \\ncontrolled. Teams will want to ensure that all fluids used in \\nsystems are high-quality and consistent across applications \\nand that any leaks are proactively identified and mitigated. \\nEquipment manufacturers can provide ongoing services to \\nprotect costly liquid cooling systems and ensure effective \\noperations. These services include: \\nLeveraging Project Services  \\nTo Get Ready for New Capacity Turn-on\\n21\\nData center owners and operators have many decisions to make at the rack and row level as power densities increase at their \\nexisting data centers to meet business and customer demands for HPC workloads like analytics and AI. \\nNavigating the critical infrastructure changes associated with higher rack power densities can be a daunting task. IT and facility \\nteams can tap the expertise of design consultants to design hybrid cooling systems that meet their business and technical needs. \\nThey can also access experts at companies that manufacture critical infrastructure thermal management solutions to learn which \\nsolutions best meet their requirements and budgets. \\nBefore recommending liquid cooling solutions, Vertiv works with customers to answer critical questions about their facilities, \\nworkloads, and more. Vertiv provides products that support the entire thermal management chain (air cooling, liquid cooling, and \\noutdoor heat rejection), power distribution equipment, racks and containment, and remote monitoring software to provide end-to-\\nend solutions and simplify procurement and other lifecycle processes. Finally, customers can leverage our remote monitoring, \\npreventive maintenance, and troubleshooting services to keep liquid cooling solutions performing at peak levels. \\nContact Vertiv for a complimentary discovery call, where we’ll \\nconsult with you to learn about your requirements and work \\nwith you to develop a solution that will meet your current \\nneeds and position your facility for future growth. \\nSchedule your no-cost discovery session today. \\nReady to buy?  \\nContact Vertiv for support in \\ndeveloping the optimal \\nhybrid cooling infrastructure \\ndesign to meet your business \\nand site requirements.\\n \\ny\\nConducting remote monitoring: Equipment \\nmanufacturers can provide a remote web portal so IT and \\nfacility teams can manage liquid cooling systems \\nworldwide. On-site teams will respond to local alarms from \\ntheir building management system, while the partner will \\nprovide condition-based monitoring. By identifying \\nanomalies, the partner can recommend and conduct \\nproactive and preventive maintenance to prevent leaks and \\nsystem breakdowns. \\n \\ny\\nPerforming preventive maintenance: During routine \\non-site visits, a partner will perform mechanical, electric, \\nand hydraulic visual and control checks, maintain pumps, \\nreplace filters, and check for any signs of corrosion. Fluid \\nmanagement can occur during these visits. Partners will \\nmanage systems to achieve predetermined SLAs and \\nprovide ongoing reports. \\n \\ny\\nManaging fluids: Any fluid that passes through manifolds \\nneeds to be exceptionally pure. A partner should sample \\nfluids at least twice a year, send it to a lab for testing, and \\ntake any corrective actions to restore fluid quality.\\n \\ny\\nOverseeing asset management: Companies that rely on \\nliquid cooled servers in their global data centers will likely \\nseek to outsource system management. A partner can \\nmanage worldwide installations, meet SLAs, and provide \\nreporting on when fluids and systems were last checked \\nand what maintenance has been performed. \\nIt's important to note that certain IT equipment vendors will \\nhave specific requirements related to the safe operation of \\ntheir products, including the application of liquid cooling \\nsystems. It is recommended to verify any proposed solution \\nwith the IT equipment vendor to avoid potential voided \\nwarranty coverage.\\nMoving Forward \\nWith Hybrid Cooling\\n22\\nThe chart below provides guidance to teams getting ready to source and procure liquid cooling equipment. It lists product groups \\nthat should be purchased at the same time to streamline procurement and integration processes. \\nProduct Type\\nInstall Type\\nDescription\\nCategory\\nBudgeting\\nIT Rack\\nNew\\nRacks support various equipment, including servers, storage,  \\nswitches, routers, PDUs, UPS units, console port servers, and  \\nKVM switches.\\nRacks & \\nContainment\\nThe IT and facility teams should \\nwork together to evaluate these \\nsolutions to ensure they fit the \\nthermal management solution \\nCable and Airflow \\nManagement\\nNew\\nVertical cable manager accessories for the rack; provide a  \\nvertical row of cable management fingers.\\nRacks & \\nContainment\\nRack Cable Trough\\nNew\\nCable trough for overhead power and network cabling.\\nRacks & \\nContainment\\nBlanking Panels\\nNew\\nCloses unused U space within the rack, promoting proper  \\nairflow and minimizing bypass air.\\nRacks & \\nContainment\\nCoolant  \\nDistribution Unit\\nNew and Retrofit\\nDesigned to remove barriers to liquid cooling in an air-cooled \\nenvironment. CDUs are available as liquid-to-liquid heat exchanger \\nfor chip and rear-door cooling applications that offer easy,  \\ncost-effective deployment in any data center.\\nLiquid Cooling\\nIT will provide power-density \\ninformation, while the design \\nconsultant will provide input.  \\nThese solutions should be bought \\nearly in the design selection.\\nCDU Accessories\\nNew and Retrofit\\nWater detection cables provide integrated leak detection for  \\nthe CDU.\\nLeak Detection\\nCDU Hoses\\nNew and Retrofit\\nOne pair each for primary and secondary distribution.\\nLiquid Cooling\\nDistribution Manifold\\nNew and Retrofit\\nIn-row manifolds enable support of liquid-cooled IT  \\nequipment by distributing fluid from CDUs to individual racks.\\nLiquid Cooling\\nRack Manifold Set\\nNew and Retrofit\\nThe in-rack manifold provides supply and return to connect liquid \\nbetween the row/room facility cooling source and the server  \\ncold plates.\\nLiquid Cooling\\nIndoor Chiller\\nRetrofit\\nWorking in tandem with a RDHx, indoor chillers integrate pumped \\nrefrigerant economization to achieve higher efficiency. They quickly \\nand efficiently cool pods of high-density racks in an air-cooled data \\ncenter. Liquid-to-air heat exchangers are located in the rack  \\nrear door.\\nHeat Rejection\\nIt may be an option based on data \\ncenter team needs and \\npreferences.\\nHigh Density \\nCondenser\\nRetrofit\\nA rooftop high-density condenser designed for use with an  \\nindoor chiller.\\nHeat Rejection\\nThe condenser will be sized \\ndepending on ambient conditions \\nand the thermal capacity required.\\nFreecooling Chiller\\nNew\\nAn outdoor freecooling chiller that provides chilled water to CRAHs in \\nan already existing solution. If correctly sized it can also be connected \\ndirectly with the CDUs. A new larger freecooling chiller can be \\nimplemented to both supply cooling capacity to CRAHs and CDUs. \\nHeat Rejection\\nRack PDU\\nNew and Retrofit\\nIn-rack PDUs with switched outlet-level monitoring offer  \\nprotection for IT equipment.\\nPower Distribution\\nThe design consultant, the \\nspecifying engineer, and the facility \\nand power teams should work \\ntogether to align the liquid cooling \\nsolution to power specifications. \\nThese solutions should be bought \\nat the same time as cooling \\nproducts. \\nBusway\\nNew and Retrofit\\nDistributes power from the facility to IT cabinets.\\nPower Distribution\\nBusway Components\\nNew and Retrofit\\nTap-off box and breakers to support rack PDU and A+B feeds.\\nPower Distribution\\nPDU and-or RPP\\nNew and Retrofit\\nFloor-mounted PDU and RPP distribute power to the busway and \\ndirect rack connections.\\nPower Distribution\\nSwitchgear\\nNew or Retrofit\\nEnsures proper flow of power, protects equipment, and isolates data \\ncenter power systems.\\nPower Distribution\\nUPS\\nNew\\nStandalone floor-mounted UPS units provide optimized power per \\nfootprint, offering high efficiency, robust electrical protection, and \\nintelligent paralleling to enhance performance at partial loads.\\nUPS\\nUPS Batteries \\n(Greenfield only)\\nNew\\nLithium-ion batteries for the UPS provide the best TCO.\\nUPS\\nAppendix A: Creating the Optimal System Design\\n23\\nAppendix B: Glossary\\nCoolant distribution unit (CDU) – CDUs distribute fluids from \\nthe chiller to the data hall to the CDU, racks, and back via a \\nsecondary loop separate from the chilled water supply. CDUs can \\nbe used with RDHx, cold plates, and immersion cooling. They \\nprovide fluid filtration capabilities to protect equipment and offer \\nflow rate and temperature monitoring, enabling teams to adjust \\nboth tooptimize operations.\\nComputational fluid dynamics (CFD) – A numerical calculation \\nperformed to simulate both turbulent and laminar flow, providing \\npressure, density, and temperature values across a volume. \\nFor data centers, this mean analyzing air-cooling and liquid-\\ncooling equipment to determine current and future cooling \\ncapacity, air and fluid temperatures and flow rates, and to \\ndetermine if existing piping is sufficient for the hybrid cooling \\ninfrastructure.\\nDirect-to-chip (DTC) liquid cooling – DTC cold plates sit atop \\nthe board’s heat-generating components to draw off heat through \\nsingle-phase cold plates or two-phase evaporation units. These \\ncooling technologies can remove about 70-75% of the heat \\ngenerated by the equipment in the rack, leaving 25-30% that must \\nbe removed by air-cooling systems.\\nFlow network modeling – An analysis process used to calculate \\nfull-system flow rates and temperatures. Data centers benefit \\nmassively from this type of study by validating current and \\nproposed fluid network designs, piping sizes, pump sizes, etc. This \\nhelps to ensure the infrastructure can support the IT deployed in \\nterms of cooling capacity. \\nHybrid cooling infrastructure – A combination of air-cooled and \\nliquid-cooled thermal management units designed to provide \\nincreased cooling capacity, efficiency and sustainability. \\nLiquid-to-air heat exchange – For a liquid-to-air system, the \\nCDU transfers the heat to the ambient environment, utilizing a \\nheat exchange coil design within the CDU to remove heat. The \\nproducts in this category do not require water pipes to be \\nconnected to the building system for heat rejection. These units \\nenable localized liquid cooling for high-output IT equipment but \\nleverage the technologies from existing data center cooling \\nsystems to dissipate heat. \\nLiquid-to-liquid heat exchange – For a liquid-to-liquid system, \\nthe CDU transfers heat from one liquid to another for heat \\nremoval. Typically this is done by passing the two fluids by each \\nother separated by a thin plate, typically in a plate heat exchanger. \\nLiquid-to-liquid CDUs yield the best cooling performance, \\nhowever, they require installing pipes and pumps to connect to \\nthe facility's water supply.\\nManifold (rack and row) – Manifolds are a fluid distribution \\nsystem that serves to bring coolant to the racks and the individual \\nservers in a data center.  \\n \\nRow manifolds include the plumbing supply and return system to \\ndistribute coolant from the CDU to the rack. This is also known as \\nthe secondary fluid network. Rack manifolds include the supply \\nand return system to distribute coolant from the secondary fluid \\nnetwork to the liquid cooled servers in the rack. \\nPower usage effectiveness (PUE) – A metric used to determine \\nthe overall energy efficiency of a data center. PUE is determined \\nby dividing the total facility energy usage by the IT equipment \\nenergy usage. The lower a data center’s PUE value, the more \\nefficiently it is operating. The target value is 1.0 with the average \\nvalue for data centers falling under 1.6. \\nRear-door heat exchangers (RDHx)\\u202f– Passive or active heat \\nexchangers replace the rear door of the IT equipment rack with a \\nliquid heat exchanger. These systems can be used in conjunction \\nwith air-cooling systems to cool environments with mixed rack \\ndensities. \\nRedundancy (N, N+1, N+2, 2N) – additional units deployed to \\navoid the risk associated with unit failure. N is defined as the \\nminimum infrastructure (power, cooling, etc.) required to support \\nthe IT deployment in the data center and does not include any \\nextra units. Systems with N+1 and N+2 redundancy have an extra \\none or two infrastructure units deployed, respectively. Systems \\nwith 2N redundancy have full 1:1 redundancy built in for each \\ninfrastructure component in the data center. \\nScope 2 emissions – Indirect greenhouse emissions associated \\nwith the purchase of electricity, steam, heat, or cooling. Though \\nthe processes to produce these occur at the facility where they \\nare generated, the organization is required to account for these as \\na result of the data center’s use of the energy. \\nThermal design power (TDP) – The maximum amount of heat \\ngenerated by a central processing unit (CPU), GPU, or chipset, as \\nmeasured in watts. The higher the thermal design power of a \\ncomponent, the more cooling capacity is needed to maintain \\nefficient operation.\\nTotal cost of ownership (TCO) – The amount required to \\npurchase equipment plus the costs of operation and maintenance. \\nBy deploying more efficient equipment in the data center, teams \\ncan lower the cost of operation, and thus reduce TCO over the life \\nof the product.\\nWater usage effectiveness (WUE) – The ratio between the \\namount of water used in the data center systems (water loops, \\nadiabatic towers, humidification, etc.) and the energy consumption \\nof the IT equipment. The lower a data center’s WUE ratio, the \\nmore efficient its use of water resources. The average data center \\nusing evaporative cooling technologies has a WUE of 1.8 liters/kW.\\nVertiv.com   |\\t Vertiv Headquarters, 505 N Cleveland Ave, Westerville, OH 43082, USA\\n© 2024 Vertiv Group Corp. All rights reserved. Vertiv™ and the Vertiv logo are trademarks or registered trademarks of Vertiv Group Corp. All other names and logos referred to are trade names, trademarks or registered trademarks of their respective owners. While every \\nprecaution has been taken to ensure accuracy and completeness here, Vertiv Group Corp. assumes no responsibility, and disclaims all liability, for damages resulting from use of this information or for any errors or omissions. Specifications, rebates and other promotional \\noffers are subject to change at Vertiv’s sole discretion upon notice.\\nSL-71113  (03/24)\\n\", 'MergeIT.uk\\nSustainable App Adjacent \\nVDI for AI & HPC Workloads\\n• \\nImmersion Cooled Workloads for \\n \\nAI/ML/HPC/VDI\\n• \\n1200x more power efficient than \\n \\ntraditional air cooling\\n• \\nHeat recapture and reuse to drive \\n \\ncarbon efficiency\\n• \\nProviding high performance Virtual \\n \\nDesktops for Data Scientists & Data \\n \\nEngineers\\n• \\nApplication adjacent Virtual Desktops for \\n       increased performance and reduced  \\n \\n       power consumption\\n• \\nOur dynamic, modular, scale up, scale out \\n      architecture is designed for AI/ML/HPC/VDI with \\n      the only change to the stack being the GPU\\n• \\nWe can ingest data locally into the cluster and \\n       provide Model Training, Inference, Data  \\n       Analytics, Data Visualisation & VDI from a single  \\n       source, fully immersed\\n• \\nMergeIT has a strategic partnership with \\n \\nCastrolON & GRC (Green Revolution Cooling)\\n', 'ON Immersion Cooling Fluid DC 20\\nSAFETY DATA SHEET\\nProduct name\\n1.1 Product identifier\\n1.3 Details of the supplier of the safety data sheet\\nLiquid.\\nProduct type\\nE-mail address\\nMSDSadvice@bp.com\\n1.2 Relevant identified uses of the substance or mixture and uses advised against\\nGeneral use of lubricants and greases in vehicles or machinery-Industrial\\nGeneral use of lubricants and greases in vehicles or machinery-Professional\\nIdentified uses\\nSECTION 1: Identification of the substance/mixture and of the company/undertaking\\nProduct code\\n470661-DE01\\n1.4 Emergency telephone number\\nEMERGENCY \\nTELEPHONE NUMBER\\nCarechem: +44 (0) 1235 239 670 (24/7)\\nSupplier\\nBP Europa SE\\nGeschäftsbereich Industrieschmierstoffe\\nErkelenzer Straße 20\\nD-41179 Mönchengladbach\\nGermany\\nTelefon: +49 (0)800 7235-074\\nSDS #\\n470661\\nUse of the substance/\\nmixture\\nCoolant lubricant\\nFor specific application advice see appropriate Technical Data Sheet or consult our company \\nrepresentative.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nUFI:\\nQ0R1-H0WQ-X00D-W2JU\\nSee sections 11 and 12 for more detailed information on health effects and symptoms and environmental hazards.\\nClassification according to Regulation (EC) No. 1272/2008 [CLP/GHS]\\nSECTION 2: Hazards identification\\n2.1 Classification of the substance or mixture\\nProduct definition\\nMixture\\nSee Section 16 for the full text of the H statements declared above.\\n2.2 Label elements\\nHazard pictograms\\nSignal word\\nHazard statements\\nPrevention\\nPrecautionary statements\\nDanger\\nH304 - May be fatal if swallowed and enters airways.\\nH332 - Harmful if inhaled.\\nP271 - Use only outdoors or in a well-ventilated area.\\nP261 - Avoid breathing vapour.\\nAcute Tox. 4, H332\\nAsp. Tox. 1, H304\\nGeneral\\nP102 - Keep out of reach of children.\\nP101 - If medical advice is needed, have product container or label at hand.\\nUFI:\\nQ0R1-H0WQ-X00D-W2JU\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 1/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 2: Hazards identification\\nResponse\\nStorage\\nDisposal\\nP304 + P312 - IF INHALED: Call a POISON CENTER or doctor if you feel unwell.\\nP301 + P310, P331 - IF SWALLOWED: Immediately call a POISON CENTER or physician.  Do \\nNOT induce vomiting.\\nP405 - Store locked up.\\nP501 - Dispose of contents and container in accordance with all local, regional, national and \\ninternational regulations.\\nSupplemental label \\nelements\\nContainers to be fitted \\nwith child-resistant \\nfastenings\\nYes, applicable.\\nTactile warning of danger\\nYes, applicable.\\nNot applicable.\\nSpecial packaging requirements\\nHazardous ingredients\\nDec-1-ene, dimers, hydrogenated\\n2.3 Other hazards\\nEU Regulation (EC) No. 1907/2006 (REACH)\\nAnnex XVII - Restrictions \\non the manufacture,\\nplacing on the market \\nand use of certain \\ndangerous substances,\\nmixtures and articles\\nNot applicable.\\nResults of PBT and vPvB \\nassessment\\nProduct does not meet the criteria for PBT or vPvB according to Regulation (EC) No. 1907/2006,\\nAnnex XIII.\\nProduct meets the criteria \\nfor PBT or vPvB according \\nto Regulation (EC) No.\\n1907/2006, Annex XIII\\nThis mixture does not contain any substances that are assessed to be a PBT or a vPvB.\\nSECTION 3: Composition/information on ingredients\\nThis product does not contain any hazardous ingredients at or above regulated thresholds.\\nSynthetic base stock. Proprietary performance additives.\\nSee Section 16 for the full text of the H statements declared above.\\nMixture\\n3.2 Mixtures\\nProduct definition\\nDec-1-ene, dimers,\\nhydrogenated\\nREACH #:\\n01-2119493069-28\\nEC: 500-228-5\\nCAS: 68649-11-6\\n≥90\\nAcute Tox. 4, H332\\nAsp. Tox. 1, H304\\nATE [Inhalation \\n(dusts and mists)] = \\n1.5 mg/l\\n[1] [2]\\n[1] Substance classified with a health or environmental hazard\\n[2] Substance with a workplace exposure limit\\nProduct/ingredient name\\nIdentifiers\\n%\\nClassification\\nSpecific Conc.\\nLimits, M-factors \\nand ATEs\\nType\\nDo not induce vomiting.  Never give anything by mouth to an unconscious person.  If \\nunconscious, place in recovery position and get medical attention immediately.  Aspiration \\nhazard if swallowed.  Can enter lungs and cause damage.  Get medical attention immediately.\\nIn case of contact, immediately flush eyes with plenty of water for at least 15 minutes.  Eyelids \\nshould be held away from the eyeball to ensure thorough rinsing.  Check for and remove any \\ncontact lenses.  Get medical attention if irritation develops.\\n4.1 Description of first aid measures\\nIf inhaled, remove to fresh air.  If not breathing, if breathing is irregular or if respiratory arrest \\noccurs, provide artificial respiration or oxygen by trained personnel.  Get medical attention if \\nsymptoms occur.\\nIngestion\\nInhalation\\nEye contact\\nSECTION 4: First aid measures\\nSkin contact\\nFlush contaminated skin with plenty of water.  Remove contaminated clothing and shoes.\\nWash clothing before reuse.  Clean shoes thoroughly before reuse.  Get medical attention if \\nirritation develops.\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 2/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 4: First aid measures\\nNotes to physician\\nTreatment should in general be symptomatic and directed to relieving any effects.\\nProduct can be aspirated on swallowing or following regurgitation of stomach contents, and can \\ncause severe and potentially fatal chemical pneumonitis, which will require urgent treatment.\\nBecause of the risk of aspiration, induction of vomiting and gastric lavage should be avoided.\\nGastric lavage should be undertaken only after endotracheal intubation.  Monitor for cardiac \\ndysrhythmias.\\nProtection of first-aiders\\nNo action shall be taken involving any personal risk or without suitable training.  If it is \\nsuspected that fumes are still present, the rescuer should wear an appropriate mask or self-\\ncontained breathing apparatus.  It may be dangerous to the person providing aid to give mouth-\\nto-mouth resuscitation.\\n4.2 Most important symptoms and effects, both acute and delayed\\n4.3 Indication of any immediate medical attention and special treatment needed\\nSee Section 11 for more detailed information on health effects and symptoms.\\nPotential acute health effects\\nInhalation\\nHarmful if inhaled.\\nAspiration hazard if swallowed -- harmful or fatal if liquid is aspirated into lungs.\\nIngestion\\nSkin contact\\nNo known significant effects or critical hazards.\\nNo known significant effects or critical hazards.\\nEye contact\\nDelayed and immediate effects as well as chronic effects from short and long-term exposure\\nInhalation\\nIngestion\\nSkin contact\\nEye contact\\nOverexposure to the inhalation of airborne droplets or aerosols may cause irritation of the \\nrespiratory tract.\\nIngestion of large quantities may cause nausea and diarrhoea.\\nProlonged or repeated contact can defat the skin and lead to irritation and/or dermatitis.\\nPotential risk of transient stinging or redness if accidental eye contact occurs.\\nNo action shall be taken involving any personal risk or without suitable training.  Promptly \\nisolate the scene by removing all persons from the vicinity of the incident if there is a fire.\\nHazardous combustion \\nproducts\\nHazards from the \\nsubstance or mixture\\nCombustion products may include the following:\\ncarbon oxides (CO, CO2) (carbon monoxide, carbon dioxide)\\nIn a fire or if heated, a pressure increase will occur and the container may burst.\\nFire-fighters should wear appropriate protective equipment and self-contained breathing \\napparatus (SCBA) with a full face-piece operated in positive pressure mode.  Clothing for fire-\\nfighters (including helmets, protective boots and gloves) conforming to European standard EN \\n469 will provide a basic level of protection for chemical incidents.\\nSpecial protective \\nequipment for fire-fighters\\nIn case of fire, use foam, dry chemical or carbon dioxide extinguisher or spray.\\n5.1 Extinguishing media\\nDo not use water jet.  The use of a water jet may cause the fire to spread by splashing the \\nburning product.\\nSuitable extinguishing \\nmedia\\nUnsuitable extinguishing \\nmedia\\nSECTION 5: Firefighting measures\\n5.2 Special hazards arising from the substance or mixture\\n5.3 Advice for firefighters\\nSpecial precautions for \\nfire-fighters\\nSECTION 6: Accidental release measures\\n6.1 Personal precautions, protective equipment and emergency procedures\\nFor non-emergency \\npersonnel\\nFor emergency responders\\nContact emergency personnel.  No action shall be taken involving any personal risk or without \\nsuitable training.  Evacuate surrounding areas.  Keep unnecessary and unprotected personnel \\nfrom entering.  Do not touch or walk through spilt material.  Floors may be slippery; use care to \\navoid falling.  Avoid breathing vapour or mist.  Provide adequate ventilation.  Put on appropriate \\npersonal protective equipment.\\nEntry into a confined space or poorly ventilated area contaminated with vapour, mist or fume is \\nextremely hazardous without the correct respiratory protective equipment and a safe system of \\nwork.  Wear self-contained breathing apparatus.  Wear a suitable chemical protective suit.\\nChemical resistant boots.  See also the information in \"For non-emergency personnel\".\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 3/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 6: Accidental release measures\\n6.2 Environmental \\nprecautions\\nStop leak if without risk.  Move containers from spill area.  Approach the release from upwind.\\nPrevent entry into sewers, water courses, basements or confined areas.  Contain and collect \\nspillage with non-combustible, absorbent material e.g. sand, earth, vermiculite or diatomaceous \\nearth and place in container for disposal according to local regulations.  Contaminated \\nabsorbent material may pose the same hazard as the spilt product.  Dispose of via a licensed \\nwaste disposal contractor.\\nAvoid dispersal of spilt material and runoff and contact with soil, waterways, drains and sewers.\\nInform the relevant authorities if the product has caused environmental pollution (sewers,\\nwaterways, soil or air).\\nLarge spill\\nStop leak if without risk.  Move containers from spill area.  Absorb with an inert material and \\nplace in an appropriate waste disposal container.  Dispose of via a licensed waste disposal \\ncontractor.\\nSmall spill\\n6.3 Methods and material for containment and cleaning up\\n6.4 Reference to other \\nsections\\nSee Section 1 for emergency contact information.\\nSee Section 5 for firefighting measures.\\nSee Section 8 for information on appropriate personal protective equipment.\\nSee Section 12 for environmental precautions.\\nSee Section 13 for additional waste treatment information.\\nStore in accordance with local regulations.  Store in a dry, cool and well-ventilated area, away \\nfrom incompatible materials (see Section 10).  Store locked up.  Keep away from heat and \\ndirect sunlight.  Keep container tightly closed and sealed until ready for use.  Containers that \\nhave been opened must be carefully resealed and kept upright to prevent leakage.  Store and \\nuse only in equipment/containers designed for use with this product.  Do not store in unlabelled \\ncontainers.\\nSECTION 7: Handling and storage\\nThe information in this section contains generic advice and guidance. The list of Identified Uses in Section 1 should be consulted \\nfor any available use-specific information provided in the Exposure Scenario(s).\\n7.1 Precautions for safe handling\\nProtective measures\\nAdvice on general \\noccupational hygiene\\n7.2 Conditions for safe \\nstorage, including any \\nincompatibilities\\n7.3 Specific end use(s)\\nRecommendations\\nPut on appropriate personal protective equipment.  Do not swallow.  Aspiration hazard if \\nswallowed. Can enter lungs and cause damage.  Never siphon by mouth.  Avoid contact with \\neyes, skin and clothing.  Avoid breathing vapour or mist.  Use only with adequate ventilation.\\nWear appropriate respirator when ventilation is inadequate.  Keep in the original container or \\nan approved alternative made from a compatible material, kept tightly closed when not in use.\\nDo not reuse container.  Empty containers retain product residue and can be hazardous.\\nDuring metal working, solid particles from workpieces or tools will contaminate the fluid and \\nmay cause abrasions of the skin.  Where such abrasions result in a penetration of the skin, first \\naid treatment should be applied as soon as reasonably possible.  The presence of certain \\nmetals in the workpiece or tool, such as chromium, cobalt and nickel, can contaminate the \\nmetalworking fluid and as a result may induce allergic skin reactions.  Small quantities of \\npolycyclic aromatic hydrocarbons (PCAs), some of which are known to be carcinogenic, may be \\ngenerated during use.  Emissions into the workplace air may be controlled by ensuring that the \\nelectrode is adequately covered with product above the point of spark erosion (a depth of 80 \\nmm is recommended). This depth will ensure that most emissions are condensed within the \\nproduct, that excessive product vapourisation does not occur, and that the possibility of fire is \\nreduced.\\nEating, drinking and smoking should be prohibited in areas where this material is handled,\\nstored and processed.  Wash thoroughly after handling.  Remove contaminated clothing and \\nprotective equipment before entering eating areas.  See also Section 8 for additional \\ninformation on hygiene measures.\\nGermany - Storage code\\n10\\nNot suitable\\nProlonged exposure to elevated temperature\\nSee section 1.2 and Exposure scenarios in annex, if applicable.\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 4/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nProduct/ingredient name\\nExposure limit values\\nDec-1-ene, dimers, hydrogenated\\nDFG MAC-values list (Germany).\\n PEAK: 20 mg/m³, 4 times per shift,  15 minutes. Issued/Revised: 7/2010 \\nForm: respirable fraction\\n TWA: 5 mg/m³ 8 hours. Issued/Revised: 7/2010 Form: respirable fraction\\nRecommended monitoring \\nprocedures\\nIf this product contains ingredients with exposure limits, personal, workplace atmosphere or \\nbiological monitoring may be required to determine the effectiveness of the ventilation or other \\ncontrol measures and/or the necessity to use respiratory protective equipment.  Reference \\nshould be made to monitoring standards, such as the following:  European Standard EN 689 \\n(Workplace atmospheres - Guidance for the assessment of exposure by inhalation to chemical \\nagents for comparison with limit values and measurement strategy)  European Standard EN \\n14042 (Workplace atmospheres - Guide for the application and use of procedures for the \\nassessment of exposure to chemical and biological agents)  European Standard EN 482 \\n(Workplace atmospheres - General requirements for the performance of procedures for the \\nmeasurement of chemical agents)  Reference to national guidance documents for methods for \\nthe determination of hazardous substances will also be required.\\nHand protection\\nUse with adequate ventilation.\\nIn case of insufficient ventilation, wear suitable respiratory equipment.\\nFor protection against metal working fluids, respiratory protection that is classified as “resistant \\nto oil” (class R) or oil proof (class P) should be selected where appropriate. Depending on the \\nlevel of airborne contaminants, an air-purifying, half-mask respirator (with HEPA filter) including \\ndisposable (P- or R-series) (for oil mists less than 50mg/m3), or any powered, air-purifying \\nrespirator equipped with hood or helmet and HEPA filter (for oil mists less than 125 mg/m3).\\nWhere organic vapours are a potential hazard during metalworking operations, a combination \\nparticulate and organic vapour filter may be necessary.\\nThe correct choice of respiratory protection depends upon the chemicals being handled, the \\nconditions of work and use, and the condition of the respiratory equipment. Safety procedures \\nshould be developed for each intended application. Respiratory protection equipment should \\ntherefore be chosen in consultation with the supplier/manufacturer and with a full assessment \\nof the working conditions.\\nGeneral Information:\\nBecause specific work environments and material handling practices vary, safety procedures \\nshould be developed for each intended application. The correct choice of protective gloves \\ndepends upon the chemicals being handled, and the conditions of work and use. Most gloves \\nprovide protection for only a limited time before they must be discarded and replaced (even the \\nbest chemically resistant gloves will break down after repeated chemical exposures).\\nSafety glasses with side shields.\\nEye/face protection\\nRespiratory protection\\nSkin protection\\nAppropriate engineering \\ncontrols\\nProvide exhaust ventilation or other engineering controls to keep the relevant airborne \\nconcentrations below their respective occupational exposure limits.\\nAll activities involving chemicals should be assessed for their risks to health, to ensure \\nexposures are adequately controlled. Personal protective equipment should only be considered \\nafter other forms of control measures (e.g. engineering controls) have been suitably evaluated.\\nPersonal protective equipment should conform to appropriate standards, be suitable for use, be \\nkept in good condition and properly maintained.\\nYour supplier of personal protective equipment should be consulted for advice on selection and \\nappropriate standards.  For further information contact your national organisation for standards.\\nThe final choice of protective equipment will depend upon a risk assessment. It is important to \\nensure that all items of personal protective equipment are compatible.\\nWash hands, forearms and face thoroughly after handling chemical products, before eating,\\nsmoking and using the lavatory and at the end of the working period.  Ensure that eyewash \\nstations and safety showers are close to the workstation location.\\n8.2 Exposure controls\\nHygiene measures\\nNo DNELs/DMELs available.\\nPredicted No Effect Concentration\\nNo PNECs available\\nSECTION 8: Exposure controls/personal protection\\nThe information in this section contains generic advice and guidance. The list of Identified Uses in Section 1 should be \\nconsulted for any available use-specific information provided in the Exposure Scenario(s).\\n8.1 Control parameters\\nDerived No Effect Level\\nIndividual protection measures\\nOccupational exposure limits\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 5/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 8: Exposure controls/personal protection\\nGloves should be chosen in consultation with the supplier / manufacturer and taking account of \\na full assessment of the working conditions.\\nRecommended:  Nitrile gloves.\\nBreakthrough time:\\nBreakthrough time data are generated by glove manufacturers under laboratory test conditions \\nand represent how long a glove can be expected to provide effective permeation resistance. It \\nis important when following breakthrough time recommendations that actual workplace \\nconditions are taken into account. Always consult with your glove supplier for up-to-date \\ntechnical information on breakthrough times for the recommended glove type.\\nOur recommendations on the selection of gloves are as follows:\\nContinuous contact:\\nGloves with a minimum breakthrough time of 240 minutes, or >480 minutes if suitable gloves \\ncan be obtained.\\nIf suitable gloves are not available to offer that level of protection, gloves with shorter \\nbreakthrough times may be acceptable as long as appropriate glove maintenance and \\nreplacement regimes are determined and adhered to.\\nShort-term / splash protection:\\nRecommended breakthrough times as above.\\nIt is recognised that for short-term, transient exposures, gloves with shorter breakthrough times \\nmay commonly be used. Therefore, appropriate maintenance and replacement regimes must \\nbe determined and rigorously followed.\\nGlove Thickness:\\nFor general applications, we recommend gloves with a thickness typically greater than 0.35 mm.\\nIt should be emphasised that glove thickness is not necessarily a good predictor of glove \\nresistance to a specific chemical, as the permeation efficiency of the glove will be dependent \\non the exact composition of the glove material. Therefore, glove selection should also be based \\non consideration of the task requirements and knowledge of breakthrough times.\\nGlove thickness may also vary depending on the glove manufacturer, the glove type and the \\nglove model. Therefore, the manufacturers’ technical data should always be taken into account \\nto ensure selection of the most appropriate glove for the task.\\nNote: Depending on the activity being conducted, gloves of varying thickness may be required \\nfor specific tasks. For example:\\n  • Thinner gloves (down to 0.1 mm or less) may be required where a high degree of manual \\ndexterity is needed. However, these gloves are only likely to give short duration protection and \\nwould normally be just for single use applications, then disposed of.\\n  • Thicker gloves (up to 3 mm or more) may be required where there is a mechanical (as well \\nas a chemical) risk i.e. where there is abrasion or puncture potential.\\nUse of protective clothing is good industrial practice.\\nPersonal protective equipment for the body should be selected based on the task being \\nperformed and the risks involved and should be approved by a specialist before handling this \\nproduct.\\nCotton or polyester/cotton overalls will only provide protection against light superficial \\ncontamination that will not soak through to the skin.  Overalls should be laundered on a regular \\nbasis.  When the risk of skin exposure is high (e.g. when cleaning up spillages or if there is a \\nrisk of splashing) then chemical resistant aprons and/or impervious chemical suits and boots \\nwill be required.\\nSkin and body\\nRefer to standards:\\nRespiratory protection: EN 529\\nGloves: EN 420, EN 374\\nEye protection: EN 166\\nFiltering half-mask: EN 149\\nFiltering half-mask with valve: EN 405\\nHalf-mask: EN 140 plus filter\\nFull-face mask: EN 136 plus filter\\nParticulate filters: EN 143\\nGas/combined filters: EN 14387\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 6/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 8: Exposure controls/personal protection\\nEnvironmental exposure \\ncontrols\\nEmissions from ventilation or work process equipment should be checked to ensure they \\ncomply with the requirements of environmental protection legislation.  In some cases, fume \\nscrubbers, filters or engineering modifications to the process equipment will be necessary to \\nreduce emissions to acceptable levels.\\nPhysical state\\nMelting point/freezing point\\nInitial boiling point and boiling \\nrange\\nVapour pressure\\nRelative density\\nRelative vapour density\\nLiquid.\\nNot available.\\nNot available.\\nNot available.\\nNot available.\\nNot available.\\nOdour\\npH\\nColourless.\\nColour\\nEvaporation rate\\nNot available.\\nAuto-ignition temperature\\nFlash point\\nNot available.\\nNot available.\\nNot applicable.\\nNot applicable.\\nViscosity\\nKinematic: 4.8 to 5.4 mm2/s (4.8 to 5.4 cSt) at 40°C\\nKinematic: 1.6 to 1.8 mm2/s (1.6 to 1.8 cSt) at 100°C \\nNot available.\\nOdour threshold\\nPartition coefficient: n-octanol/\\nwater\\nExplosive properties\\nNot available.\\nOxidising properties\\n9.1 Information on basic physical and chemical properties\\nAppearance\\n9.2 Other information\\nDecomposition temperature\\nNot available.\\nSECTION 9: Physical and chemical properties\\nFlammability (solid, gas)\\nNot available.\\nNo additional information.\\nPour point\\n-75 °C\\nDensity\\n<1000 kg/m³ (<1 g/cm³) at 15°C\\nParticle characteristics\\nMedian particle size\\nNot applicable.\\nIngredient name\\n°C\\n°F\\nMethod\\nDec-1-ene, dimers, hydrogenated\\n324\\n615.2\\nASTM D 2158\\nThe conditions of measurement of all properties are at standard temperature and pressure unless otherwise indicated.\\nIngredient name\\nVapour Pressure at 20˚C\\nVapour pressure at 50˚C\\nmm Hg kPa\\nMethod\\nmm \\nHg\\nkPa\\nMethod\\nDec-1-ene, dimers,\\nhydrogenated\\n0.01\\n0.0013\\nASTM E \\n1194-87\\nNot available.\\nClosed cup: >150°C (>302°F) [Pensky-Martens ASTM D 93]\\nSolubility(ies)\\nMedia\\nResult\\nwater\\nNot soluble\\nLower and upper explosion \\nlimit\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 7/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\n10.6 Hazardous \\ndecomposition products\\n10.4 Conditions to avoid\\nAvoid all possible sources of ignition (spark or flame).\\nUnder normal conditions of storage and use, hazardous decomposition products should not be \\nproduced.\\nThe product is stable.\\n10.2 Chemical stability\\n10.5 Incompatible materials\\n10.3 Possibility of \\nhazardous reactions\\nUnder normal conditions of storage and use, hazardous reactions will not occur.\\nUnder normal conditions of storage and use, hazardous polymerisation will not occur.\\nSECTION 10: Stability and reactivity\\n10.1 Reactivity\\nNo specific test data available for this product.  Refer to Conditions to avoid and Incompatible \\nmaterials for additional information.\\nReactive or incompatible with the following materials: oxidising materials.\\nPotential chronic health effects\\nPotential acute health effects\\nInhalation\\nHarmful if inhaled.\\nAspiration hazard if swallowed -- harmful or fatal if liquid is aspirated into lungs.\\nIngestion\\nSkin contact\\nNo known significant effects or critical hazards.\\nNo known significant effects or critical hazards.\\nEye contact\\nUsed product\\nSmall quantities of polycyclic aromatic hydrocarbons (PCAs), some of which are known to be \\ncarcinogenic, may be generated during use.  Avoid skin contact with used fluid and \\ncontaminated filters.\\nGeneral\\nNo known significant effects or critical hazards.\\nCarcinogenicity\\nNo known significant effects or critical hazards.\\nMutagenicity\\nDevelopmental effects\\nNo known significant effects or critical hazards.\\nFertility effects\\nNo known significant effects or critical hazards.\\nSymptoms related to the physical, chemical and toxicological characteristics\\nSkin contact\\nIngestion\\nInhalation\\nAdverse symptoms may include the following:\\nnausea or vomiting\\nheadache\\ndrowsiness/fatigue\\ndizziness/vertigo\\nunconsciousness\\nAdverse symptoms may include the following:\\nnausea or vomiting\\nNo specific data.\\nEye contact\\nNo specific data.\\nRoutes of entry anticipated: Dermal, Inhalation.\\nSECTION 11: Toxicological information\\nInformation on likely \\nroutes of exposure\\nDelayed and immediate effects as well as chronic effects from short and long-term exposure\\nInhalation\\nIngestion\\nSkin contact\\nEye contact\\nOverexposure to the inhalation of airborne droplets or aerosols may cause irritation of the \\nrespiratory tract.\\nIngestion of large quantities may cause nausea and diarrhoea.\\nProlonged or repeated contact can defat the skin and lead to irritation and/or dermatitis.\\nPotential risk of transient stinging or redness if accidental eye contact occurs.\\nAcute toxicity estimates\\nProduct/ingredient name\\nOral (mg/\\nkg)\\nDermal \\n(mg/kg)\\nInhalation \\n(gases)\\n(ppm)\\nInhalation \\n(vapours)\\n(mg/l)\\nInhalation \\n(dusts \\nand mists)\\n(mg/l)\\nON Immersion Cooling Fluid DC 20\\nN/A\\nN/A\\nN/A\\nN/A\\n1.5\\nDec-1-ene, dimers, hydrogenated\\nN/A\\nN/A\\nN/A\\nN/A\\n1.5\\n11.1  Information on hazard classes as defined in Regulation (EC) No 1272/2008\\n11.2 Information on other hazards\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 8/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 11: Toxicological information\\n11.2.2 Other information\\nNot available.\\n11.2.1 Endocrine disrupting properties\\nNot available.\\nRemarks - Endocrine \\ndirsupting properties for \\nhuman health Summary/\\nConclusion (All ingredients)\\nNot available.\\nMobility\\nSpillages may penetrate the soil causing ground water contamination.\\n12.3 Bioaccumulative potential\\n12.1 Toxicity\\n12.2 Persistence and degradability\\nSECTION 12: Ecological information\\n12.4 Mobility in soil\\nSoil/water partition \\ncoefficient (KOC)\\nNot available.\\n12.5 Results of PBT and vPvB assessment\\nNot expected to be rapidly degradable.\\nThis product is not expected to bioaccumulate through food chains in the environment.\\nEnvironmental hazards\\nNot classified as dangerous\\nProduct does not meet the criteria for PBT or vPvB according to Regulation (EC) No. 1907/2006, Annex XIII.\\nSpills may form a film on water surfaces causing physical damage to organisms. Oxygen \\ntransfer could also be impaired.\\nOther ecological information\\nNot available.\\nRemarks - Endocrine \\ndisrupting properties for \\nenvironment Summary/\\nConclusion (All ingredients)\\nNot available.\\n12.7 Other adverse effects\\nNo known significant effects or critical hazards.\\n12.6 Endocrine disrupting \\nproperties\\nEuropean waste catalogue (EWC)\\nYes.\\nHazardous waste\\nWhere possible, arrange for product to be recycled.  Dispose of via an authorised person/\\nlicensed waste disposal contractor in accordance with local regulations.\\nMethods of disposal\\nSECTION 13: Disposal considerations\\nThe information in this section contains generic advice and guidance. The list of Identified Uses in Section 1 should be consulted \\nfor any available use-specific information provided in the Exposure Scenario(s).\\n13.1 Waste treatment methods\\nProduct\\nPackaging\\nWaste code\\nWaste designation\\nMethods of disposal\\nSpecial precautions\\n13 08 99*\\nwastes not otherwise specified\\nWhere possible, arrange for product to be recycled.  Dispose of via an authorised person/\\nlicensed waste disposal contractor in accordance with local regulations.\\nThis material and its container must be disposed of in a safe way.  Care should be taken when \\nhandling emptied containers that have not been cleaned or rinsed out.  Empty containers or \\nliners may retain some product residues.  Empty containers represent a fire hazard as they may \\ncontain flammable product residues and vapour. Never weld, solder or braze empty containers.\\nAvoid dispersal of spilt material and runoff and contact with soil, waterways, drains and sewers.\\nHowever, deviation from the intended use and/or the presence of any potential contaminants may require an alternative waste \\ndisposal code to be assigned by the end user.\\nReferences\\nCommission 2014/955/EU \\nDirective 2008/98/EC\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 9/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 13: Disposal considerations\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nNot regulated.\\nNot regulated.\\nNot regulated.\\n-\\n-\\n-\\nSECTION 14: Transport information\\nADR/RID\\nIMDG\\nIATA\\n14.2 UN proper \\nshipping name\\n14.3 Transport \\nhazard class(es)\\n14.4 Packing \\ngroup\\nADN\\nAdditional \\ninformation\\n14.5 \\nEnvironmental \\nhazards\\nNo.\\nNo.\\nNo.\\nNot available.\\n14.6 Special precautions for \\nuser\\nNot available.\\nNot regulated.\\n-\\n-\\nNo.\\n-\\n-\\n14.1  UN number \\nor ID number\\n14.7 Maritime transport in \\nbulk according to IMO \\ninstruments\\nOther regulations\\nREACH Status\\nThe company, as identified in Section 1, sells this product in the EU in compliance with the \\ncurrent requirements of REACH.\\nSECTION 15: Regulatory information\\n15.1 Safety, health and environmental regulations/legislation specific for the substance or mixture\\nEU Regulation (EC) No. 1907/2006 (REACH)\\nAnnex XIV - List of substances subject to authorisation\\nSubstances of very high concern\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nAll components are active or exempted.\\nAll components are listed or exempted.\\nAll components are listed or exempted.\\nUnited States inventory \\n(TSCA 8b)\\nAustralia inventory (AIIC)\\nCanada inventory\\nChina inventory (IECSC)\\nJapan inventory (CSCL)\\nKorea inventory (KECI)\\nPhilippines inventory \\n(PICCS)\\nTaiwan Chemical \\nSubstances Inventory \\n(TCSI)\\nAll components are listed or exempted.\\nOzone depleting substances (1005/2009/EU)\\nNot listed.\\nNone of the components are listed.\\nAnnex XIV\\nEU Regulation (EC) No. 1907/2006 (REACH)\\nAnnex XVII - Restrictions \\non the manufacture,\\nplacing on the market \\nand use of certain \\ndangerous substances,\\nmixtures and articles\\nNot applicable.\\nNone of the components are listed.\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 10/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 15: Regulatory information\\nNational regulations\\n1\\nHazard class for water\\n15.2 Chemical safety \\nassessment\\n(classified according AwSV)\\nPrior Informed Consent (PIC) (649/2012/EU)\\nNot listed.\\nSeveso Directive\\nThis product is not controlled under the Seveso Directive.\\nA Chemical Safety Assessment has been carried out for one or more of the substances within \\nthis mixture. A Chemical Safety Assessment has not been carried out for the mixture itself.\\nProhibited Chemicals \\nRegulation \\n(ChemVerbotsV)\\nOccupational restrictions\\nWhen placed on the market in Germany, this product is not subject to the Prohibited Chemicals \\nRegulation (ChemVerbotsV).\\nObserve employment restrictions in the following:\\nGesetz zum Schutz der arbeitenden Jugend (Jugendarbeitsschutzgesetz – JArbSchG)\\nGesetz zum Schutz von Müttern bei der Arbeit, in der Ausbildung und im Studium \\n(Mutterschutzgesetz – MuSchG)\\nHazardous incident ordinance\\nEU - Water framework directive - Priority substances\\nNone of the components are listed.\\nPersistent Organic Pollutants\\nNot listed.\\nSECTION 16: Other information\\nAbbreviations and acronyms\\nADN = European Provisions concerning the International Carriage of Dangerous Goods by \\nInland Waterway\\nADR = The European Agreement concerning the International Carriage of Dangerous Goods by \\nRoad\\nATE = Acute Toxicity Estimate\\nBCF = Bioconcentration Factor\\nCAS = Chemical Abstracts Service\\nCLP = Classification, Labelling and Packaging Regulation [Regulation (EC) No. 1272/2008]\\nCSA = Chemical Safety Assessment\\nCSR = Chemical Safety Report\\nDMEL = Derived Minimal Effect Level\\nDNEL = Derived No Effect Level\\nEINECS = European Inventory of Existing Commercial chemical Substances\\nES = Exposure Scenario\\nEUH statement = CLP-specific Hazard statement\\nEWC = European Waste Catalogue\\nGHS = Globally Harmonized System of Classification and Labelling of Chemicals\\nIATA = International Air Transport Association\\nIBC = Intermediate Bulk Container\\nIMDG = International Maritime Dangerous Goods\\nLogPow = logarithm of the octanol/water partition coefficient\\nMARPOL = International Convention for the Prevention of Pollution From Ships, 1973 as \\nmodified by the Protocol of 1978. (\"Marpol\" = marine pollution)\\nOECD = Organisation for Economic Co-operation and Development\\nPBT = Persistent, Bioaccumulative and Toxic\\nPNEC = Predicted No Effect Concentration\\nREACH = Registration, Evaluation, Authorisation and Restriction of Chemicals Regulation \\n[Regulation (EC) No. 1907/2006]\\nRID = The Regulations concerning the International Carriage of Dangerous Goods by Rail\\nRRN = REACH Registration Number\\nSADT = Self-Accelerating Decomposition Temperature\\nSVHC = Substances of Very High Concern\\nSTOT-RE = Specific Target Organ Toxicity - Repeated Exposure\\nSTOT-SE = Specific Target Organ Toxicity - Single Exposure\\nTWA = Time weighted average\\nUN = United Nations\\nUVCB = Complex hydrocarbon substance\\nVOC = Volatile Organic Compound\\nvPvB = Very Persistent and Very Bioaccumulative\\nVaries = may contain one or more of the following 64741-88-4 / RRN 01-2119488706-23,\\n64741-89-5 / RRN 01-2119487067-30, 64741-95-3 / RRN 01-2119487081-40, 64741-96-4/ RRN \\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 11/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nConforms to Regulation (EC) No. 1907/2006 (REACH), Annex II, as amended by Commission Regulation (EU) 2020/878\\nSECTION 16: Other information\\nDate of issue/ Date of \\nrevision\\nNotice to reader\\nDate of previous issue\\n01-2119483621-38, 64742-01-4 / RRN 01-2119488707-21, 64742-44-5 / RRN \\n01-2119985177-24, 64742-45-6, 64742-52-5 / RRN 01-2119467170-45, 64742-53-6 / RRN \\n01-2119480375-34, 64742-54-7 / RRN 01-2119484627-25, 64742-55-8 / RRN \\n01-2119487077-29, 64742-56-9 / RRN 01-2119480132-48, 64742-57-0 / RRN \\n01-2119489287-22, 64742-58-1, 64742-62-7 / RRN 01-2119480472-38, 64742-63-8,\\n64742-65-0 / RRN 01-2119471299-27, 64742-70-7 / RRN 01-2119487080-42, 72623-85-9 /\\nRRN 01-2119555262-43, 72623-86-0 / RRN 01-2119474878-16, 72623-87-1 / RRN \\n01-2119474889-13\\nProcedure used to derive the classification according to Regulation (EC) No. 1272/2008 [CLP/GHS]\\nClassification\\nJustification\\nAcute Tox. 4, H332\\nCalculation method\\nAsp. Tox. 1, H304\\nCalculation method\\nAll reasonably practicable steps have been taken to ensure this data sheet and the health, safety and environmental information \\ncontained in it is accurate as of the date specified below. No warranty or representation, express or implied is made as to the \\naccuracy or completeness of the data and information in this data sheet.\\nThe data and advice given apply when the product is sold for the stated application or applications. You should not use the \\nproduct other than for the stated application or applications without seeking advice from BP Group.\\nIt is the user’s obligation to evaluate and use this product safely and to comply with all applicable laws and regulations. The BP \\nGroup shall not be responsible for any damage or injury resulting from use, other than the stated product use of the material,\\nfrom any failure to adhere to recommendations, or from any hazards inherent in the nature of the material. Purchasers of the \\nproduct for supply to a third party for use at work, have a duty to take all necessary steps to ensure that any person handling or \\nusing the product is provided with the information in this sheet. Employers have a duty to tell employees and others who may be \\naffected of any hazards described in this sheet and of any precautions that should be taken.  You can contact the BP Group to \\nensure that this document is the most current available.  Alteration of this document is strictly prohibited.\\nHistory\\nProduct Stewardship\\nPrepared by\\nIndicates information that has changed from previously issued version.\\n14/10/2022.\\n14/10/2022.\\nFull text of abbreviated H \\nstatements\\nFull text of classifications \\n[CLP/GHS]\\nH304\\nMay be fatal if swallowed and enters airways.\\nH332\\nHarmful if inhaled.\\nAcute Tox. 4\\nACUTE TOXICITY - Category 4\\nAsp. Tox. 1\\nASPIRATION HAZARD - Category 1\\nProduct name\\nVersion 1.01\\nON Immersion Cooling Fluid DC 20\\nPage: 12/16\\n470661-DE01\\nProduct code\\nDate of issue 14 October 2022\\nFormat Germany\\nLanguage\\nENGLISH\\n(Germany)\\nDate of previous issue\\n14 October 2022.\\nGeneral use of lubricants and greases in vehicles or machinery - Industrial\\nIndustrial\\nList of use descriptors\\nIdentified use name: General use of lubricants and greases in vehicles or \\nmachinery-Industrial\\nProcess Category: PROC01, PROC02, PROC08b, PROC09\\nSector of end use: SU03\\nSubsequent service life relevant for that use: No.\\nEnvironmental Release Category: ERC04, ERC07\\nSection 1: Title\\nShort title of the exposure \\nscenario\\nCode\\n470661-DE01\\nProduct definition\\nMixture\\nProduct name\\nON Immersion Cooling Fluid DC 20\\nIdentification of the substance or mixture\\nProcesses and activities \\ncovered by the exposure \\nscenario\\nCovers general use of lubricants and greases in vehicles or machinery in closed \\nsystems. Includes filling and draining of containers and operation of enclosed \\nmachinery (including engines) and associated maintenance and storage activities.\\nATIEL-ATC SPERC 4.Biv1\\nSpecific Environmental Release Category:\\nAnnex to the extended Safety Data Sheet (eSDS)\\nSection 2.1 Control of worker exposure\\nProduct characteristics:\\nPhysical state:\\nLiquid, vapour pressure < 0.5 kPa\\nOther conditions affecting workers exposure:\\nAssumes use at not more than 20°C above ambient temperature.\\nAssumes a good basic standard of occupational hygiene is \\nimplemented\\nContributing scenarios: Operational conditions and risk management measures\\nFrequency and duration of use:\\nCovers daily exposures up to 8 hours\\nSection 2 Operational conditions and risk management measures\\nGeneral measures applicable to all activities:\\nAvoid direct skin contact with product. Identify potential areas for indirect skin contact. Wear gloves (tested to EN 374)\\nif hand contact with substance likely. Clean up contamination/spills as soon as they occur. Wash off any skin \\ncontamination immediately. Provide basic employee training to prevent/minimise exposures and to report any skin \\nproblems that may develop. Avoid direct eye contact with product also via contamination on hands.\\nGeneral exposures (closed systems):\\nNo other specific measures identified.\\nInitial factory fill of equipment Use in contained systems:\\nNo other specific measures identified.\\nInitial factory fill of equipment Open systems:\\nProvide a good standard of controlled ventilation (10 to 15 air changes per hour). Avoid carrying out operation for \\nmore than 4 hours.\\nOperation of equipment containing engine oils and similar  Use in contained systems:\\nNo other specific measures identified.\\nEquipment cleaning and maintenance:\\nDrain down system prior to equipment break-in or maintenance. Provide a good standard of general ventilation (not \\nless than 3 to 5 air changes per hour). Wear chemical-resistant gloves (tested to EN374) in combination with specific \\nactivity training. Retain drain-downs in sealed storage pending disposal or for subsequent recycle.\\nEquipment cleaning and maintenance Operation is carried out at elevated temperature (> 20°C above ambient \\ntemperature):\\nDrain down and flush system prior to equipment break-in or maintenance. Provide extract ventilation to emission \\npoints when contact with warm (>50°C) lubricant is likely. Wear chemical-resistant gloves (tested to EN374) in \\nConcentration of substance in product:\\nCovers use of substance/product up to 100 % (unless stated \\ndifferently)\\n13/16\\nGeneral use of lubricants and greases in vehicles or \\nmachinery - Industrial\\nON Immersion Cooling Fluid DC 20\\ncombination with intensive management supervision controls. Retain drain-downs in sealed storage pending disposal \\nor for subsequent recycle.\\nStorage:\\nStore substance within a closed system.\\nSection 2.2: Control of environmental exposure\\nNo exposure scenario is presented because the product is not classified for the Environment\\nSection 3: Exposure estimation and reference to its source\\nExposure assessment (environment):\\nNo exposure scenario is presented because the product is not \\nclassified for the Environment\\nExposure assessment (human):\\nThe ECETOC TRA tool has been used to estimate workplace \\nexposures unless otherwise indicated.\\nExposure estimation and reference to its source - Workers\\nExposure estimation and reference to its source - Environment\\nSection 4: Guidance to check compliance with the exposure scenario\\nEnvironment\\nGuidance is based on assumed operating conditions which may not \\nbe applicable to all sites; thus, scaling may be necessary to define \\nappropriate site-specific risk management measures. Further details \\non scaling and control technologies are provided in SPERC factsheet.\\nIf scaling reveals a condition of unsafe use (i.e., RCRs > 1),\\nadditional RMMs or a site-specific chemical safety assessment is \\nrequired. For further information see www.ATIEL.org/REACH_GES\\nHealth\\nWhere other risk management measures/operational conditions are \\nadopted, then users should ensure that risks are managed to at least \\nequivalent levels.\\n14/16\\nGeneral use of lubricants and greases in vehicles or \\nmachinery - Industrial\\nON Immersion Cooling Fluid DC 20\\nGeneral use of lubricants and greases in vehicles or machinery - Professional\\nProfessional\\nList of use descriptors\\nIdentified use name: General use of lubricants and greases in vehicles or \\nmachinery-Professional\\nProcess Category: PROC01, PROC02, PROC08a, PROC08b, PROC20\\nSector of end use: SU22\\nSubsequent service life relevant for that use: No.\\nEnvironmental Release Category: ERC09a, ERC09b\\nSection 1: Title\\nShort title of the exposure \\nscenario\\nCode\\n470661-DE01\\nProduct definition\\nMixture\\nProduct name\\nON Immersion Cooling Fluid DC 20\\nIdentification of the substance or mixture\\nProcesses and activities \\ncovered by the exposure \\nscenario\\nCovers general use of lubricants and greases in vehicles or machinery in closed \\nsystems. Includes filling and draining of containers and operation of enclosed \\nmachinery (including engines) and associated maintenance and storage activities.\\nESVOC SpERC 9.6b.v1\\nSpecific Environmental Release Category:\\nAnnex to the extended Safety Data Sheet (eSDS)\\nSection 2.1 Control of worker exposure\\nProduct characteristics:\\nPhysical state:\\nLiquid, vapour pressure < 0.5 kPa\\nOther conditions affecting workers exposure:\\nAssumes use at not more than 20°C above ambient temperature.\\nAssumes a good basic standard of occupational hygiene is \\nimplemented\\nContributing scenarios: Operational conditions and risk management measures\\nFrequency and duration of use:\\nCovers daily exposures up to 8 hours\\nSection 2 Operational conditions and risk management measures\\nGeneral measures applicable to all activities:\\nAvoid direct skin contact with product. Identify potential areas for indirect skin contact. Wear gloves (tested to EN 374)\\nif hand contact with substance likely. Clean up contamination/spills as soon as they occur. Wash off any skin \\ncontamination immediately. Provide basic employee training to prevent/minimise exposures and to report any skin \\nproblems that may develop. Use suitable eye protection. Avoid direct eye contact with product also via contamination \\non hands.\\nOperation of equipment containing engine oils and similar Use in contained systems:\\nNo other specific measures identified.\\nMaterial transfers Non-dedicated facility:\\nAvoid carrying out activities involving exposure for more than 4 hours per day. Wear chemical-resistant gloves (tested \\nto EN374) in combination with specific activity training.\\nEquipment cleaning and maintenance Dedicated facility:\\nDrain down system prior to equipment break-in or maintenance. Retain drain-downs in sealed storage pending \\ndisposal or for subsequent recycle.\\nStorage:\\nStore substance within a closed system.\\nConcentration of substance in product:\\nCovers use of substance/product up to 100 % (unless stated \\ndifferently)\\n15/16\\nGeneral use of lubricants and greases in vehicles or \\nmachinery - Professional\\nON Immersion Cooling Fluid DC 20\\nSection 2.2: Control of environmental exposure\\nNo exposure scenario is presented because the product is not classified for the Environment\\nSection 3: Exposure estimation and reference to its source\\nExposure assessment (environment):\\nNo exposure scenario is presented because the product is not \\nclassified for the Environment\\nExposure assessment (human):\\nThe ECETOC TRA tool has been used to estimate workplace \\nexposures unless otherwise indicated.\\nExposure estimation and reference to its source - Workers\\nExposure estimation and reference to its source - Environment\\nSection 4: Guidance to check compliance with the exposure scenario\\nEnvironment\\nGuidance is based on assumed operating conditions which may not \\nbe applicable to all sites; thus, scaling may be necessary to define \\nappropriate site-specific risk management measures. Further details \\non scaling and control technologies are provided in SPERC factsheet.\\nIf scaling reveals a condition of unsafe use (i.e., RCRs > 1),\\nadditional RMMs or a site-specific chemical safety assessment is \\nrequired. For further information see www.ATIEL.org/REACH_GES\\nHealth\\nWhere other risk management measures/operational conditions are \\nadopted, then users should ensure that risks are managed to at least \\nequivalent levels.\\n16/16\\nGeneral use of lubricants and greases in vehicles or \\nmachinery - Professional\\nON Immersion Cooling Fluid DC 20\\n', \" \\n \\n \\n \\nFive Reasons to Adopt Liquid \\nCooling  \\nExecutive summary \\nIT equipment chip densities has been the commonly-\\ndiscussed driver for adopting liquid cooling.  But, \\nthere are four other reasons why data center owners \\nshould consider liquid cooling including low PUE tar-\\ngets, space constraints, harsh IT environment, and \\nwater restrictions.  This paper describes these rea-\\nsons.  With this information, data center owners can \\nmake an informed decision on whether liquid cooling \\nhas advantages for their application. \\nRevision 0 \\nWhite Paper 279 \\nby Paul Lin and Tony Day \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       2 \\nFive Reasons to Adopt Liquid Cooling \\n \\n \\nCompared with conventional air cooling, liquid cooling provides benefits for data \\ncenter owners such as higher energy efficiency, smaller footprint, lower TCO, en-\\nhanced server reliability, lower noise, etc.  However, liquid cooling also has some \\ndrawbacks such as higher investment, retrofit or redesign of servers, new data cen-\\nter operation and maintenance skills required, etc.  For more information on this \\ntopic, see White Paper 265,  Liquid Cooling Technologies for Data Centers and \\nEdge Applications.  Data center owners need to determine if liquid cooling is the \\nright solution that solves the challenges they’re facing. \\n \\nIn this paper, we present five reasons to consider adopting liquid cooling: \\n \\n1. Rising chip and rack densities \\n2. Pressure to reduce energy consumption \\n3. Space constraints  \\n4. Water usage restrictions  \\n5. Harsh IT environments \\n \\nThis paper describes each reason in detail and explains how liquid cooling ad-\\ndresses it.  \\n \\n \\nChanges in IT equipment technology have always been a primary driver in the de-\\nvelopment of infrastructure cooling solutions.  Today, the demands of cloud, IoT, AI, \\nand edge applications are once again resulting in IT technology changes which im-\\npact the supporting cooling infrastructure.  We summarize some of these changes \\nbelow: \\n \\n• CPU power consumption has increased.  Processor performance continues \\nto improve as the number of cores increases along with processor power. This \\nresults in a corresponding increase in CPU heat flux and overall higher heat \\ndensity within the server enclosure itself.  Meanwhile, overclocking is also \\nused to improve compute performance in certain applications like gaming and \\nhigh-performance computing, which also leads to hotter chips (as shown in \\nFigure 1). \\n• Increasing use of high-power GPUs.  With its origins in rendering 3D gam-\\ning, the GPU is being used alongside the CPU to accelerate computational \\nworkloads in areas such as finance, analytics, AI, scientific research, and oil & \\ngas exploration.  A CPU has relatively few cores with significant cache memory \\nthat can handle a few software threads at a time.  GPUs are composed of hun-\\ndreds of cores that can handle thousands of threads simultaneously and have \\ncorrespondingly much higher power consumption (as shown in Figure 1). \\n• Lower latency requirements result in increased heat density.  As compo-\\nnent performance increases, the interconnections between these components \\nstart becoming a bottleneck in terms of latency.  To take full advantage of this \\nimproved performance, CPUs, GPUs, and other components on the board \\nsuch as the memory chipsets, are moving closer together to reduce latency.  \\nResulting in increased physical density and temperatures within the server. \\n \\nAll the above IT technology trends drive the increase in server power consumption \\nand heat density.  When a rack is heavily populated with these kinds of servers, the \\nrack power density may be too high to be air-cooled.  There are examples of ex-\\ntreme air-cooling such as floor tile fans that can push perimeter cooling systems up \\nto 40 kW rack, or active rear door heat exchangers. Row-based cooling units have \\nIntroduction \\n1. Rising chip \\nand rack  \\ndensities  \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       3 \\nFive Reasons to Adopt Liquid Cooling \\nbeen used to provide up to 67 kW for supercomputer HPC installations.  However, \\nwhen these extreme applications are used for more than just a few high-density \\nracks, the cooling system total cost of ownership (TCO) significantly increases.  \\nThese applications also cause other issues such as high airflow velocities, extreme \\nfan noise, and risk of equipment damage due to a cooling outage. \\n \\n \\nSource: Alibaba1  \\n \\nThe Green Grid (TGG) suggests a range of 15-25 kW per rack as the limit for air-\\ncooled racks “without the use of additional cooling equipment such as a rear door \\nheat exchanger”.  Therefore, The Green Grid uses 20 kW per rack in its air-cooled \\nexamples within white paper 70, Liquid Cooling Technology Update.  By contrast, \\nliquid-cooled systems will comfortably deliver twice this capacity and scale to over \\n100 kW per rack.  Liquid cooling provides more predictable thermal control without \\nthe over provisioning and airflow management needed in air cooling.  Liquid cooling \\ncan also improve chip and hard drive reliability by providing a lower stable operating \\ntemperature & removing humidity issues.     \\n \\n \\nAs more data centers are built (including cloud, regional, and edge), more energy is \\nconsumed by the data center industry, which was about 1% of total global energy \\nconsumption in 20172.  This high energy use is prompting regulations requiring data \\ncenter PUEs (power usage effectiveness) to be below a certain value to reduce en-\\nergy consumption.  For example, Shanghai requires that new data center PUEs be \\nbelow 1.3 and existing data center retrofits be below 1.4.  Data center energy ex-\\npense is becoming an increasingly larger portion of the total cost of ownership \\n(TCO).  After IT energy, cooling system energy tends to be the next biggest energy \\nconsumer.  As a result, data center owners are eager to reduce the cooling system \\nenergy consumption. \\n \\nIt’s important to note that we normally use PUE as a metric to evaluate the efficiency \\nlevel of a data center, however, for data centers with liquid cooling, we think energy \\nconsumption is a better metric.  With liquid cooling, some or all of the IT equipment \\nfans are removed which might lead to worse PUEs despite the fact that the overall \\nelectric bill decreases.  For immersive liquid cooling, the removal of server fans can \\nlower server energy consumption by approximately 4-15%, depending on the re-\\nquired airflow (8% is typically used for energy analysis).  For new or retrofit applica-\\ntions, liquid cooling can also reduce the energy consumption by replacing more en-\\nergy-intensive air-cooling systems.  A workgroup consisting of various US national \\n                                            \\n1 https://www.opencompute.org/files/Immersion-Cooling-for-Green-Computing-V1.0.pdf \\n2 https://www.iea.org/tcep/buildings/ict/  \\n2. Pressure to \\nreduce energy  \\nconsumption  \\nFigure 1 \\nThermal Design Power \\n(TDP) is trending up \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       4 \\nFive Reasons to Adopt Liquid Cooling \\nlaboratories and Intel derived a new metric to bring visibility to this issue and iTUE \\nand TUE3 were created.  As of the writing of this paper, use of these metrics is lim-\\nited, but gaining attention as liquid cooling adoption increases. \\n \\n \\nWhile air cooling systems can deal with relatively dense IT loads (e.g. deployments \\nrunning 67+ kW per rack exist), the cost and complications of doing so, increase di-\\nrectly in proportion with the increasing IT load.  Although rack count goes down as \\ndensity increases, the ratio of physical space dedicated to cooling equipment grows \\nhigher.  This is also true for overhead cooling devices that don’t occupy floor space \\n(they sit on top of the rack) because their cooling capacity is generally limited to 20 \\nkW/rack.  \\n \\nLiquid cooling provides an opportunity to reduce the overall data center space for a \\ngiven IT load.  Not only is the white space compacted, but since warm water cooling \\ncan be used, reduction of chiller systems and supporting switchgear reduces the re-\\nquired grey space.  In a 100% liquid-cooled deployment, hot & cold isle require-\\nments go away, and IT could theoretically be placed almost anywhere.  As the de-\\nmand for IT deployments in urban areas, high rise buildings, and at the edge in-\\ncrease, the need for placement in constrained locations will increase.  Liquid cooling \\ncan provide the compaction needed to enable compute in places air cooling cannot \\nsupport. \\n \\n \\nCooling towers and other evaporative cooling techniques are popular heat rejection \\nsolutions for hyperscale data centers because of their high efficiency and large cool-\\ning capacity.  However, these use the evaporation of water as a heat rejection \\nmechanism, which consumes significant amounts of water (as shown in Figure 2).  \\nWater usage increases the operation costs, but also in some regions with limited \\nwater resources, the local AHJs4 are putting more pressure on data center owners \\nto reduce water usage. The Green Grid introduced a metric called water usage ef-\\nfectiveness (WUE) to address water usage in data centers in its white paper 35,  \\nWater Usage Effectiveness (WUETM): a Green Grid Data Center Sustainability Met-\\nric. \\n \\n \\n \\n                                            \\n3 iTUE (IT-power usage effectiveness); TUE (Total-power usage effectiveness) \\n4 Authority having jurisdiction \\n3. Space \\nconstraints \\n4. Water usage \\nrestrictions  \\nFigure 2 \\nCooling towers use the \\nevaporation of water for \\nheat rejection \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       5 \\nFive Reasons to Adopt Liquid Cooling \\nThere are several ways to reduce water usage in data centers including IT load re-\\nduction, improve cooling system efficiency, optimize cooling tower operation, etc.  \\nAmong these ways, liquid cooling can reduce or eliminate water usage from a cool-\\ning system design.  Because most liquid cooling techniques can use warm water di-\\nrectly to the IT (up to 45°C/113°F), simple dry coolers can be used in most climates \\nto reject the heat. \\n \\n \\nAs more IT equipment is deployed at the edge of the network, the requirement to \\nplace it in non-ideal environments is increasing.  Examples such as IoT in manufac-\\nturing facilities and distribution centers can present challenges.  The environment is \\noften harsh both in terms of airborne contaminants and the quality of the power sup-\\nply (Figure 3 shows two examples).  Cost effective standard IT systems are often \\ndeployed in these scenarios, sometimes resulting in lower reliability than expected.  \\nAs IT becomes more integrated with production processes, any downtime affects the \\noutput of the manufacturing plant itself.  \\n \\n  \\n    \\n            Oil & gas facility                             Automotive manufacturing plant \\n \\nContainment in ruggedized enclosures with integrated air conditioning offer one so-\\nlution, but can be large, costly, and inefficient.  Liquid cooling provides and alterna-\\ntive approach in these environments.  Immersive liquid cooling isolates servers from \\nthe environment.  Removing the fans from the IT equipment provides protection \\nagainst airborne contaminants in harsh environments such as heavy industrial man-\\nufacturing plants. \\n \\n \\nWhile the five reasons above are seen to drive the adoption of liquid cooling, the fol-\\nlowing benefits of liquid cooling are worth mentioning: \\n \\n• Minimal heat added to room \\n• Elimination of fans  \\n• Waste heat recovery \\n• Less layout complexity (hot/cold aisle & containment) \\n• Less dependent on geographic climate \\n \\nMinimal heat added to room  \\nDirect to chip liquid cooling can remove 70-80% of IT heat from the IT space to the \\noutdoor environment, while immersive liquid cooling can remove over 95% of the \\nheat.  As a result, it’s easier to achieve a comfortable working environment in the IT \\nspace as minimal IT heat is added to the room.  In contrast, for data centers with tra-\\nditional air cooling, to improve energy efficiency, the IT inlet and return temperatures \\nare increased, which could result in heat stress to individuals working in the hot aisle \\nfor long periods of time.  For more information on this topic, see White Paper 123, \\n5. Harsh IT \\nenvironments \\nAdditional \\nbenefits of \\nliquid cooling \\nFigure 3 \\nTwo examples of harsh \\nIT environments \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       6 \\nFive Reasons to Adopt Liquid Cooling \\nImpact of High-Density Hot Aisles on IT Personnel Work Conditions.  Liquid cooling \\nallows for increased density while efficiently maintaining a comfortable working envi-\\nronment. \\n \\nElimination of fans \\nThe elimination of fans not only saves 4-15% of energy consumption as discussed \\nabove, but also eliminates the occupational health risks caused from fan noise.  \\nWith traditional air-cooled systems, as rack density increases, the associated in-\\ncrease in facility fan noise not only impacts operational and maintenance staff, \\nwhich may violate OSHA regulations or ISO7243 guidelines5, but also increases the \\nrisk for IT disk drive failures.  Many server designers now find themselves getting \\nvery close to the maximum occupational health limits of fan noise associated with \\nservers.  Data center operators will face the same health issue due to fan noise from \\ncooling devices. \\n \\nFor liquid cooling, the removal or reduction of server fans and removal or reduction \\nof IT space cooling (e.g. CRAH units or CRAC units), provides a very low noise sig-\\nnature.  This is of particular benefit in military and commercial office applications \\nand prevents the occupational health risks from fan noise. \\n \\nWaste heat recovery \\nHot water (over 30°C/86°F) can be used to remove the heat from the chips for di-\\nrect-to-chip liquid cooling or the heat in the dielectric loops for immersive liquid cool-\\ning.  The hot return water provides practical recovery of waste heat and energy re-\\nuse.  The recovered heat can be used for facility or district heating, which can re-\\nduce the operational expense and the facility’s overall carbon footprint, especially in \\ncold climates.  This is very difficult to achieve with air cooling and is an attribute of \\nliquid cooling that municipalities hope to leverage. \\n \\nLess layout complexity (hot/cold aisle & containment) \\nFor data centers with air cooling, the best practice for airflow management is to have \\nthe racks in a cold/hot aisle layout.  Containment systems are then used to contain \\nthe cold or hot aisle, to separate the cold and hot airflow, in order to save cooling \\nenergy and eliminate hot spots.  For data centers with immersive liquid cooling, the \\nserver and CRAH/CRAC fans are eliminated, therefore there is no concern for air-\\nflow management, and no need for cold/hot aisle layout and containment.  The rack \\nlayout is simplified and is dependent only on the piping paths and maintainability, al-\\nlowing for configurations currently not imagined. \\n \\nLess dependent on geographic climate \\nAir-cooled economizer solutions used to cool IT equipment are dependent on a spe-\\ncific geographic location for their effective deployment.  Because liquid cooling uses \\nwarm water – up to 45°C/113°F, full economization can be achieved in most parts of \\nthe world.  This provides advantages in standardization and high efficiency in both \\nregional data centers and edge deployments. \\n \\n \\n                                            \\n5OSHA (Occupational Safety & Health Administration) Technical Manual section III, Chapter 4 ISO \\n(International Organization for Standardization) 7243, “Hot environments – Estimation of the heat stress \\non working man based on WBGT index” \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       7 \\nFive Reasons to Adopt Liquid Cooling \\n \\n \\nLiquid cooling is most often viewed as a technical solution for high performance \\ncomputing and high-density applications.  But the advantages liquid cooling brings \\nto both large data centers and edge deployments may help overcome many of the \\nchallenges data center designers and operators face today.  Reducing energy con-\\nsumption in all geographies, reducing water usage, deployment is space constrained \\nand harsh environments are reasons we expect to see liquid cooling adopted more \\nbroadly in the IT industry in coming years. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbout the authors \\nPaul Lin is a Research Director at Schneider Electric's Data Center Science Center.  He is respon-\\nsible for data center design and operation research and consults with clients on risk assessment \\nand design practices to optimize the availability and efficiency of their data center environment.  \\nBefore joining Schneider Electric, Paul worked as the R&D Project Leader in LG Electronics for \\nseveral years.  He is now designated as a “Data Center Certified Associate”, an internationally rec-\\nognized validation of the knowledge and skills required for a data center professional.  He is also a \\nregistered HVAC professional engineer.  Paul holds a master’s degree in mechanical engineering \\nfrom Jilin University with a background in HVAC and Thermodynamic Engineering. \\nTony Day is a Director of Business Development within the IT Division’s Office of the CTO at \\nSchneider Electric. He is responsible for working with clients to develop integrated data centre de-\\nsign solutions.  He is a Chartered Architect and mechanical engineer with experience in both indus-\\ntrial engineering and construction industries.  Prior to joining Schneider Electric, he worked both in \\nprofessional private practice and within commercial organisations with a focus on construction of \\nhighly serviced environments including manufacturing, telecommunications, computer rooms, finan-\\ncial trading floors and data centers. \\n \\nConclusion \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric  –  Data Center Science Center      White Paper 279   Rev 0       8 \\nFive Reasons to Adopt Liquid Cooling \\n \\n \\nLiquid Cooling Technologies for Data Centers and Edge Applications \\nWhite Paper 265 \\n \\nImpact of High-Density Hot Aisles on IT Personnel Work Conditions Conditions   \\nWhite Paper 123 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nContact us \\nFor feedback and comments about the content of this white paper: \\nData Center Science Center \\ndcsc@schneider-electric.com \\nIf you are a customer and have questions specific to your data center project: \\nContact your Schneider Electric representative at \\nwww.apc.com/support/contact/index.cfm \\nBrowse all  \\nwhite papers  \\nwhitepapers.apc.com  \\ntools.apc.com  \\nBrowse all  \\nTradeOff Tools™ \\nResources \\n© 2019 Schneider Electric. All rights reserved. \\n\", 'Case Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 1\\nSubmer’s SmartPodXL+ with OCP and 19” servers\\nTelefónica proves that the SmartPod XL+ is \\nefficient and sustainable while working under the \\nworst climate conditions. \\nMadrid, Spain\\nTelecommunications \\nType of Solution\\nHighlight\\nAvailability\\nIndustry\\nIntroduction\\nOne of the biggest telco operators and mobile network providers in the world, Telefónica globally \\nprovides everything from fixed and mobile telephony, to broadband, to subscription television. \\nTelefónica decided to run a pilot test with Submer’s SmartPod XL+, which took place over the course \\nof a month in Bellas Vistas, Madrid, presenting the added circumstances of a record-breaking hot \\nsummer. This project found this immersion cooling solution to have a maintained conventional level of \\nreliability, high efficiency, and improved hardware (HW) performance.\\nTelefónica Puts Submer’s Core Product \\nThrough the Toughest of Testing\\nThe SmartPod\\nThe Submer SmartPod product line was the very \\nfirst OCP-compatible1 immersion cooling system \\nto hit the market. Available in both sizes (800 and \\n920mm depth) for the entire family X (21U/19OU \\nand <50kW), XL (44U/42OU and <50kW), and XL+ \\n(42U/39OU and <100kW / 50kW Redundant), the \\nSmartPod is a compact, modular LIC2 solution \\nwith up to 100kW heat dissipation capacity and \\nan mPUE as low as 1.03. Compatible with most \\nof the market’s IT HW and as a perfect fit for \\nHigh-Density (HPC) solutions3 and Edge4, the \\nSmartPod accommodates 21” and/or 19” HW. For \\nmore information on the pod, visit our  \\nproduct page.\\nSubmer’s SmartPod XL+\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 2\\nThe use of the SmartPod’s high heat dissipation capability entails a method of cooling IT equipment of \\ngreater energy efficiency than traditional methods such as chillers, air conditioners, and refrigerant \\ngases. The PoC was based on the Submer’s generated heat transfer from the IT equipment using heat \\nexchangers with water recirculation, taking the heat outside to an adiabatic dry-cooler5 installed on \\nthe roof.  \\nAdiabatic Dry Cooler and Deck Water Pumps\\nReliability \\nTelefónica carried out thermal capacity tests to check that the Submer cooling system would \\nmaintain the temperature of the coolant in the SmartPod below the setpoint limit of 55º C and under \\nmaximum load conditions of the equipment. \\nThe conditions of the tests were as follows: \\n•\\t\\nA maximum thermal load of 50 kW.  \\n•\\t\\nSince the power of the IT equipment available did not equal 50 kW, 5 electric heating elements of \\n9 kW, each with self-regulation, were introduced into the SmartPod.\\n•\\t\\nThe set point of the coolant fluid was set at a maximum of 55ºC. \\n•\\t\\nThe inlet water temperature was set to be at a maximum between 34-37ºC. \\n•\\t\\nThe electrical consumption and temperatures reached in fluids were registered by the network \\nanalyzers and the SmartPod CDU6 sensors.\\n•\\t\\nThe redundancy testings of the two CDUs, recorded the following measurements: \\nMode 1: With the 2 CDUs in operation. \\nMode 2: 1 CDU is turned off and cooled only with the other CDU. \\nMode 3: The turned off CDU is turned on and the one switched on is turned off. \\nMode 4: Turn on the two CDUs again and finish the test.\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 3\\n13:00\\n14:00\\n15:00\\n16:00\\n17:00\\n0,00\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n200,00\\n400,00\\n600,00\\n800,00\\n1000,00\\n1200,00\\n1400,00\\n1600,00\\n1800,00\\n2000,00\\nCoolant temperature in (surface)\\nCoolant temperature out (bottom)\\nWater temperature in\\nConsumption CDU 2\\nConsumption CDUs (W)\\nConsumption CDU 1\\nWater temperature out\\nTemperature (°C)\\nCDU Redundancy Test\\nDATE: 27 / 10 / 22\\nThe tests proved that Submer’s immersion cooling technology kept the temperature of the coolant \\nstable, even in extreme operation, below the maximum target temperature of 55º C, and with the \\nmaximum possible load of equipment inside, dissipating 50 thermal kW.\\nTelefónica found that the technical design of the SmartPod’s associated equipment, such as the air \\ncooler, water pumps, pipes, etc, favorably meets the needs of the system, but that in extreme weather \\nconditions (such as during a Spanish summer), water and coolant must be used at 55ºC with the use of \\nadiabatic cooling as support.\\nFinally, the system offers 100% redundancy thanks to the two CDUs and can work with the CDUs \\ntogether or either one on its own. Both were deemed capable of coping independently with the 50 kW \\nthermal load of the SmartPod.\\nThe Submer SmartPod \\nIT equipment and resistors \\nThe Submer SmartPod \\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 4\\nOf course, efficiency was an aspect at the very top of Telefónica’s testing list; specifically, to obtain \\nthe efficiency indexes of Submer’s immersion cooling solution. Since there are no differences in \\nterms of power elements, such as the UPS7, transformers, etc, between immersion cooling and \\ntraditional solutions, the focus lay on the cooling part.  \\nAs the SmartPod temperature cooling can usually be achieved without chillers, the temperatures of \\nthe IT load were achieved using dry coolers with adiabatic systems, which use a significant amount \\nof water. For that, the team took this into account in their comparison, using the relationship between \\nthe cost of energy and water as 1,5 €/m3 vs 0,20 €/kW/h.  \\nTo take the measurements, the team set the coolant to its maximum limit of 55ºC and the activation \\ntemperature of the water vaporization in the dry cooler at 36ºC inlet temperature. This meant that \\nif the water in the circuit entered at more than 36ºC, the dry cooler would have sprayed water on its \\npanels. \\nThe test commenced with 5 resistors on plus, the load of traditional servers, then two were turned off \\n2 days later, then the other two, then the remaining. The weather conditions were unfavorable as you \\ncan see depicted on the graph below. \\nEfficiency \\nTest 1: Clamp Meter \\nDATE: 20 to 28/10/22\\nPUE with water\\nIT power consumption (kW)\\nPUE\\nIT Load (kW)\\nPUE\\n20/07\\n21/07\\n22/07\\n23/07\\n24/07\\n25/07\\n26/07\\n27/07\\n28/07\\n0\\n2.000\\n1.800\\n1.600\\n1.400\\n1.200\\n1.000\\n0.800\\n0.600\\n0.400\\n0.200\\n0.000\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n1.212\\n1.089\\n1.140\\n1.270\\n1.340\\n1.709\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 5\\nAverage Water Consumption Test\\nDATE: 20 to 28/10/22\\nExterior temperature (°C)\\nCoolant temperature (°C)\\nIT power consumption (kW)\\nAverage water consumption (m3/h)\\nAverage Water Consumption (m3/h)\\nTemperature(ºC), IT power consumption (kW)\\n20/07\\n21/07\\n22/07\\n23/07\\n24/07\\n25/07\\n26/07\\n27/07\\n28/07\\n0.52\\n60\\n50\\n40\\n30\\n20\\n10\\n0.000\\n0.54\\n0.56\\n0.58\\n0.6\\n0.62\\n0.64\\n0.66\\nHere, the analyzers were placed as in the above test but the load evolution plan to take the \\nmeasurements was as follows: \\n•\\t\\nThe coolant set point was varied with values of 45ºC, 50ºC, and 55ºC. \\n•\\t\\nThe load of the SmartPod was varied with the help of resistors: low load (~9kW), medium load \\n(~22kW), and high load (~50kW). \\n•\\t\\nThe activation temperature of the water vaporization in the dry cooler was set at 36ºC inlet \\ntemperature. \\n•\\t\\nThe amount of water spent by the dry cooler was monitored - the meter had no time \\ndiscrimination so the consumption of each test was prorated by its same duration. \\nTest 2: Wibeees\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 6\\nPUE with water\\nIT power consumption (kW)\\nPUE\\nPUE Evolution in Function of IT Load\\n0\\n0.00\\n0.50\\n1.00\\n1.50\\n2.00\\n2.50\\n4.00\\n3.50\\n3.00\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\nIT Load (kW)\\nPUE\\n13:00\\n10/08\\n12/08\\n13/08\\n15/08\\n16/08\\n18/08 20/08 21/08 23/08 24/08 26/08 27/08 29/08 31/08\\n01/08 03/08 04/08\\n03:00\\n17:00\\n07:00\\n21:00\\n11:00\\n01:00\\n15:00 05:00 19:00 09:00 23:00\\n13:00 03:00\\n17:00\\n07:00\\n21:00\\nSETPOINT = 45°C\\nSETPOINT = 50°C\\nSETPOINT = 55°C\\n1.55\\n1.15\\n1.08\\n1.04\\n1.04\\n1.04\\n1.08\\n1.08\\n1.15\\n1.15\\n1.23\\n1.13\\n1.13\\n1.24\\n1.45\\n1.40\\n1.22\\n1.11\\nExterior temperature (°C)\\nIT power consumption (kW)\\nCoolant temperature\\nAverage water consumption (m3/h)\\n0\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nAverage Water Consumption (m3/h)\\nIT Power Consumption (kW)\\n13:00\\n10/08\\n12/08\\n14/08\\n15/08\\n17/08\\n29/08\\n21/08\\n22/08 24/08\\n26/08\\n28/08\\n29/08\\n31/08\\n02/08\\n04/08\\n07:00\\n01:00\\n19:00\\n13:00\\n07:00\\n01:00\\n19:00\\n13:00\\n07:00\\n01:00\\n19:00\\n13:00\\n07:00\\n01:00\\nSETPOINT = 45°C\\nSETPOINT = 50°C\\nSETPOINT = 55°C\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 7\\nOverall, taking into account the two aforementioned tests, the water meter, CDU sensors, dry cooler, \\nand weather checks, the results reflect that:  \\n•\\t\\nthe higher the IT load, the better PUE \\n•\\t\\nthe water consumption of the dry cooler is influenced more by the outside temperature than  \\nby the IT load \\n•\\t\\nwith the 50 kW dry cooler and 50 kW load, the coolant temperature cannot be kept below than \\n48ºC in summer conditions \\n•\\t\\nwith the contribution of water, the consumption of the non-IT part is only slightly dependent on \\nthe outside temperature; however, without water input, non-IT consumption is highly dependent \\non temperature changes \\n•\\t\\nthe coolant temperature remains very stable (between 45-55ºC) for each test, which enables \\nbetter reutilization; the greater the load, the greater the stability \\nTest Number\\n1\\n2\\n3\\n4\\n5\\n6\\nStart\\n15-8-22 22:00\\xa0\\n17-8-22 15:00\\xa0\\n2\\n1,08\\xa0\\n1,23\\xa0\\n22,25\\xa0\\xa0\\n19,95\\n27ºC\\n31ºC\\n57%\\n45\\n17-8-22 15:00\\xa0\\n19-8-22 15:00\\xa0\\n2\\n1,04\\n1,13\\n44,74\\n26,06\\xa0\\n25ºC\\n29ºC\\n54%\\n45\\n19-8-22 15:00\\xa0\\n22-8-22 11:00\\xa0\\n3\\n1,04\\n1,13\\n44,70\\n36,63\\xa0\\n29ºC\\n35ºC\\n52%\\n50\\xa0\\n22-8-22 11:00\\xa0\\n24-8-22 12:00\\xa0\\n2\\n1,08\\xa0\\n1,25\\xa0\\n21,52\\n25,2\\xa0\\n31ºC\\n36ºC\\n51%\\n50\\xa0\\n29-8-22 15:00\\xa0\\n31-8-22 8:00\\xa0\\n2\\n1,08\\xa0\\n1,22\\n21,58\\n18\\n27ºC\\n32ºC\\n66%\\n55\\xa0\\n31-8-22 8:00\\xa0\\n5-9-22 11:00\\xa0\\n5\\n1,04\\xa0\\n1,11\\n44,40\\n52,15\\n28ºC\\n33ºC\\n56%\\n55\\xa0\\nEnd\\nDuration (days)\\nSet Point (ºC)\\xa0\\nPUE Average\\nWUE Average\\nAverage kW Dissipation\\nWater Consumption (m3)\\nRoom Temperature\\nExternal Temperature\\nHumidity\\nEfficiency Testing Summary\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 8\\nHardware Performance\\nWith HW performance testing came multiple \\nobjectives. The main objective was to determine \\nwhether the performance of the equipment \\nwas affected to any extent by varying the \\ntemperature of the coolant. The Telefónica \\nteam also aimed to monitor the temperature \\nreached inside the equipment (mainly the CPU) \\nto determine how they behave when reaching \\ntemperatures higher than usual in air cooling \\nenvironments. Then, to compare the results with \\nthe usual operating values in air cooling. Lastly, \\nto detect any potential drawbacks of this type of \\nsolution, taking into account how the operation \\nand maintenance by Telefónica’s IT technical \\nexperts at their datacenter.  \\nTo do so, the following equipment was required:  \\n•\\t\\nAn HP DL380 Gen10 piece of IT HW \\n•\\t\\nTwo Fujitsu pieces of IT HW, in a chassis with \\nthe capacity for four blades \\n•\\t\\nTwo items of IT HW from the company 2CRSI, \\none standard and one based on Open Compute \\nstandards \\n•\\t\\nA chassis from Hypertec with the capacity for \\n6 blades  \\n•\\t\\nA Netapp storage cabinet, model  \\nAFF-250 adapted in collaboration with the \\nmanufacturer, eliminating fans and modifying \\nthe BIOS8 to avoid auto-shutdown when \\ndetecting the absence of this cooling element \\nAnd so, to obtain an accurate reading of the HW performance, Telefónica:  \\nStressed servers with coolant temperatures between 45ºC and 55ºC \\n•\\t\\nMonitored the temperature of the CPUs of the equipment \\n•\\t\\nMeasured the performance of equipment in different load profiles, between 45ºC and 55ºC \\n•\\t\\nCompared the data with that of equipment under air cooling \\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 9\\nThe servers used were standard servers (HP, Fujitsu) and servers based on Open Compute designs, \\nboth certified and adapted to immersion cooling. The monitoring was conducted with the servers at \\nrest, at maximum performance, and performance at 80% of their capacity i.e. the expected value of \\nusual operation. Above this value is where the first warnings of high CPU consumption are generated.\\nIn this aspect, Telefónica concluded that the processing capacity does not vary in immersion cooling \\nand there were savings of 5% in IT compared to traditional solutions. \\nIt is worth mentioning that some customized solutions can reach considerably higher temperatures \\nso the lifetime of the equipment must be guaranteed by the manufacturer.  \\nTelefónica is of the opinion that, in order to optimize the efficiency of this solution, it should be used \\nonly for processing and in high density and that the access and operation of the IT layer are possible \\nfor specific HW solutions and not so for traditional HW.\\nSETPOINT = 55 °C\\n20\\n40\\n60\\n80\\n100\\n120\\nWITHOUT LOAD\\n100%\\nFujistu 1 & 2\\n2 CRSI 19\\n2 CRSI 21\\nAir cooling\\n57\\n103\\n46\\n80\\n55\\n74\\n40\\n69\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 10\\nIn summation, immersion cooling has proved itself to be a sustainable and innovative solution that \\neliminates the need for cooling gases (maintaining traditional reliability values (Tier III9)) and can \\nreduce non-IT consumption by 75%. \\nThe energy savings presented by the SmartPod XL+ could mean a return on investment in less than 5 \\nyears. While looking at just the price of a unique server may seem expensive, this is reimbursed in the \\nlong run.\\nThe solution is only viable with specific HW (very good for the OCP) however the current costs imply a \\ndealbreaker for these solutions. The increase in cost must be limited in 10% for ROI to become inferior \\nto the life span of the HW (5 years). \\nAs for the facilities, the increase in Capex is moderate, and tends to be optimized as the cost of the \\nSmartPod is covered. \\nTelefónica finds the SmartPod to be ideal for Edge solutions, as it is compact and, thanks to the \\nimprovement of the PUE, allows for the optimization of the use of the electrical connections for IT. \\nProjects such as these, which demand these high densities, could be the trigger that this technology \\nneeds to open up opportunities within Telefónica for High Performance Computing (HPC).  \\nWhat’s more, the findings show a lot of promise for the possibility of heat recovery for other uses. \\n“Sustainability is a basic pillar to guarantee the viability of any business or \\nproject. Without innovation, we would not be able to guarantee sustainability. \\nImmersion cooling technology shows us that we are on the right track.” \\nPablo Casado, Head of Global Data Center & Core Sites at Telefónica\\nConclusion\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 11\\nTelefónica deems it important that procurement is initiated to fine-tune the cost of the IT HW and \\nensure that the lifetime risk is passed on to the manufacturer. \\nIn its pipeline, Telefónica is working towards a European EDGE project, its Nabiax Alcalá Phase 1 \\nground floor expansion area, and expanding in Hispanic America with some HW upgrades.  \\nFounded in 2015, Submer provides best-in-class technology, enabling data centers around the world \\nto leverage the\\u202fpower of immersion cooling for HPC, hyperscale, data centers, Edge, AI, DL and \\nblockchain\\u202fapplications. Submer is headquartered in Barcelona and with offices in Virginia and Palo \\nAlto, California. For more information, visit\\u202fsubmer.com.\\nTelefónica is one the largest telecommunications service providers in the world. The company offers \\nfixed and mobile connectivity as well as a wide range of digital services for residential and business \\ncustomers. With more than 383 million customers, Telefónica operates in Europe and Latin America. \\nTelefónica is a 100% listed company and its shares are traded on the Spanish Stock Market and on \\nthose in New York and Lima. \\nNext Steps\\nAbout Submer\\nAbout Telefónica\\nCase Study\\nImmersion Cooling  |  Telecommunications\\nPAGE: 12\\nGlossary\\nTerm\\nDefinition\\n1  OCP-compatible\\nThe Open Compute Project is an organization that shares designs \\nof datacenter products and best practices among companies. It’s a \\ncommunity in which collaborators contribute designs for hardware \\ntechnology so that the industry can keep up with compute infrastructure \\ngrowth. Nowadays, it’s also very focused on immersion cooling.  \\n2  LIC\\nLiquid Immersion Cooling i.e. the method of submerging computer \\ncomponents or full servers in a thermally conductive, dielectric liquid.  \\n3  High-Density (HPC) \\nsolutions\\nHigh performance computing solutions are specially designed to efficiently \\nhandle high densities.  \\n4  Edge\\nEdge computing happens in small facilities close to the application itself.   \\n5  Adiabatic dry cooler\\nA dry cooler is a heat-transfer device that uses air to remove excess heat \\ninto the atmosphere, typically with a heat exchanger and fans. Adiabatic \\nfunctions may use water to cool down on extreme temperatures. \\n6  CDU\\nThe cooling distribution unit is the “brain” of Submer’s immersion cooling \\nsolutions. Within it, you can find a pump system, heat exchanger, and \\nProgrammable Logic Controller (PLC), amongst other electronics.  \\n7  UPS\\nUninterruptible Power Supply.  \\n8  BIOS\\nThe Basic Input/Output System which performs hardware initialization.   \\n9  Tier III\\nDatacenters are typically rated from between 1 and 4 (4 being the best), \\nbased on the reliability and performance of their electrical systems and \\ninfrastructure.\\nFor more industry jargon explained,\\nvisit our library\\n', \"Immersion Cooling\\nSolution\\nIn partnership with:\\nIMMERSION BORN DESIGN\\nADVANCED RETROFIT\\nRETROFIT\\nFEATURES\\nHypertec only\\nSome Vendors + Hypertec\\nMultiple Vendors\\nAvailability\\nImmersion-Born\\nCOTS Air-Cooled Server\\nOTS Air-Cooled Server\\nBase\\nVanity-Free Design\\nHigh\\nMedium\\nLow\\nEase of Installation and Serviceability\\nHypertec only\\nPerformance Availability (OC or Turbo Lock)\\nSupports Both AC & OCP Power\\nSustainable Composite Material\\nCustom Immersion Cooling Heatsink\\nIntel Xeon and AMD EPYC 3rd Gen Compatibility\\nIntel Xeon W/SP and AMD EPYC 4/5th Gen Ready\\n252\\n168\\n168\\nCPU Density Per SmartPod (42RU/48RU)\\nFull Warranty\\nUp to 5 years NBD 24x7\\nQuestionable\\nQuestionable\\nWarranty\\nImmersion Cooling Product Comparison\\nOEM Immersion Retrofit\\n2U 4 Nodes Example\\nObstructed flow by \\nchassis housings\\nSSD drives and the \\nbackplane at the \\nbottom blocks the \\nflow.\\nNative Immersion Cooling\\n1U 2 Nodes Example\\nFluid moving freely\\nfor better efficiency\\nPower supply\\nat the bottom\\nOur 360° Made-for-Immersion Solution\\n\\uf0fcSite Assessment\\n\\uf0fcPower budget\\n\\uf0fcPlanning\\n\\uf0fcTank and pod layout\\n\\uf0fcFactory assembly\\n\\uf0fcBIOS & Firmware settings\\n\\uf0fcSoftware & OS installation\\n\\uf0fcCustomer image\\n\\uf0fcOnsite installation\\n\\uf0fcCabling & labeling\\n\\uf0fcNetworking & power\\n\\uf0fcWarranty\\nYou can now enjoy all the benefits of our 360° immersion \\ncooling solution without the headache, while saving \\nmoney on OPEX and CAPEX.\\nCIARA TRIDENT Product Portfolio\\nImmersion-Born\\nPower Supply\\nStorage / Node\\nPCIe Slots / Node\\nMemory / Node\\nProcessors / Node\\nForm Factor\\nProduct\\nUp to 82/88/96 \\nNodes in 41/44/48U\\nSmart Pod 920(c)\\n2 x M.2 SSD up \\nto 2280 (PCIe/SATA)\\n2 x Optional Fixed \\n2.5” SATA HDD\\n2 x PCIe 5.0 x16 LPHL\\n16 x DIMM slots DDR5 \\nRDIMM 4800 MHz\\n5th gen CPU: up to \\n5600MHz\\nDual 4th Gen Intel® or \\n5th Gen Intel® Xeon®\\nScalable\\nTDP up to 350W\\n1U-2 x compute \\nnodes in a 19'' \\nchassis\\nUp to 123/132/144 \\nNodes in 41/44/48USm\\nart Pod 920 with 270W\\nCPU. Up to 108 Nodes \\nin 41 or 48U\\nSmart Pod 920 with\\n350W CPU\\n2 x M.2 SSD up \\nto 2280 (PCIe/SATA)\\n2 x Optional Fixed \\n2.5” SATA HDD\\n2 x PCIe 5.0 x16 LPHL\\n16 x DIMM slots DDR5 \\nRDIMM 4800 MHz\\n5th gen CPU: up \\nto 5600MHz\\nDual 4th Gen Intel® or \\n5th Gen Intel® Xeon®\\nScalable\\nTDP up to 350W\\n1U-3 x compute \\nnodes in a 21'' \\nchassis\\nUp to 82/88/96 \\nNodes in 41/44/48U\\nSmart Pod 920(c)\\n2 x M.2 SSD \\nup to 2280 (PCIe/SATA)\\n2 x PCIe 5.0 x16 LPHL\\n24 x DIMM slots DDR5 \\nRDIMM 4800 MHz\\nDual AMD EPYC\\n9004 Series Processors\\nTDP up to 400W\\n1U-2x compute \\nnodes in a 19'' \\nchassis*\\n*OCP OrV3 available\\n*21'' chassis available\\nTRIDENT iC610TR-G6*\\nTRIDENT iC615DR-G6\\nTRIDENT iC610DR-G6\\nCIARA TRIDENT Product Portfolio\\nImmersion-Born\\nPower Supply\\nStorage / Node\\nPCIe Slots / Node\\nMemory / Node\\nProcessors / Node\\nForm Factor\\nProduct\\n1+1 redundant 1600W \\n80+ Titanium PSU\\n16 x E1.S NVMe Drive Bays\\n(Up to 128TB with 8TB)\\nOr\\n16 x E1.L NVMe Drive Bays\\n(up to 1PB with 64TB)\\n2 x M.2 SSD up to 2280 \\n(PCIe/Sata)\\n1 x PCIe 5.0 x16 LPHL\\n1 x PCIe 5.0 x16 FHFL\\n1 x OCP 3.0 PCIe 5.0 x 16\\n8 x DIMM slots DDR5 \\nRDIMM 4800 MHz\\nSingle AMD EPYC\\n9004 Series\\nTDP up to 400W\\n1U, 19'' chassis\\n1+1 redundant 80+\\nTitanium PSU\\n(wattage specs TBC)\\n2 x M.2 up to 2280\\n3 x PCIe 5.0 FHFLDW\\n2 x PCIe 5.0 LPHL\\n1 x OCP 3.0\\nSupports 3 x GPU\\n32 x DIMM slots \\nDDR5 RDIMM 4800 \\nMHz\\n5th gen CPU: \\nup to 5600 MHz\\nDual 4th gen \\nIntel® or 5th Gen\\nIntel® Xeon® Scalable\\nTDP up to 350W\\n1U, 19'' chassis\\n1+1 redundant 80+\\nTitanium PSU\\n(wattage specs TBC)\\n2 x M.2 up to 2280\\n4 x E1.L or E1.S (TBC)\\n4 x PCIe 5.0 FHFLDW\\n2 x PCIe 5.0 LPHL\\n1 x OCP 3.0\\nSupports 4 x GPU\\n32 x DIMM slots \\nDDR5 RDIMM 4800 \\nMHz\\n5th gen \\nCPU: up to 5600 MHz\\nDual 4th gen \\nIntel® or 5th Gen\\nIntel® Xeon® Scalable\\nTDP up to 350W\\n1U, 21'' chassis\\nTRIDENT iG610R-G6\\nTRIDENT iGW610R-G6\\nTRIDENT iS515R-G6\\nCiara TRIDENT iC610TR-G6\\nImmersion-born design | Ultra high density\\nDENSITY\\nIndustry best density \\nwith up to up 84/96 Nodes \\nin 42/48U SmartPod \\nTCO\\nSustainable hardware design \\nminimizes bill of materials \\ncosts, extends useful life and \\nimproves serviceability\\nFLEXIBILITY\\nPlatform is designed to \\naccommodate multiple form \\nfactors, chipsets, storage\\nconfigs and power configs\\niS515R-G6 (prototype)\\nRedundant 1200 Watt or \\n1600 Watt CRPS\\nE1.X backplane\\n16x Front (top surface) E1.S \\nor E1.L drives\\n16x dual height slot \\nsupport for a FHFL PCIe \\nCard or GPU\\niS515R-G6\\nImmersion Server\\niS515R-G6\\nPCIe x16 Slot for NICs, \\nGRAID and other cards\\nOCP 3.0 Slot for NICs\\n16x Front (top surface) E1.S \\nor E1.L drives\\niS515R-G6\\nImmersion Server\\nTRIDENT iCO Server:\\nMulti-node architecture ideal for \\nCDN, HCI, Cloud workload. Designed \\nto drive digital transformations and \\nIT agility\\nFor cloud service providers, \\nFintech, Hyperscaler, Virtualization\\nTarget workloads\\nTRIDENT iCO610TR-G6 Server\\n• 1OU 3 Power Rendered nodes\\nEach Node Specifications:\\n• Dual 4th Generation Intel® Xeon® Scalable processors\\n• 16 x slots DDR5 4800MHz RDIMM\\n• 2 x M.2 + optional 2 x 2.5'' SSD\\n• 2 x PCIe 5.0 x16 HHHL slots\\n• OCP Design\\nTRIDENT iCO SERIES\\nSuper High Density \\nmulti node design\\nOpen Compute Project Design\\nOptimal liquid flow\\nopen chassis concept for \\nimmersion submerge POD\\nCompliant with Open Compute and \\nPower infrastructure\\nOpen Rack v3 (ORv3) Platform compatible\\n21'' server width\\nTRIDENT OCP TRIDENT\\nImmersion Born Compute OCP design server\\nPS0\\nTRIDENT iCO SERIES\\n1OU-3 OCP Powered Render Nodes\\nForm Factor\\nDual 4th Gen Intel® Xeon® Scalable Processors TDP up to 350W\\nProcessor/Render Node\\nImmersion\\nCooling System / Node\\nUp to 4TB DDR5 4800 MHz memory\\n8 x memory channel architecture\\n16x memory slots\\nMemory / Node\\nIntel X710 10 GbE LAN Port\\nNetwork Controller / Node\\n2 x M.2 SSD up to 22110 (SATA or Gen4 x2 link)\\n2 x 2.5'' NVMe front bays (optional)\\nStorage / Node\\n2 x PCIe 5.0 x 16 HHHL\\nExpansion Slots / Node\\nDedicated Management 1 GbE 1000 BASE-T\\nManagement / Node\\nOCP OrV3 Compatible\\nPower Supply / Node\\n800mm x 537mm x 48.06mm\\nDimensions (DxWxH)\\nDatasheet\\nTECH SPECS\\nCIARA TRIDENT iCO610TR-G6\\nCY Q4-24\\nCY Q3-24\\nCY Q2-24\\nCY Q1-24\\nPOC PERIOD\\nCIARA TRIDENT Series Product Roadmap\\nServer Availability\\niC610DR-G6\\n(1U-2 NODES) Dual 4th or 5th Gen Intel® Xeon® Scalable Processors\\niC615DR-G6\\n(1U-2 NODES (19’’)) Dual AMD EPYC 9004\\niC610WTR-G6\\n(1U-3 NODES) Dual 4th or 5th Gen Intel® Xeon® Scalable Processors\\niCW615DR-G6\\n(1U-2 NODES (21’’)) Dual AMD EPYC 9004\\niCO610TR-G6\\n(1U-3 NODES (OCP OrV3 ) Dual 4th or 5th Gen Intel® Xeon® Scalable Processors\\niS515R-G6\\n(1U Storage) Single AMD EPYC 9004, 16 x E1.S or 16 x E1.L and 2 x M.2\\niG610R-G6\\n(1U, 3 x GPU (19’’)) Dual 4th or 5th Gen Intel® Xeon® Scalable Processors\\n\", ' \\n \\n \\n \\nCapital Cost Analysis of  \\nImmersive Liquid-Cooled vs.  \\nAir-Cooled Large Data Centers \\nExecutive summary \\nThere are several known benefits of choosing liquid cooling \\nover traditional air cooling including energy savings.  Capi-\\ntal cost, however, is viewed as a common obstacle.  In this \\npaper, we first demonstrate that at a like-for-like rack den-\\nsity of 10 kW in a 2 MW data center, the data center capex \\nis roughly equal for both a traditional air-cooled data center \\nand a chassis-based immersive liquid cooled data center.  \\nBecause high density compaction is a key benefit of liquid \\ncooling, we also quantify the capex difference when liquid \\ncooling is deployed at 20 kW/rack and 40 kW/rack for the \\nsame capacity data center.  The result is 10% and 14% \\ncapex savings, respectively. \\n \\nVersion 1 \\nWhite Paper 282 \\nby  \\nRobert Bunger  \\nWendy Torell   \\nVictor Avelar \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     2 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\n \\nLiquid-cooled  IT equipment is not new.  It’s been around for decades. We tend to \\nhear about it for niche solutions – such as for high performance computing (HPC) \\nand gaming.  But today, there are some key trends and drivers that make it an ap-\\npealing solution for the more mainstream IT audience.  White Paper 279, Five Rea-\\nsons to Adopt Liquid Cooling explains these reasons.   \\n \\nThere are some clear benefits of liquid cooling over traditional air cooling.  These in-\\nclude: \\n \\n• Reduced need for water – Local municipalities are putting pressure on the \\ndata center industry in geographies with water resource constraints.  Air cool-\\ning uses high volume of water for evaporative cooling, which is commonly used \\nto achieve PUEs of <1.2.   \\n• Space savings – Liquid cooling allows for significant compaction of the IT, \\nwhich provides companies more placement options across the globe, including \\nspace constrained regions like Asia. \\n• Energy savings – The Green Grid has published a report about energy impact \\nthat showed up to 48% energy savings due to improved efficiency (White Pa-\\nper 70). \\n \\nCapex is less understood, however, and is often viewed as a common obstacle to \\nliquid cooling adoption.  In this paper, we analyze a 2 MW data center using both \\ntraditional air-cooled technology (air-cooled chiller) and liquid cooled technology \\n(chassis-based immersive).  We walk through different density scenarios for the liq-\\nuid cooled technology, to show the impact it has on cost. \\n \\nFigure 1 shows the results of the capex analysis overall.  It demonstrates that for \\nlike densities (10kW/rack), the data center cost of an air-cooled and liquid-cooled \\ndata center are roughly equal.  But as described above, liquid cooling also enables \\ncompaction of the IT, and with compaction, there is an opportunity for a capex sav-\\nings.  When compared to the traditional data center at 10 kW/rack, a 2x compaction \\n(20 kW/rack) results in a first cost savings of 10%.  When 4x compaction is assumed \\n(40 kW/rack), savings goes up to 14%. \\n \\n \\n \\nIn the remaining sections of this paper, we will explain the architectures compared, \\ndescribe the methodology, assumptions and data used in the analysis, and walk \\nthrough waterfall diagrams to demonstrate where the cost differences exist. \\nIntroduction \\nFigure 1 \\nCapex overview \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     3 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\nThe baseline case for this capital cost analysis is an air-cooled data center, using a \\npackaged chiller. This architecture features: \\n \\n• Redundant packaged air-cooled chillers \\n• Redundant pump package \\n• Redundant chilled water computer room air handling units (CRAH) for the IT \\nspace and facility spaces \\n• Hot aisle containment system with ducted plenum back to CRAHs \\n• No raised floor \\n \\nThis architecture was chosen as the baseline for comparison because it is a cost-\\neffective design and very common for medium to large sized data centers.  White \\npaper 59, The Different Technologies for Cooling Data Centers describes this archi-\\ntecture in greater detail, as well as other common designs.   Figure 2 is a high-level \\ndiagram of this architecture. \\n \\n \\n \\n \\nFor this analysis, we chose chassis-based immersive liquid cooling because it fits \\nthe familiar data center rack architecture and removes nearly all the heat via liquid.  \\nFigure 3 is a high-level diagram of this liquid cooling approach.  White Paper 265, \\nLiquid Cooling Technologies for Data Centers and Edge Applications explains the \\narchitectures that exist, and the differences between them.   \\n \\n \\n \\n \\n \\nThe facility infrastructure is greatly simplified with chassis-based immersive cooling \\nsince the IT equipment can use warm water for cooling.  We assumed 40°C (104°F) \\nas the inlet temperature, which allows for 100% free cooling in many climates.  Fig-\\nure 4 is a high-level diagram of the architecture we analyzed.  \\n \\n \\nLiquid cooled  \\narchitecture \\nAir-cooled  \\narchitecture \\nFigure 2 \\nAir-cooled architecture \\nFigure 3 \\nChassis-based immer-\\nsive cooling \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     4 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\n \\nIn this architecture, although most of the heat is rejected via the warm water loop, \\nsupplemental cooling is still required for electrical rooms, as well as removing IT \\nequipment heat not captured in the warm water.  This will maintain the facility air \\ntemperature suitable for people and ensure equipment temperature limits are not ex-\\nceeded, for example, batteries and breakers. \\n \\n \\nWe first analyzed the cost differences of a 2 MW data center at 10 kW/rack, for both \\nthe air-cooled architecture and liquid-cooled.  Table 1 provides the key assumptions \\nof the architectures being compared. \\n \\nAir cooled packaged chiller\\nChassis-based \\nimmersive liquid cooled\\nCore & Shell cost\\nPower redundancy\\nIT equipment cost\\nCooling redundancy\\nN+1 chiller, CRAH, & pumps\\nN+1 (DX CRAC / dry cooler / pumps)\\nIT design capacity (kW)\\n2,000\\n1,880\\nNon-cooling server capacity (kW)\\n1,820\\n1,820\\nIT equipment fan savings\\nN/A\\n9%\\nMicro-pump penalty\\nN/A\\n3%\\nRack heat loss to air\\nN/A\\n5.5%\\nAdiabatic cooling\\nYes\\nNo\\nFloor type\\nHard floor\\nHard floor\\nContainment\\nHACS\\nNone\\nChilled / Condenser water setpoint \\n20/30°C (68/86°F)\\n40/47.75°C (104/117.95°F)\\nGlycol %\\n25%\\n25%\\nIT room supply air setpoint\\n24°C (75.2°F)\\n23.8°C (74.84°F)\\nIT room supply air delta T\\n14°C (25.2°F)\\n9.8°C (17.64°F)\\nElectrical room supply air setpoint\\n24.6°C (76.28°F)\\n24.6°C (76.28°F)\\nElectrical room supply air delta T\\n8.4°C (15.12°F)\\n8.4°C (15.12°F)\\nRack density (kW/rack)\\n10\\n9.4\\nRack U height\\n42\\n42\\nQuantity of racks\\n200\\n200\\nRacks per pod\\n40\\n40\\n2N power dist / 2N UPS / N+1 Generator\\n$90/sq ft ($969/sq m)\\nServer cost not included\\n*\\n \\n* A server fan energy value of 9% of overall server energy is conservative.  Fan en-\\nergy values of 15 – 20% have been measured, especially on high intensity compute \\nequipment such as GPUs. \\n \\nBefore we present the findings, there are two important considerations we need to \\ndescribe, as they (1) impact the ability to show an apples-to-apples comparison, and \\n(2) explain the cost assumption for the emerging liquid cooled technology. \\n \\nTable 1 \\nArchitecture  \\nassumptions \\nCapex analysis \\nwith same \\ndensity \\nFigure 4 \\nLiquid cooled architecture \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     5 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nNormalizing data center cost per watt – Communicating the cost to build a data \\ncenter is traditionally done as cost per IT watt capacity.  For example, if a data cen-\\nter can host up to 2 MW of IT load, and it cost $20 million USD to build, then its cost \\nis $10/watt.  Costs can vary greatly based on redundancy or architecture choices. \\n \\nWhen comparing an air-cooled architecture to liquid cooled architectures, we were \\nfaced with a challenge when calculating cost per IT watt.  For the same amount of \\ncompute, liquid cooling has a lower overall IT load.  Internal IT fans of air-cooled \\nservers consume more power than the internal micro-pumps for chassis-based im-\\nmersion, for the same IT compute load.  To compensate for this, we defined the IT \\nload as “non-cooling server capacity” and used that for the denominator in the \\ncost/watt calculations. This is shown in our assumptions (Table 1) as well as total IT \\ndesign capacity for both air-cooled and chassis-based immersion. \\n \\nChassis-based immersion cost assumptions – For this liquid cooling architecture, \\nthere is technology and associated costs added to the IT equipment to enable liquid \\ncooling.  For the purposes of this analysis, we aggregated all the costs that would be \\nincurred inside the server as well as the rack.  This includes:  dielectric fluid, micro \\npumps, tubing, heat exchanger(s), liquid heat sinks, dripless connectors, sealed \\nchassis, and rack water manifold.  Note that for chassis level immersion, there is a \\nsavings for air heat sinks and fans, so the cost in this study is the assumed delta.  \\nChassis level immersive technology is not fully mature, so costs can vary quite a bit \\nfrom one-off proof of concept, to future optimized supply chain.  We conducted a \\nsensitivity analysis on these costs and estimate the range to be ~$1.10/watt on the \\nhigh side and ~$0.50/watt on the low side.  For this study, we chose $0.77/watt at \\n10kW / rack as a conservative value that should be achievable in any at-scale de-\\nployment.  Note that this value improves as density goes up from the baseline of 10 \\nkW/rack.  Our study considers savings such as fewer rack manifolds and chassis \\nper kW of IT when there is compaction. \\n \\nFindings \\nMany cost studies on liquid cooling consider overall TCO as well as compaction.  \\nThis makes it difficult to understand where the savings occur or where costs shift.  \\nThis study focuses only on capex and first looks at like-for-like rack density then \\nshows two more scenarios of increasing densities. \\n \\nThe waterfall chart in Figure 5 shows the main categories of costs and their change.  \\nThe costs are inclusive of equipment, installation, design, and project costs.  At 10 \\nkW per rack this shows that a 100% chassis immersed data center would cost \\nroughly the same as a traditional air-cooled data center.   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 5 \\nCost/watt difference of air-cooled vs liquid-cooled data center, both at 10 kW/rack \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     6 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\nChiller/CRAH – $0.91/W savings. This represents the removal of the air-cooled \\nchillers and computer room air handlers (CRAHs). \\n \\nLiquid cooling technology – $0.77/W premium.  This premium represents the in-\\ncrease in costs at the server and rack level.  As mentioned previously, this includes \\nthe sealed chassis, dielectric fluid, liquid heat sinks, tubing, micro pumps, heat ex-\\nchanger, dripless connectors, and rack manifold. \\n \\nDry coolers & CRACs – $0.31/W premium.  This architecture can reject heat di-\\nrectly to a dry cooler, without the aid of compressors.  This cost adder includes N+1 \\ndry coolers, as well as additional DX computer room air conditioners (CRACs) to re-\\nmove the heat that the warm fluid cannot.  Note that other architectures are possi-\\nble, such as downsizing the chiller and using CRAHs, which might provide benefits \\nfor larger facilities. \\n \\nPumps & piping change – $0.03/W premium.  This small cost increase considers \\nchanges from the chilled water (CW) piping to warm water piping for liquid cooled \\nservers.   Additional piping needs to be run down the rows of racks with take-off \\nvalves for each rack.  This is nearly offset from the savings of no piping insulation.  \\nA benefit for liquid cooling is that much of the piping does not need to be insulated.  \\nWith water temperatures at 40°C (104°F), there is little to no chance of condensa-\\ntion.  The pumping system is N+1 and the water loop design provides for mainte-\\nnance of sections of racks. \\n \\nReaders familiar with liquid cooling deployments may note that we have not men-\\ntioned a “CDU”.  A cooling distribution unit (CDU) is a device that separates the Fa-\\ncility Cooling System (FCS) from the Technical Cooling System (TCS), which is the \\nwater supplied to the racks.  CDUs provide several functions, such as: \\n \\n• ensuring water to racks is the right chemistry and cleanliness.  This is im-\\nportant for many cold plate deployments and where the FCS is of poor quality. \\n• providing warm water loop to racks in mixed facility, when connecting to a CW \\nloop. \\n• maintaining a lower water pressure than FCS, especially needed in multi-story \\npiping systems. \\n \\nIt is analogous to a transformer in an electrical design, providing separation of two \\nsystems for multiple purposes.  We did not include CDUs in our architecture be-\\ncause the layout is single story, the water loop is dedicated to liquid cooling, and \\nchassis immersion heat exchangers are quite robust and can accept facility quality \\nwater.  As a rule of thumb, CDUs will add about $0.10 to $0.20 per watt of capex. \\n \\nReduce UPS & switchgear size – $0.14/W savings.  Removal of the chiller system \\nand CRACs reduces the amount switchgear provisioned for cooling.  Additionally, \\nsince the IT load is slightly lower due to the change from IT fans to micropumps, \\nfewer UPSs and batteries are needed.  \\n \\nSpace, rack & containment savings – $0.10/W savings.  Although there is no \\ncompaction of the IT white space, facility space is saved by the reduction of cooling \\nsystem switchgear and UPS systems.  Savings include other associated costs linked \\nto space, like fire suppression, lighting, etc.  Additionally, liquid cooling does not re-\\nquire any air containment, so this is removed.  \\n \\n \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     7 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\nAs we discussed in the introduction, one of the known benefits of liquid cooling is \\nthe ability to compact the IT equipment and save space.  But beyond space savings, \\nthis compaction allows for less IT racks and less rack PDUs.  In this section, we \\nquantify the total capex of a data center with 2x compaction and one with 4x com-\\npaction, using chassis-immersive liquid cooling, and compare those scenarios to our \\nbaseline of the air-cooled data center at 10 kW described earlier. \\n \\nTable 2 provides the new assumptions for rack density, and number of racks, given \\nthis compaction.  All other assumptions remain the same as the original liquid \\ncooled scenario in Table 1. \\n \\nChassis-based immersive liquid \\ncooled at 2x compaction\\nChassis-based immersive liquid \\ncooled at 4x compaction\\nRack density (kW/rack)\\n18.8\\n37.6\\nRack U height\\n42\\n42\\nQuantity of racks\\n100\\n50\\nRacks per pod\\n20\\n10\\nNumber of pods\\n5\\n5\\n \\n \\n \\nFindings at 2x compaction \\nWe started first with a 2x compaction, meaning the density per rack for liquid cooling \\nis 20 kW/rack vs. the 10 kW/rack for the air-cooled architecture.  Figure 6 illustrates \\nthe capex savings of 10%. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nChiller/CRAH – $0.91/W savings.  This savings is identical to the 10 kW/rack sce-\\nnario and represents the removal of chillers and CRAH units. \\n \\nLiquid cooling technology – $0.71/W premium.  This is a smaller cost increase \\nthan the 10 kW/rack scenario.  There is an improvement to this cost when increasing \\nrack density since fewer rack manifolds are needed. \\n \\nDry coolers & CRACs – $0.31/W premium.  This is the same as at 10 kW/rack, as \\nthe total IT load and associated losses are the same. \\n \\nPumps & piping change – $0.03/W savings.  Although piping is required down \\neach row and to the rack compared to the air-cooled scenario, as the IT space con-\\ntracts & rack kW increases, less piping and valves are required compared to the 10 \\nkW/rack scenario. Note that generally the pipe diameter increases as density \\nThe impact of \\ncompaction  \\nFigure 6 \\nCost/watt difference of air-cooled vs liquid-cooled data center, \\naccounting for 2x compaction (20 kW/rack) for liquid cooling \\nTable 2 \\nAssumption variables that \\nchange for compaction \\nscenarios \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     8 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nincreases, but the price difference between the pipes isn’t as significant as the de-\\ncrease in the number of racks. \\n \\nReduce UPS & switchgear size – $0.14/W savings.  This is the same as the 10kW \\nscenario, as rack density doesn’t impact the UPS and switchgear sizing. \\n \\nSpace, rack & containment savings – $0.63/W savings.  This significant savings is \\ncomprised core & shell savings, fewer rack and rack PDUs and less structure over \\nthe racks for cabling supports.  Savings are also achieved in fire suppression and \\nlighting as the IT space gets smaller. \\n \\nFindings at 4x compaction \\nThe next compaction scenario we compared was 4x compaction, or 40 kW/rack for \\nthe liquid-cooled architecture, compared to 10 kW/rack for air-cooled.  As compac-\\ntion increases, the capex savings increases.  Figure 7 illustrates the resulting capex \\nsavings of 14%. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nThe majority savings by further compaction come in two areas.  Less cost for rack \\nlevel liquid cooling equipment, and the costs associated with smaller IT space. \\n \\n \\n \\n \\nChiller/CRAH – $0.91/W savings.  This savings is again identical to the 10 kW/rack  \\nand 20 kW/rack scenarios and represents the removal of chillers and CRAH units. \\n \\nLiquid cooling technology – $0.68/W premium.  This is a smaller cost increase \\nthan the 20 kW/rack scenario.  There is an improvement to this cost when increasing \\nrack density.  Fewer rack manifolds are needed, which improves the overall cost / \\nwatt in this category. \\n \\nDry coolers & CRACs – $0.31/W premium.  This is the same as at 10 kW/rack and \\n20 kW/rack scenarios, as the total IT load and associated losses are the same. \\n \\nPumps & piping change – $0.04/W savings.  This is a slightly higher savings than \\nthe 20 kW/rack scenario as the IT space shrinks and fewer racks are supplied water. \\n \\nReduce UPS & switchgear size – $0.14/W savings.  This is the same as the 10 \\nkW/rack and 20 kW/rack scenarios. \\n \\nSpace, rack & containment savings – $0.90/W savings.  An additional $0.27/W is \\nsaved over the 20 kW per rack design.  This significant savings is comprised core & \\nshell savings, fewer rack and rack PDUs, and less structure over the racks for \\nFigure 7 \\nCost/watt difference of air-cooled vs liquid cooled data center, \\naccounting for 4x compaction (40 kW/rack) for liquid cooling \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     9 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\ncabling supports.  Savings are also achieved in fire suppression and lighting as the \\nIT space gets smaller. \\n \\n \\nLiquid cooling has been around for some time now, but recently has gained interest \\nfor more mainstream data center applications.  While the energy savings are clear, \\nsome are concerned about the capital cost implications.  This paper demonstrates \\nthat deploying chassis-based immersive liquid cooling is similar in capital cost to air \\ncooling when deploying at equivalent rack densities and can save up to 14% when \\ncompacting the IT equipment (and therefore the racks) by a factor of four.   \\n \\nAlthough chassis-based immersive cooling is not a mature technology, these costs \\nare representative of a near-term deployment at scale.  Additional savings can be \\nexpected as the technology and manufacturing efficiencies improve. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAbout the authors \\nRobert Bunger is a Program Director within the CTO office at Schneider Electric.  In his 21 years \\nat Schneider Electric, Robert has held management positions in customer service, technical sales, \\noffer management, business development & industry associations.  While with APC / Schneider \\nElectric, he has lived and worked in the US, Europe, and China.   Prior to joining APC, he was a \\ncommissioned officer in the US Navy Submarine force. Robert has a BS in Computer Science from \\nthe US Naval Academy and MS EE from Rensselaer Polytechnic Institute. \\nWendy Torell is a Senior Research Analyst at Schneider Electric’s Data Center Science Center.  \\nIn this role, she researches best practices in data center design and operation, publishes white pa-\\npers & articles, and develops TradeOff Tools to help clients optimize the availability, efficiency, and \\ncost of their data center environments.  She also consults with clients on availability science ap-\\nproaches and design practices to help them meet their data center performance objectives.  She \\nreceived her bachelor’s of Mechanical Engineering degree from Union College in  \\nSchenectady, NY and her MBA from University of Rhode Island. Wendy is an ASQ Certified  \\nReliability Engineer. \\nVictor Avelar is the Director and Senior Research Analyst at Schneider Electric’s Data Center Sci-\\nence Center.  He is responsible for data center design and operations research, and consults with \\nclients on risk assessment and design practices to optimize the availability and efficiency of their \\ndata center environments.  Victor holds a bachelor’s degree in mechanical engineering from Rens-\\nselaer Polytechnic Institute and an MBA from Babson College.  He is a member of AFCOM. \\n \\nConclusion \\nRATE THIS PAPER     \\uf0ea\\uf0ea\\uf0ea\\uf0ea\\uf0ea \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     10 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\n \\nLiquid Cooling Technologies for Data Centers and Edge Applications   \\nWhite Paper 265 \\n \\nFive Reasons to Adopt Liquid Cooling  \\nWhite Paper 279 \\n \\n \\n \\n \\n \\n \\n \\nData Center Capital Cost Calculator \\nTradeOff Tool 4 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nContact us \\nFor feedback and comments about the content of this white paper: \\nData Center Science Center \\ndcsc@schneider-electric.com \\nIf you are a customer and have questions specific to your data center project: \\nContact your Schneider Electric representative at \\nwww.apc.com/support/contact/index.cfm \\nBrowse all  \\nwhite papers  \\nwhitepapers.apc.com  \\ntools.apc.com  \\nBrowse all  \\nTradeOff Tools™ \\nResources \\n© 2019 Schneider Electric. All rights reserved. \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     11 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\n \\nDrawings \\nThe appendix contains the high-level floor layouts and piping diagrams that were \\nused as the basis for the cost analysis. The detail may be difficult to read at this \\npage size, so feel free to reach out to the Data Center Science Center at \\ndcsc@se.com if you are interested in the raw drawing files. \\n \\nThe layout drawings included are: \\n \\n• Air-cooled layout at 10 kW/rack – Page 12 \\n• Liquid-cooled layout at 10 kW/rack – Page 13 \\n• Liquid-cooled layout at 20 kW/rack – Page 14 \\n• Liquid-cooled layout at 40 kW/rack – Page 15 \\n \\nThe piping diagrams included are: \\n \\n• Air-cooled layout at 10 kW/rack – Page 16 \\n• Liquid-cooled layout at 10 kW/rack – Page 17 \\n• Liquid-cooled layout at 20 kW/rack – Page 18 \\n• Liquid-cooled layout at 40 kW/rack – Page 19 \\n \\n \\nAppendix \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     12 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nAir-cooled floor layout, 10kW per rack \\n \\n \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     13 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nChassis immersion floor layout, 10kW per rack \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     14 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nChassis immersion floor layout, 20kW per rack \\n \\n \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     15 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nChassis immersion floor layout, 40kW per rack \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     16 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nAir-cooled piping diagram, 10kW per rack \\n \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     17 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nChassis immersion piping diagram, 10kW per rack \\n \\n \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     18 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nChassis immersion piping diagram, 20kW per rack  \\n  \\n \\n \\nSchneider Electric  –  Data Center Science Center    White Paper 282    Version 1     19 \\nCapital Cost Analysis of Immersive Liquid Cooled vs. Air Cooled Large Data Centers \\nChassis immersion piping diagram, 40kW per rack  \\n \\n', 'Sustainability with substance \\n1\\nSustainability \\nWith Substance  \\nfor Your Hyperscale Datacenter\\nBuilding a Greener Future With Immersion Cooling\\nSustainability With Substance \\n2\\nTable of Contents\\n1. Introduction \\t\\n03\\n2. Assessing the Datacenter Impact\\t\\n04\\n3. Datacenter Sustainability Challenges \\t\\n07\\n3.1 Hardware\\t\\n07\\n3.2 Energy  \\t\\n07\\n3.3 Water\\t\\n08\\n3.4 Heat Reuse\\t\\n09\\n4. The Game-Changing Cooling Technologies on Offer\\t\\n11\\n4.1 Direct Liquid Cooling (DLC)\\t\\n11\\n4.2 Single-Phase Immersion Cooling\\t\\n12\\n4.3 Two-Phase Immersion Cooling\\t\\n13\\n5. Submer: Leading Immersion Cooling Innovation\\t\\n14\\n6. Conclusion: Cooling Our Data-Driven Future\\t\\n16\\n3\\nIntroduction\\nHyperscalers are at a critical \\ncrossroads. Business is booming and \\ncapacity is rising, set to triple by 2028 \\nby Synergy Research Group’s forecast1. \\nArtificial Intelligence (AI) workloads \\nare one of the key drivers proving a \\nchallenge of hyperscale proportions. \\nChips – central processing units \\n(CPUs) and graphics processing units \\n(GPUs) – becoming more powerful, \\nmore power-hungry, and inevitably \\nmore heat-producing. This explains \\nwhy energy and water consumption \\nare on the up. And why we’re seeing \\nlegislators from the US to Europe \\nto Asia-Pacific regulating multiple \\nissues, from carbon emissions and \\npower usage effectiveness (PUE) to \\nwater consumption and heat reuse. \\nWhile these directives are only set to \\nget more stringent, they’re not the \\nonly driver for a greener approach to \\ndatacenter operations.\\nCommunities are more conscious \\nof the impact of datacenters on the \\nenvironment, and they’re making \\ntheir opinions known – for example, \\nin Uruguay2 and the Netherlands3.  \\nCompanies are taking a closer look at \\ntheir supply chains, and in an effort \\nto comply with guidelines such as \\nthose from the OECD (Organization \\nfor Economic Co-operation and \\nDevelopment), they’re choosing \\nproviders that are transparent and \\nsustainable. Plus, hyperscalers \\nhave their own ambitious goals \\nas they strive to be greener while \\ndelivering the immense performance \\nrequirements of today’s and \\ntomorrow’s data workloads. \\nIn this white paper, we’ll explore \\nthe pressures hyperscalers face as \\nenablers of our new world; where \\nsustainability is the Holy Grail, but \\ninnovation requires performance \\nat ever-higher levels, pushing \\nresources to the limit and requiring \\nsmarter solutions. Plus, we’ll see how \\nimmersion cooling – the way Submer \\ndoes it – is a critical component of \\nyour sustainable, future-proofed \\ndatacenter strategy.\\n 1 https://www.srgresearch.com/articles/hyperscale-data-center-capacity-to-almost-triple-in-next-six-years-driven-by-ai \\n2 https://www.theguardian.com/world/2023/jul/11/uruguay-drought-water-google-data-center \\n3 https://www.techerati.com/news-hub/hyperscale-data-centres-under-fire-in-holland/\\n4\\n“To reduce the amount of energy used, environmental sustainability \\nrequirements for the industry may become mandatory and more stringent \\nover the next decade. This shift may include introducing a carbon tax for \\ncompanies that do not meet the requirement of net-zero emissions.”\\n451 Research, June 2022\\nAssessing the Datacenter Impact\\nIt’s indisputable that datacenters are \\nessential to our data-driven lives. But their \\nincreasing scale also presents a growing \\nconcern for the environment. Datacenters \\ntoday are inefficient, consuming excess \\nenergy and water, valuable resources, and \\ngenerating waste heat.\\nThere’s a need for change. As the Uptime \\nInstitute’s 2023 Survey states: “Regulations \\naimed at datacenter energy use require \\nurgent attention, investment, and \\naction.” Global electricity use by Amazon, \\nMicrosoft, Google, and Meta more than \\ndoubled between 2017 and 2021 to reach \\n72TWh,2  with global datacenter electricity \\nconsumption in 2022 at around 1 to 1.5% of \\ntotal global electricity demand.4 And then \\nthere’s water. It’s estimated that a small \\n1MW datacenter can use around 25.5 million \\nliters of it each year.5  \\nGiven the backdrop of climate change, \\ngovernment regulation is becoming \\nstricter worldwide. The European Union \\n(EU) Energy Efficiency Directive6 requires \\ndatacenters of 500kW or above to report \\non energy performance. Within the EU, \\nGermany is taking a lead, with the German \\nEnergy Efficiency Act mandating a PUE of \\n1.2 or less for new datacenters by 2026.7 \\nIn the US, the New Energy Act covers \\nenergy-efficiency initiatives for the \\ndatacenter industry. Singapore, a major \\ndatacenter hub, recently lifted a three-\\nyear moratorium on builds, but with strict \\nrequirements for highly energy-efficient \\ndesigns going forward.\\nSECTION 1\\n4   https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks\\n5  https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks\\n6  https://smartwatermagazine.com/news/h2o-building-services/how-much-water-do-data-centres-use\\n7  https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ%3AJOL_2023_231_R_0001&qid=1695186598766\\n5\\n8  https://sustainablefutures.linklaters.com/post/102ir6a/the-new-german-energy-efficiency-act-implications-for-data-centers-and-other-com\\n9 https://www.techerati.com/news-hub/hyperscale-data-centres-under-fire-in-holland/\\n10 https://www.datacenterknowledge.com/buildconstruction/hyperscalers-2024-where-next-world-s-biggest-data-center-operators#close-modal\\n11 \\x07https://www.gartner.com/en/newsroom/press-releases/2023-05-02-gartner-predicts-75-percent-of-organizations-will-have-implemented-a-\\ndata-center-infrastructure-sustainability-program-by-2027\\n9  \\x07https://blog.datacentersystems.com/the-importance-of-sustainable-data-centers-why-it-matters-for-the-environment-and-your-organization\\nThere’s also pressure from communities, \\nsuch as the protests seen in the Netherlands \\nwhere groups including climate activists \\nand farmers opposed new datacenter \\nconstruction on environmental grounds8.\\nSo, with investment in new facilities well \\ninto the billions of dollars9, – and in the face \\nof legislation, community opposition, and \\nenvironmental, social, and governance \\n(ESG) targets – hyperscalers need to plot \\na sustainable path forward to ensure their \\nvery existence. There are many benefits to \\nusing sustainable practices in datacenter \\ndesign and operation. The first, of course, is \\nto reduce environmental impact. In addition, \\n“sustainable datacenters can also save \\nmoney through reduced energy and water \\nconsumption and can improve the reliability \\nand performance of the infrastructure11 ”, \\naccording to datacenter provider Iron Mountain.\\n75% of organizations will have \\nimplemented a datacenter \\ninfrastructure sustainability \\nprogram driven by cost \\noptimization and stakeholder \\npressures by 2027, up from \\nless than 5% in 2022.10\\n75%\\nGartner \\n6\\nHyperscalers’ Carbon \\nReduction Goals\\nAWS Climate Pledge plots \\npath to net zero by 2040.\\nMeta committed to net zero \\nacross value chain by 2030.\\nGoogle committed to \\nnet zero by  2030.\\nMicrosoft has committed to carbon \\nneutrality by 2030, addressing both \\ndirect and indirect emissions.\\n2040\\n2030\\n2030\\n2030\\n7\\nDatacenter Sustainability Challenges \\nGetting to a greener cloud requires understanding what’s going on in the \\ndatacenter, specifically the key considerations – use of energy and water, heat \\ncapture, and hardware lifecycles – in the sustainability landscape.\\n3.1 Hardware \\n  \\nOptimizing hardware is a top priority for \\nhyperscalers. The rise in chip density and \\ngeneral compute, distributed heat flux \\nworkloads, and increasingly hot DIMMs, \\nFPGAs, and many other components means \\ncooling needs are growing exponentially. \\nThe impact on total cost of ownership (TCO) \\nis inescapable, and it’s why hyperscalers \\nneed cooling technologies that ensure \\nthe entire IT estate can be properly and \\nefficiently cooled. Rather than refreshing \\nhardware every 3 to 5 years as is typical, \\nhyperscalers must look to new immersion \\ncooling technology that enables them to \\nkeep hardware at peak condition for longer, \\noutlasting several IT generations and \\nfuture-proofing facilities.\\n3.2 Energy  \\nWe’ve touched on PUE. This is the most \\ncommonly used measurement to show how \\nefficient a datacenter is. More precisely, \\nit is the ratio of total amount of energy \\nused by a datacenter facility to the energy \\ndelivered to computing equipment. So \\nthe lower the PUE, the more efficient the \\ndatacenter.\\nAccording to the Uptime Institute, while \\nPUE gains have been made over the years \\n(dropping significantly from 2.5 in 2007 to \\n1.67 in 2014), PUE has remained flat for the \\n4 years leading up to 2023.12 \\nA lower PUE can be achieved through \\nefficient cooling systems. As for reducing \\nenergy consumption, investment in areas \\nsuch as datacenter design, best practices, \\nand new technology is required.\\nSECTION 2\\nThe global average PUE of \\ndatacenters has remained flat \\nfrom 2019 to 2023.\\n8\\n3.3 Water \\n \\nWater usage has historically been a \\nlower priority when it comes to tracking \\ndatacenter sustainability, but it’s \\nincreasingly important as the scale of \\nwater use in cooling facilities has grown \\nand water has become more of a scarce \\nresource. Most of the water consumed \\nin the datacenter is for cooling the \\nenvironment. Water usage effectiveness \\n(WUE) is calculated as the ratio between \\nwater used at the datacenter and \\nelectricity delivered of the IT hardware. \\nAlong with environmental impacts, the \\ncost of water is also a major concern for \\nhyperscalers. At its San Antonio, Texas \\ndatacenter, Microsoft found that, when \\nrisk-adjusted, the true cost of the water \\nit was using was 11 times more than it was \\npaying.13 \\nWater replenishment schemes are one \\nway that hyperscalers are addressing \\ntheir water consumption. And most \\n(AWS, Meta, Microsoft) have pledged to \\nbe water positive by 2030 or have water \\nstewardship goals in place (Google). But \\nwithout addressing cooling, massive \\nreliance on water will remain a significant \\nenvironmental issue. And cost.\\n “The datacenter industry has the unique opportunity to be a catalyst \\nfor meaningful water stewardship action. While the path(s) toward \\nsustainability is uncertain, bold environmental commitments are driving \\ninnovation and engaging stakeholders and investors.” 14\\n451 Research, June 2022\\nAWS, Meta, and Microsoft \\nare committed to being \\nwater-positive by 2030.\\n12  \\x07https://www.businesswire.com/news/home/20230718020841/en/Uptime%E2%80%99s-13th-Annual-Global-Data-Center-\\nSurvey-Shows-Widening-Range-of-Challenges\\n13  https://oerc.ox.ac.uk/case-studies/the-true-cost-of-water-guzzling-data-centres/\\n14  https://www.watertechonline.com/water-reuse/article/14215042/data-center-water-sustainability-and-stewardship\\n9\\n3.4 Heat Reuse\\nEnergy efficiency is a key goal for the smart, \\nsustainable datacenter – and within this, heat \\nreuse is a critical component with potential \\ngreat implications for carbon emissions. As \\nwe’ve seen, datacenters create a lot of heat \\n– a typical datacenter campus generates \\nenough to power a mid-sized city.15  In the \\nEU, the Energy Efficiency Directive requires \\ndatacenters to recycle waste heat, and heat \\nreuse schemes have gained some traction. \\nFor example, district heating initiatives \\ncapture waste heat and transfer it via heat \\nexchangers to be reused in other buildings or \\nfacilities. These initiatives not only benefit \\nthe environment and communities but also \\npositively impact TCO through potential \\nrevenue generation for hyperscalers.\\nWhile this is no new idea (there are around 60 \\nheat recapture projects currently in action \\naround Europe16), reusing heat waste at scale \\nis challenging to implement. Not least because \\nof the new partnerships and technology it \\nrequires, as well as the fact datacenters are \\noften located in remote areas.\\n15  \\x07https://www.achrnews.com/articles/146987-data-centers-get-larger-hotter-making-them-attractive-sources-of-heat#:~:text=Heat%20\\nCenters,power%20a%20mid%2Dsized%20city.\\n16  https://uptimeinstitute.com/resources/research-and-reports/heat-reuse-a-management-primer\\n10\\nThe heat reuse process centers on the Heat Recovery Unit (HRU) within the Heat Recovery \\nSystem. This transfers heat to the application of choice, whether heating, hot water, radiant \\nfloor, or other. The HRU guarantees optimal energy recovery, and works by receiving the heat \\ngenerated on IT devices and hardware, and, if needed, increasing the supply temperature, \\ndepending on the application requirement.\\nHeat Reuse in Action: Excess Heat \\nTransformation with W.E. District & RISE\\nSubmer provides modular immersion cooling pods for W.E. \\nDistrict – a coalition of nine European companies that aims \\nto demonstrate that district heating can be built using \\nrenewable energy sources – and Research Institutes of \\nSweden (RISE). The project will capture heat generated by a \\ndatacenter in Luleå, Sweden, and repurpose it for the local \\ndistrict heating in the area. The generated electricity will be \\nused to power the datacenter itself.\\n11\\nThe Game-Changing Cooling \\nTechnologies on Offer\\nAs datacenters have grown bigger and \\nhotter, cooling technology has become \\nmore and more important for future-\\nproofing hyperscaler sustainability. The \\ntransition from air cooling has been driven \\nby the need to handle chips with greater \\nthermal design power (TDP). Today, air \\ncooling is already largely inefficient. Soon, \\nit will be entirely ineffective without the \\naddition of technologies like direct liquid \\ncooling (DLC) or partial immersion cooling.\\n4.1 Direct Liquid Cooling (DLC)\\nAlso known as DTC (Direct to chip) cooling, \\nDLC marks the next phase in cooling \\ntechnology, offering numerous benefits \\nfor datacenter sustainability. With the \\ncapability to effectively manage chips \\nwith a thermal design power (TDP) of \\n400W and above, DLC involves circulating \\nliquid directly over hardware components, \\nincluding entire servers, to efficiently \\ndissipate heat. \\nThis method represents a substantial \\nimprovement over conventional air cooling, \\nresulting in reduced energy consumption, \\nlower carbon emissions, and enhanced \\nsustainability. It also holds promise for heat \\nreuse, further maximizing its environmental \\nbenefits.\\nSECTION 3\\n12\\nDespite its advantages, DLC presents \\ncertain limitations that must be \\nconsidered. While offering superior cooling \\nperformance, it is not a complete solution \\nas it relies on air cooling to manage \\ncomponents that are not compatible with \\nDLC. \\nUp to 60% of components may still require \\ntraditional air cooling methods and \\nwhile DLC represents a significant step \\nforward in datacenter sustainability, it \\nwould benefit from further innovation and \\noptimization in cooling technology.\\n4.2 Single-Phase Immersion Cooling\\nImmersion cooling represents the most \\nenergy-efficient way to cool at scale. \\nBy submerging hardware in a specially \\nformulated dielectric coolant, heat is \\ntransferred directly from components. \\nThis method has high heat dissipation \\ncapacity, is safe to use, and is compatible \\nwith any IT hardware. It’s a game-changer \\nfor datacenter sustainability and provides \\ncompetitive TCO advantages: no overhaul \\nof infrastructure required, support of \\nmulti-vendor hardware, and significant \\nenergy savings once operational. \\n13\\n“Air cooling is not enough. That’s what’s driving us to immersion cooling \\n[…] Liquid cooling enables us to go denser, and thus continue the \\nMoore’s Law trend at the datacenter level.”\\nChristian Belady, Distinguished Engineer and Vice President, Microsoft Datacenter Advanced Development Group\\n“I believe there’s been more energy around immersion cooling in the \\nlast year that I’ve heard in a very long time,”   \\n“I think the time is now.”\\nZane Ball, Corporate VP and General Manager Datacenter Engineering and Architecture, Intel\\n4.3 Two-Phase Immersion Cooling\\nWhile it provides heat dissipation capacity \\nsimilar to single-phase, two-phase \\nimmersion cooling requires complex \\nfiltration systems and expensive fluid. \\nMost significantly, it uses poly-fluorinated \\nalkyl substances (PFAS), which are toxic, \\nbioaccumulative chemicals that can’t \\nbe broken down by natural processes. \\nAs a result, the two-phase system, while \\neffective at cooling, is problematic \\nbecause it poses an environmental and \\nhealth risk. This is why the manufacture \\nand use of two-phase cooling are subject \\nto regulations across the globe. The \\nEuropean Chemicals Agency has proposed \\nrestrictions on PFAS. 3M is set to halt \\nproduction of the chemicals in 2025. And \\nin both the US and Europe, the scale of \\ncontamination is being regularly tracked. \\nThis supports the choice of single-phase \\nimmersion cooling as the technology of \\nchoice for the datacenter.\\n14\\nSECTION 4\\nSubmer: Leading Immersion Cooling \\nInnovation\\nBorn from years of experience in traditional datacenters and a passion for a \\ngreener future, Submer has developed immersion cooling technology that’s \\ntransforming datacenter sustainability by addressing energy efficiency, \\ncarbon emissions, water management, and circularity.\\n•\\x07 A unique 3-stage heat transfer \\napproach: Heat from hardware is \\ntransferred to the coolant; from \\ncoolant to water; and from water to \\nany number of possibilities for reuse. \\n•\\x07 \\x07Ground-breaking Forced Convection \\nHeat Sink (FCHS) technology means \\nwe’re one step closer to cooling \\n1000W+ chips.17\\n•\\x07 \\x07PUE of less than 1.10, reducing \\nservers’ energy consumption by over \\n5%, with less leakage current and no \\nfans, and helping hyperscalers meet \\nenergy efficiency targets.\\n•\\x07 \\x07Transforming water management, \\nwith reduction in both direct \\n(cooling, facilities, humidification) \\nand indirect (energy, water \\ntreatment) water consumption.\\n•\\x07 \\x07Providing a boost to hardware \\nlifespans – no moving parts, no dust, \\nno vibrations – and easy integration \\nto heat reuse solutions, increasing \\ncircularity. \\n•\\x07 \\x07Immersion cooling is resilient to \\nhigher environmental temperatures \\nand leads to reduced emissions, \\nwhile dry coolers and closed-\\nloop water circuits mitigate water \\nwastage.\\n•\\x07 \\x07Proven reliability as seen at \\nTelefonica’s datacenter in Madrid \\nthat experienced record-breaking \\ntemperatures, but maintained 100% \\nredundancy, improved hardware \\nperformance, and a PUE of  \\nbelow 1.08. \\n15\\nSmartPod EVO: Plug & Play  \\nImmersion at Scale\\n\\t Efficient and sustainable\\n\\t Easy to deploy\\n\\t Smooth and predictable IT operations\\n\\t The easiest path to immersion cooling!\\nSmartCoolant \\nOur in-house developed synthetic dielectric fluid tailor-\\nmade for immersion cooling. Readily biodegradable, it \\nhas a Global Warming Potential (GWP) of 0 and a projected \\nlifespan of 10 to 15 years.\\nSubmer: Part of Your Sustainable Supply Chain \\nAt Submer, we’re not just cooling experts; we’re a strong link \\nin your sustainable supply chain.\\n  \\x07Equipment aligned with the EU Taxonomy technical \\nscreening criteria for sustainability, enabling datacenter \\noperators to meet their own sustainability requirements\\n  \\x072 out of 3 investors are impact investors \\n  \\x07A top-100 impact company\\n17   Intel and Submer Illuminate the Path to Immersion Cooling for 1000W TDP\\n16\\nSECTION 6\\nConclusion: Cooling Our Data-Driven \\nFuture\\nDatacenter sustainability matters to us all. To continue benefiting from \\ndata-driven experiences and solutions, we need the performance that \\nlarge-scale datacenters give us. But we need to get there in the greenest \\nway possible. This is evidenced by hyperscalers’ sustainability pledges, \\ngovernments’ evolving regulation, and community action and concern. \\nThe challenge before hyperscalers today is balancing performance and \\nsustainability. \\nAt Submer, we’re cooling experts - but adopting our immersion cooling solutions \\ndoesn’t mean going at it alone. We’re here to support you in using our technology \\nas part of your sustainability strategy, so that, together, we can create climate-\\nresilient datacenters that can power a digital tomorrow.\\nDiscover all you need to know about our immersion cooling \\nsolutions here.  \\n', 'Visit asperitas.com/about/ for more\\ninfo\\nVisit aspe\\nritas.com/about/ for more info\\nR e v i s i o n  6\\nR o l f  B r i n k\\nA concept driven by sustainability, efficiency and flexibility. Using the most efficient model for \\noperating IT, Total Liquid Cooling, and going far beyond just technology. Immersed Computing® \\nfocuses on IT optimisation and includes an optimised way of work, highly effective deployment, \\nflexible choice of IT and drastic simplification of datacentre design. Offering great advantages on \\nall levels of the IT platform value chain. Realising maximum results in Cloud, HPC and Edge.\\nFor more information, feedback or to suggest improvements for this document, please contact us \\nat: whitepapers@asperitas.com\\nInformation in this document can be freely used and distributed for internal use only. Copying or re-distribution for \\ncommercial use is strictly prohibited without written permission from Asperitas.\\n•\\t\\nJanuary 02, 2017-Original publication \\n•\\t\\nMarch 06, 2017-Second publication \\n•\\t\\nJune 25, 2017-Third publication \\n•\\t\\nApril 5, 2019-Fourth publication \\n•\\t\\nDecember 30, 2021-Fifth publication\\n•\\t\\nFebruary 14, 2021-Sixth publication\\nImmersed Computing® \\nBy Asperitas\\nImmersed Computing® is an Asperitas \\xadregistered trademark.\\nCopyright © 2022 by \\xadAsperitas, \\nLaarderhoogtweg 18, \\n1101EA Amsterdam, \\nThe Netherlands\\nEstimated reading time: 15 minutes\\nReadability general: basic\\nReadability technically: simplified\\nTable of \\ncontents\\n1. Background\\x08\\n4\\n2. Immersed Computing® explained\\x08\\n6\\n2.1 Total Liquid Cooling\\x08\\n6\\n2.2 Enclosed and self contained\\x08\\n9\\n2.3 Flexibility\\x08\\n10\\n2.4 Way of work\\x08\\n12\\n3. Benefits\\x08\\n14\\n3.1 Energy efficiency\\x08\\n14\\n3.2 Datacentre build\\x08\\n16\\n3.3 Datacentre facilities\\x08\\n17\\n3.4 Datacentre operations\\x08\\n18\\n3.5 IT hardware\\x08\\n19\\n3.6 Software cost\\x08\\n19\\n3.7 Environments with greatest benefits\\x08\\n20\\n4. Asperitas solutions\\x08\\n21\\n5. Best practices\\x08\\n23\\n5.1 Datacentre planning\\x08\\n23\\n5.2 Liquid processes\\x08\\n25\\n6. Asperitas company\\x08\\n26\\n4 | Immersed Computing® whitepaper\\n1. Background\\nThe datacentre industry is at the peak of its \\ngrowth. New datacentres are continuously \\nbeing built and the challenges for datacentres \\nare growing as fast as the industry itself. This \\ngrowth is accompanied by a high demand for \\nhigh density datacentres and platforms. The \\nmain cause of the increasing demand is the \\nInternet of Things (IoT), big data and a global \\nmove to cloud based computing.\\nThe industry is consuming about 5% of the \\nglobal electricity supply and it is still growing. \\nThis has caused the focus to shift from high \\namounts of floor space with distributed IT \\nenvironments, to high density and energy \\n\\xadefficient centralised cloud environments.\\nAir based cooling becomes an ever growing \\nchallenge with the increasing demand for these \\nhigh density cloud environments. Extreme \\nwind speeds within server racks (10+ Beau-\\nfort within chassis) are required to cool high \\ndensity environments and air becomes ever \\nmore problematic in these environments with \\nvibration issues, overhead power for fan energy, \\nzinc whiskers across IT components and power \\nthirsty cooling installations.\\nThe focus has been on cooling, the biggest \\noverhead. The Power Usage Effectiveness \\n(PUE) has been adopted as a major KPI for \\ndatacentres. The formula is simple: total energy \\nfootprint of the facility divided by the energy \\nconsumed by IT. The downside of this approach \\nis that IT inefficiency is being rewarded, thus \\nleaving the focus on high energy savings on \\ncooling installations and less on possible \\nenergy reduction by increasing the energy \\n\\xadefficiency of the IT itself.\\nIn the past years, liquid cooling has been (re)\\nintroduced, although mostly in High Perfor-\\nmance Computing (HPC) environments like \\nsupercomputing. The requirements in this part \\nof the industry are such that more effective \\ncooling allows for higher performance. There-\\nfore, liquid was quickly adopted and today it is \\ninconceivable that a supercomputer will be built \\nwithout some form of liquid cooling.\\n5 | Immersed Computing® whitepaper\\nCloud platforms now have energy and density challenges, which are identical to HPC, although \\nthe density challenges have a different focus compared to HPC environments. Cloud platforms \\nare designed for continuity, flexibility and resiliency. Not necessarily the highest performance. This \\nis where liquid cooling often runs into limitations. The available technologies are often limiting in \\nsome way: complexity, cost, maintenance, cleanliness, compatibility with existing whitespaces or \\nbound to proprietary IT.\\nTo make liquid a viable solution for the cloud industry, a different approach was required. An \\napproach which addresses sustainability, continuity, flexibility, Total Cost of Ownership (TCO), \\n\\xadtidiness, cleanliness and compatibility with existing environments.\\nThis approach is called \\nImmersed Computing®\\nImage 1 Asperitas Immersed Computing® solution\\n6 | Immersed Computing® whitepaper\\n2. Immersed Computing® \\nexplained\\nImmersed Computing® is a concept consisting of technological and operational aspects. It is \\nfocused on a holistic approach towards efficiency. It starts with the view that the digital platform \\nis the central value which needs to be facilitated above anything else. The physical IT platform is \\ntherefore the most critical part of the infrastructure. By optimising the IT platform and all aspects \\naround it, Immersed Computing® allows full optimisation of not only processing, power and \\ncooling, but also maintenance, lifecycle and flexibility of the end-to-end operation.\\n2.1 Total Liquid Cooling\\nTotal Liquid Cooling of IT, also called Immersion Cooling, is at the foundation of Immersed Compu-\\nting®. It refers to the complete immersion of electronic components in a dielectric liquid. By doing \\nso, all the heat generated by the IT is captured in the liquid. Suitable dielectric liquids can absorb \\napproximately 1500 times more heat energy than air with the same volumes and temperatures.\\nTotal Liquid Cooling is not new. It has been an accepted method of conditioning electronics \\nfor more than half a century. In the late sixties, the first patents for oil immersion systems were \\nalready granted. These were abandoned after some time due to the lack of focus on energy effici-\\nency. Air based systems were already abundant and 19” racks already were accepted standards. \\nThe use of Total Liquid Cooling has only remained common practice in niche markets where other \\nfactors became problematic like deep sea research (pressure vessels) or high voltage installations.\\nImage 2 Natural Convection\\n7 | Immersed Computing® whitepaper\\nSuitable dielectric liquids today are hydrocar-\\nbons and fluorocarbons. With the core values \\nof Immersed Computing® in mind, Asperitas is \\nmainly working with medicinal quality synthetic \\noils as a primary cooling medium. This is due to \\nthe wide availability throughout the world, the \\nlow cost and the minimised safety concerns \\ninvolved. Medicinal synthetic oils are manu\\xad\\nfactured by multiple manufacturers and brands \\nmay sometimes be mixed or interchanged, \\ndepending on manufacturer statements. \\n\\xadFluorocarbons, although quite suitable for \\nImmersed Computing®, usually relate to a cost \\nwhich is at least 30 times higher. This makes \\nit less interesting for an “open bath” approach. \\nThe synthetic oils used by Asperitas are similar \\nproducts as Vaseline, although with a higher \\npurity and much lower viscosity.\\nTraditional single phase liquid circulation \\nrequires infrastructures with pumps and piping \\nfor circulating dielectric liquids from a basin, \\nthrough a facility, to a cooling tower and back \\ninto the immersion basin. This same \\xadcirculation \\npushes the dielectric liquids through the IT \\nchassis and over the micro-electronics, thus \\nremoving their generated heat. \\nPassive circulation of the dielectric liquid is \\nwhere Immersed Computing® is unique and \\nground-breaking. Asperitas eliminates any \\n\\xadinfrastructure for the dielectric liquid. Instead, \\nthe liquid is circulated by natural means and \\ndoes not leave the immersion system. The \\nliquid circulates by the heat generated by the \\nIT and water-cooled “Convection Drives®”. \\nThis means that the primary circulation is \\n\\xadcompletely driven by the thermal expansion of \\nthe liquid and gravity. The only requirement for \\nheat rejection is any common liquid coolant \\ninfrastructure.\\n8 | Immersed Computing® whitepaper\\nThe ensured high quality of the liquid in Immersed Computing® is facilitated by the purity of the \\nused liquids, the lack of moving parts and the fact that everything in touch with the liquids has \\nbeen thoroughly tested and certified before commissioning. No air is ever mixed into the liquid in \\nthe system and the lid is closed by default, since a closed lid improves the effectiveness of the \\nthermodynamic process. This prevents oxygen from reacting with the liquid. The thermal stability \\nof the liquid is also guaranteed by liquid quality and extensive safety mechanisms which are inte-\\ngrated in Immersed Computing® solutions.\\nA double hull and cold shell protect the environment of the system from any liquid or thermal \\nleakage. The double hull is insulated and provides an optimal safeguard against any form of \\nleakage. The only place for the heat to go is into the water circuit. This means that the result of \\nImmersed Computing® is coolant with the same amount of thermal energy as the IT electrical \\nconsumption. Simply put, 22 kW IT equals 22 kW heat captured in the liquid, which is rejected with \\nwater at minimised losses.\\nAny type of server can be inserted into the system, although it does not maintain the traditional \\nserver shapes as we know them in air-cooled racks. After all, traditional servers are not designed \\nto allow liquid to flow through the chassis by itself. Immersed Computing® servers consist of IT \\ncomponents like mainboards with components attached, power supplies and storage. Specialised \\nAsperitas Universal Cassettes or AUCs are used for Asperitas systems. All information on the \\nAUCs is publicly available for hardware designers and manufacturers.\\nThe Asperitas certification process addresses any reliability concerns by optimising server \\ndesigns and rigorously testing each design in collaboration with the IT manufacturers. This \\nprocess includes full component compatibility research and qualification, establishing thermal \\nboundaries and limitations and full design documentation which is supported and used by manu-\\nfacturers and integrators to allow volume delivery and support. All server designs are aimed at the \\nhighest temperature tolerances.\\n9 | Immersed Computing® whitepaper\\n2.2 Enclosed and self contained\\nThe focus is on compute, as IT is at the basis of every platform or datacentre. All other aspects \\nof a server, rack, cooling or even power system only facilitate the IT. Immersed Computing® allows \\nfor a shift in focus from Cooling to Compute. The self-contained approach allows for a much \\ndeeper level of integration than any other rack- or liquid solution. Everything required to facilitate \\nand manage IT like power and data connectivity is integrated and fully manageable with Immersed \\nComputing®. \\nIntegrated power distribution is used for powering all IT in the system. The power distribu-\\ntion system can be redundant and fully managed and must be applied in such a way that cable \\nmanagement is simplified and nothing needs to be routed outside the immersion system; \\nIntegrated universal switching allows for the distribution of Ethernet connectivity throughout the \\nsystem. Universal Switching Cassettes allow for any brand of switch to be immersed, and the \\nnetwork connections are distributed to each server with minimised and intuitive cable manage-\\nment; \\nThe integrated cable management approach allows for standardised and optimised serviceable \\ncabling and logical cable management without bundles or over lengths. Only backbone/uplink \\ncabling comes out of the system.\\n10 | Immersed Computing® whitepaper\\n2.3 Flexibility\\nFlexibility is key for Immersed Computing®. Platforms must be easily expandable and should grow \\nwith the environment. To address the current challenges in the industry, systems must be easy to \\ndeploy and the platforms should not be limited to proprietary IT, a fixed shape or size or predefined \\nIT specifications.\\nImmersed Computing® is plug and play. Due to the self-contained approach, a single system \\nrequires only power, access to a water loop and data connectivity to operate. This can all be \\nsupported in up-to 2N configurations which enables full deployment and commissioning in any \\ntype of datacentre environment within a few hours.\\nFuture proofing and climate independence. Immersed Computing® platforms are prepared for \\nhigh cooling temperatures which makes any platform climate independent (global chiller-less \\ndeployments) and the accompanied optimised high return temperature tolerance enables reusable \\nenergy. This enables new deployment strategies for platform and datacentre planning.\\nModularity is achieved with the self-contained immersion systems with a scalable footprint. A \\nsingle water loop can be shared across multiple systems and modules can be placed back-to-back \\nand side-to-side. Since there is no air required for the system to operate, large rows of intercon-\\nnected systems can be placed in relatively small spaces.\\nThe scalability of Immersed Computing® allows for very fast deployment of datacentre locations \\nbecause there is hardly any infrastructure required. Power systems are minimised with less over-\\nhead, cooling infrastructure is minimised and IT is optimised for high utilisation.\\n11 | Immersed Computing® whitepaper\\nThe open IT approach allows for any type of IT to be used with Immersed Computing®, \\xadregardless \\nof brands. Limiting factors are related to the use of liquid. Material \\xadcompatibility is addressed by \\nthe certification process which also ensures an optimised design for liquid.\\nAn end-to-end ecosystem of partners enables Asperitas in the development of any technology \\nwithin immersion. Asperitas enables the development of liquid optimised technologies by building \\nstrategic collaborations with server, chip and component manufacturers around the world. By \\nsharing knowledge within this ecosystem, any immersion challenge is addressed and solved.\\n12 | Immersed Computing® whitepaper\\n2.4 Way of work\\nThe most critical factor with Immersed Computing® is the mind-set and way of work. This does \\nnot only cover the design or certification process. Implementing Immersed Computing® means \\nworking with liquids. Asperitas has been focused on creating a solution where working with liquid \\nis safe, clean and easy.\\nUnderstanding liquid itself is an important first step. By adopting liquids, certain aspects of a data-\\ncentre operation and way of work will need to change. Most obviously, liquid behaves differently \\nthan air. The interaction with any device inside liquid is therefore also different. Anything which \\nis immersed will be wet when it is extracted and this may be the case during servicing or after \\nreplacements. Anything taken from the liquid, when properly worked with, may leak when removed. \\nAlthough dielectric liquids seem similar to water (colour, odour, viscosity etc.), they behave diffe-\\nrently.\\nPreventing leakage is the first step in the design or deployment of any system and the way of \\nwork. The system itself is already focused at no-spill. This leaves the way of work. People servicing \\nIT must be prepared to deal with liquids, to adjust the work process, to have proper supplies and \\ntake the time required for liquids to drain. Immersed Computing® includes all the tools required to \\ndeal with regular and irregular maintenance.\\n13 | Immersed Computing® whitepaper\\nCleaning the smallest spills must be a routine. Depending on the liquid type, leakage may not \\nevaporate, but can remain on a surface unless properly cleaned. A small drop is not a big deal, but \\nlack of containment or clean-up of multiple small spills over time does become an issue. Each drop \\nshould therefore be wiped immediately.\\nSupplies and consumables which are specialised for the type of liquid used are required for main-\\ntenance and clean operations. Water can easily be cleared with a towel or sponge and residue \\nwill simply evaporate, but any hydrocarbon will not be absorbed by the same materials, nor will it \\nevaporate easily. Widely available, but specialised supplies need to be available wherever people \\nare working on Immersed Computing® platforms. Asperitas includes all absorbent materials \\nrequired for normal maintenance and any level of spill management.\\nMaintenance on electronic components requires removal of the IT from the liquid. Since all the \\nIT is placed in a vertical position, it becomes impractical to manually lift a server from a module. \\nThis is addressed with a specialised, (semi-)automatic hoisting mechanism. The Asperitas Service \\nTrolley is specialised for hoisting, servicing and transporting IT, cleaning up spills and servicing \\nliquid.\\nTraining of service staff on the properties and operation of dielectric liquids and the use of essen-\\ntial supplies is of utmost importance. It is quite simple and lessons are easily handed over, but the \\nbasic knowledge should be there.\\n14 | Immersed Computing® whitepaper\\n3. Benefits\\nImmersed Computing® provides benefits for many layers within the IT value chain, from the \\nphysical geographical location to the end user of a platform. Each type of environment is different \\nso it helps to create more insight in the layers underneath a dense platform.\\n3.1 Energy efficiency\\nImmersed Computing® offers the highest energy efficiency of IT environments. This is caused by \\ntwo main factors and several side effects.\\nThe lack fans reduces 6-45% of it energy. \\nAny air-cooled IT equipment requires air circulation. In servers this consumes between 6% and 45% \\nof the total energy footprint. 1U servers with average CPU power with good utilisation (i.e. 80W \\nper CPU) often end up with the highest fan overhead. Larger servers (2-5U) have larger fans which \\nconsume less energy, but these servers take up a lot more space in a rack which makes the rack \\nless effective. This fan overhead and space limitation is completely eliminated with Total Liquid \\nCooling.\\n1U server energy reduction A single 1U fan consumes about 12 watts at full power. A fan \\nassembly usually consists of 2 fans. 1U servers often require at least four of these assemblies \\nacross the width of the chassis and additional fans for the power supplies. This adds up to about \\n120 watts. With low powered servers, this is up to 45% of the energy footprint.\\n15 | Immersed Computing® whitepaper\\nThe high heat capacity of liquid allows IT to operate within higher temperatures compared to air. \\nThis means that running IT in higher environmental temperatures, still allows the IT components \\nto operate well below the maximum component temperature tolerances. Immersed Computing® \\nplatforms are aimed towards 40+°C cooling temperatures, so there are normally no chiller units \\nrequired and energy overhead is reduced to a minimum.\\nEnergy reuse is greatly optimised as all IT energy is captured in the form of heat inside water. After \\nall, the enclosed system is liquid cooled and there is no other way for the heat to go besides the \\nfacility (water-based) coolant which runs through the Convection Drives®. Warm water can easily \\nbe transported or even stored for energy reuse scenarios.\\nThe lack of pumps ensures the highest efficiency in liquid cooling. Mechanically circulated immer-\\nsion systems require significant overhead energy for pumps and control systems. The saving of \\nImmersed Computing® systems due to the natural circulation which regulates itself is enabling the \\nhighest efficiency cooling.\\n16 | Immersed Computing® whitepaper\\nAir-cooled rack\\nAn air-cooled rack can commonly support \\nabout 5kW of IT power, and takes the \\nspace of 2 floor tiles. In order to get suffi-\\ncient air through the rack, 1 or 2 floor tiles \\nare needed in front and in the rear of the \\nrack, also to allow space for servicing. The \\ntotal footprint of the rack now becomes \\n600x2400/3600 mm.\\nThis results in a power density of 1,5-2kW \\nper m2\\nImmersed Computing® \\nAn Immersed Computing® platform which \\nsupports 32 kW IT power with a footprint \\nof 600x1200 mm, needs no airflow. With \\na service area of 600x1200 mm for each \\nmodule.\\nThis results in a power density of more \\nthan 22 kW/m2\\n3.2 Datacentre build\\nThe cheapest datacentre is the one you don’t need to build.\\nReduced floorspace is one of the obvious benefits of Immersed Computing®. Compared to an \\naverage air-cooled cloud datacentre, Immersed Computing® can facilitate 5-10 times as much \\ndensity.\\nNo raised floors and isle separation schemes are required. Since there is no air involved with \\nImmersed Computing®, there are no air flows to separate. Although raised floors are fully \\nsupported and even convenient for routing of facility cooling circuits, they are not a requirement for \\nImmersed Computing®.\\nThe physical location of the datacentre becomes less challenging with Immersed Computing®. \\nSince there is hardly any environmental impact like noise, datacentres can be built in urban areas. \\nThis opens up possibilities to get closer to the edge of the network to allow further growth of \\nInternet of Things and delivery of content to end users with minimised impact on the core network.\\n17 | Immersed Computing® whitepaper\\n3.3 Datacentre facilities\\nOverhead facilities can be downsized or existing capacity can allow for more IT.\\nThe minimised cooling requirements for Immersed Computing® result in smaller and heavily \\nsimplified cooling installations. However, if these are already present, there is more capacity for \\nmore IT power. In reusable heat scenarios, warm water can simply be used and cooled by a heat \\nuser which can eliminate coolers all together.\\nMinimised power requirements have a significant impact on the utilisation of the facility power \\nenvelope. The cooling power budget can be used for IT or emergency no-break systems can be \\nreduced. No-break systems are expensive and depending on the type of datacentre, these need \\nto be sized to allow all IT to function during power outages, as well as the cooling installations to \\nfacilitate the IT. The power systems can be downsized or, when already present, facilitate more IT.\\n3.3 Datacentre facilities\\nOverhead facilities can be downsized or existing capacity can allow for more IT.\\nThe minimised cooling requirements for Immersed Computing® result in smaller and heavily \\nsimplified cooling installations. However, if these are already present, there is more capacity for \\nmore IT power. In reusable heat scenarios, warm water can simply be used and cooled by a heat \\nuser which can eliminate coolers all together.\\nMinimised power requirements have a significant impact on the utilisation of the facility power \\nenvelope. The cooling power budget can be used for IT or emergency no-break systems can be \\nreduced. No-break systems are expensive and depending on the type of datacentre, these need \\nto be sized to allow all IT to function during power outages, as well as the cooling installations to \\nfacilitate the IT. The power systems can be downsized or, when already present, facilitate more IT.\\n18 | Immersed Computing® whitepaper\\n3.4 Datacentre operations\\nImmersed Computing® offers significant benefits towards business continuity and maintenance \\ncosts.\\nBusiness continuity is improved as total immersion of IT protects the IT itself much better than \\ntraditional air environments, because: \\n•\\t\\nNo oxygen gets in touch with the actual components which prevents oxidation. The \\nliquid creates a protective barrier since oxidation requires oxygen and water. The liquid \\nkeeps both away from the IT. This drastically reduces any physical degradation of the IT \\n\\xadcomponents over time; \\n•\\t\\nMoisture does not mix well with dielectric liquids. Since there are no moving parts and \\nall liquid movement is maintained by natural means, any accidental water accumulation \\n(unlikely spills or condensation) will just stay underneath hydrocarbon-based environments, \\nor on the surface of fluorocarbons. Residual moisture can easily be taken out by a liquid \\npolishing system which is part of the service trolley; \\n•\\t\\nThermal Shock is greatly reduced due to the high heat capacity of liquid. Where air cooled \\nsystems have enormous temperature fluctuations within the chassis when utilisation \\n\\xadfluctuates, the immersed environment only has minor fluctuations. This greatly reduces \\nstress by thermal expansion on micro-electronics.\\n•\\t\\nReduced component failures are the result of these effects. These in turn reduce compo-\\nnent costs and workload for service staff.\\nNormal maintenance costs are greatly reduced. Since total immersion eliminates the root cause \\nof most electronic component failures, the most important situation to address is regular main-\\ntenance, upgrades and renewals. Reduced and simplified overhead installations for power and \\ncooling also result in reduced potential for failure and maintenance.\\n19 | Immersed Computing® whitepaper\\n20 | Immersed Computing® whitepaper\\n3.5 IT hardware\\nIT hardware should be optimised for Total \\nLiquid Cooling to ensure the maximum \\n\\xadbeneficial effect. This means that servers will \\nbe able to perform much better than \\xadtraditional \\nair-cooled servers. The loading of a single \\nimmersed server can be sized to replace three \\nor more air-cooled servers. This results in \\nan enormous consolidation where less IT is \\nrequired to provide with the same amount of \\ndigital \\xadcapacity.\\n3.6 Software cost\\nFewer OS and CPU licences are required due \\nto the use of optimised IT hardware. Because \\nof the smaller number of physical servers, \\nfewer operating system licenses are required. \\nThe same can be said about applications \\nwhich adopt a per CPU licensing structure. \\nOften \\xaddatabase servers and virtualisation \\nsystems benefit from fewer CPUs from a licen-\\nsing perspective. Since licensing cost often \\noutweigh the hardware cost, this may in some \\ncases be the highest financial saving in the \\nentire value chain.\\nImage 3 Immersed Computing® facility\\n21 | Immersed Computing® whitepaper\\nCloud Value Chain\\nSingle tenant\\nHosting (IAAS/PAAS)\\nPrivate in colocation\\nSAAS in colocation\\nEnergy efficiency\\nSoftware efficiency\\nIT hardware\\nDC operations\\nDC facilities\\nDC build\\n3.7 Environments with greatest benefits\\nAll IT operators can benefit from investing in Immersed Computing®. Immersed Computing® not \\nonly positively impacts sustainability, but also many CAPEX and OPEX factors, dramatically redu-\\ncing TCO. However, the more links a company controls in the value chain, the more \\xadf\\xad\\xadinancially \\nattractive the business case for adopting Immersed Computing®. Especially for greenfield \\n\\xaddeployments.\\nIT owner/operators will achieve the best results, especially with HPC requirements in single tenant \\nenvironments.\\nHigh performance computing Immersed \\nComputing® solves density requirements and \\nallows for liquid cooling on a much larger scale \\nthan usual; \\nPrivate datacentres Private / hybrid cloud \\noperators who own the entire value chain expe-\\nrience the same benefits as cloud operators; \\nCloud optimised or HPC colo datacentres Colo-\\ncation datacentres which focus on \\xadfacilitating \\nHPC of high density cloud providers can faci-\\nlitate the growth of compute platforms by \\noffering Immersed Computing® as a dedicated \\nhousing environment; \\nEdge computing The limited infrastructure \\nrequirements of Immersed Computing®, as well \\nas the remote management capabilities enable \\nmicro/mobile edge deployments in urban areas; \\nMobile solutions The plug and play solutions \\nand minimal infrastructure requirements make \\napplications possible in remote geographical \\nareas where \\xadavailability of energy supply and \\ncooling is problematic.\\nImage 4 Immersed Computing® for different types of IT owners/operators\\n22 | Immersed Computing® whitepaper\\n4. Asperitas solutions\\nAsperitas has developed the solutions to fully integrate IT platforms within Immersed \\xadComputing®. \\nIt is based on immersion technology with a modular system, the AIC24, supplemented with the \\ntooling required for operating IT. The most important elements of the tooling are the service \\ntrolley and the maintenance supplies for working with immersed IT. Asperitas provides training \\nwhich \\xadaddresses the new elements around operating and maintaining Immersed Computing® as \\n\\xadeffectively as possible.\\nAIC24\\nThe AIC is a fully integrated, self-contained, plug and play, modular unit. The standard AIC24 has a \\nfootprint of 600x1200 mm and can contain up to 48 servers, 288 GPUs or any other combination of \\nIT components.\\nImage 5 AIC24\\n23 | Immersed Computing® whitepaper\\nTooling\\nAll the tooling required for the maintenance with wet IT is developed by Asperitas, including ESD \\nleak trays, spill kits, liquid supplies and more.\\nImage 6 Service trolley\\nTraining\\nThe following training is available at the \\n\\xadAsperitas Technology Centre: \\n•\\t\\nOperating AIC24 and the service trolley; \\n•\\t\\nDesigning IT for Immersed Computing®; \\n•\\t\\nRisk management with Immersed \\n\\xadComputing®.\\nService trolley\\nThe ESD protected service trolley has been \\ndeveloped and optimised for Immersed \\n\\xadComputing® with the AIC24 modules. It \\naddresses all maintenance: hoisting cassettes, \\nservicing and \\xadtransporting IT, cleaning up spills \\nand polishing liquid.\\n5. Best practices\\nBest practices in datacentre planning, processes and IT and platform design, help get the \\nmaximum results out of Immersed Computing®.*\\n5.1 Datacentre planning\\nConsidering high cooling temperatures when choosing the geographical location. Reusing the \\nenergy used in a datacentre becomes viable with Immersed Computing® because the heat is \\nalready captured in the facility liquid circuit, which allows easily transportable heat. This heat can \\nbe used in a completely different industry to replace energy used to heat something (heat grid, \\nindustrial process, swimming pools etc.). A datacentre built in the proximity of such an industry \\ncan achieve the maximum reuse of energy.\\nUsing the best technology for the best purpose is made easy with Immersed Computing®. Its \\n\\xadflexibility enables a hybrid approach to any IT platform environment where the best of all worlds \\ncan be applied. The simplicity and \\xadscalability of Immersed Computing® allows it to be applied in \\nharmony with any other \\xad(existing) environment.\\nOptimising temperatures for water loops is made possible by aligning different liquid-cooled \\n\\xadtechnologies appropriately. By implementing temperature chaining, which involves setting up \\ndifferent technologies in series, an air-cooled datacentre can build up the \\xadtemperature in the \\nwater loop. Water-cooled HVAC systems require the lowest temperatures, then a specific liquid IT \\nenvironment like rear door cooling, followed by an Immersed Computing® environment after which \\na Direct Liquid Cooled environment adds the last energy to the loop. This gives high temperature \\ndifferences on a cooling infrastructure which makes the cooling process more efficient, because \\na higher percentage of energy can be rejected without compression cooling. (Please also refer to \\n“The datacentre of The Future” whitepaper by Asperitas.\\n* In addition to: European Code of Conduct for Data Centres - Best practice guidelines OCP Immersion Requirements\\n24 | Immersed Computing® whitepaper\\nImage 7 ESD protected maintenance area\\n25 | Immersed Computing® whitepaper\\nDesignated liquid maintenance areas ensure clean and safe (large scale) maintenance of liquid-\\ncooled IT. A maintenance room for liquid is easily set up. Such a room is similar to common IT \\nmaintenance areas (ESD protected), but with a few fundamental differences like sufficient tooling \\nfor liquid management: Leak trays, consumables (cloths and gloves), ESD lab coats and a drain.\\n26 | Immersed Computing® whitepaper\\n5.2 Liquid processes\\nThere are specific safety protocols for liquid systems like spill management or fire safety. The \\noverall safety is not affected negatively, but awareness of different ways of work is required. \\nAwareness of potential disasters is usually within the basic mode of operation of any datacentre \\norganisation.\\nDocumented procedures and inventory of dielectric liquids are part of the safety protocols. All \\ndielectric liquids are chemicals, therefore each type of liquid comes with a Material Safety Data \\nSheet (MSDS) and a Technical Data Sheet (TDS). The MSDS contains all necessary safety related \\ninformation like classification and recommended safety procedures including fire management. \\nThe TDS contains all technical data like ingredients viscosity, density, pour- and flashpoint. These \\ndocuments should be available to anyone with unsupervised access to a facility where liquid is \\nused and easily and quickly reachable for emergency services.\\n27 | Immersed Computing® whitepaper\\n6. Asperitas company\\nAsperitas is a clean-tech company focused on greening the datacentre industry by introducing \\nImmersed Computing®.\\nSince 2014 Asperitas has developed Immersed Computing® as a unique approach to the IT \\nindustry. Building on existing IT platforms technologies by integrating liquid immersion cooling, \\npower and network components, improved cooling physics and a strong focus on design and \\n\\xadengineering for usability, Asperitas has come up with multiple integrated IT platform solutions \\nwhich can be effectively utilised in most, if not all situations.\\nThe Asperitas development partners include Intel, SuperMicro, Boston, Shell, Schleifenbauer \\n\\xadAqualectra and Brink Industrial. Asperitas is leading the industry through OCP and ASHRAE. \\nAsperitas is furthermore recognised and supported by the Netherlands Enterprise Agency as a \\n\\xadCleantech company.\\nImmersed Computing® whitepaper\\nCopyright © 2022 By Asperitas, www.asperitas.com\\nLaarderhoogtweg 18\\n1101 EA Amsterdam\\nThe Netherlands\\nKnooppunt Holendrecht\\nGaasperdammerwe\\nLaarderhoogtweg\\nKort\\nHaarlerbergweg\\nLuttenbergwe g\\nH\\na\\nk\\nfort\\nHakfort\\nAtlas ArenA\\nHessenbergweg\\nHondsrugweg\\nHullenbergweg\\nIkea\\n+31 88 96 000 00\\ninfo@asperitas.com\\nVisit www.asperitas.com\\n', 'Our ICEraQ line of micro-modular immersion cooling systems oﬀ ers \\nbreakthrough potential in rack density, location ﬂ exibility, and capacity \\nplanning, while also reducing the expense of building, running, and \\nexpanding a data center. It needs no energy-intensive air conditioners, \\noversize generators, or raised ﬂ oors. As a result, it enables rapid \\ndeployment of super-reliable, chilled, or chiller-free cooling right where you \\nwant it. And with GRC’s Earth-friendly focus, the ICEraQ Series 10 products \\nbypass the limits set by predecessors while advancing sustainability.\\nFeatures & Beneﬁ ts\\n \\n• Cuts cooling energy by up to 95%\\n \\n• Provides a pPUE of <1.03 \\n \\n• Lowers upfront costs by 50% \\n \\n• Reduces server power draw 10-20% \\n \\n• Cools up to 184 kW/rack 1 \\n \\n• Compatible with leading OEM servers \\n \\n• Fast deployment: typically within three months\\nCommon Applications:\\n \\n• Overcome space or power constraints\\n \\n• Surmount rising energy costs\\n \\n• Integrate high-density racks\\n \\n• Deploy capacity quickly\\n \\n• Reduce data center build costs\\n \\n• Take full advantage of virtualization beneﬁ ts\\n \\n• Support sustainability/green goals\\n \\nIncludes:\\n \\n• Rack(s) ﬁ lled with our high-performance, synthetic \\n \\n ElectroSafe® coolants\\n \\n• Coolant distribution unit (CDU)\\n \\n• Assured reliability with 2N-redundant pumps and control system\\n \\n• Schneider Electric’s Machine Advisor cloud based and local  \\n \\n monitoring capabilities with conﬁ gurable PagerDuty email alerts \\n \\n• Integrated cable management\\n \\n• Service bars for easy, in-rack server maintenance\\n \\n• Limited warranty, 24/7 on-call support and remote monitoring \\nExperience the Freedom to Add High-Density Compute Capacity Anywhere — Easily\\n+1.512.692.8003 • ContactUs@grcooling.com • grcooling.com\\nHigh-Eﬃ  ciency, Scalable, Rack-Based, \\nGreen Cooling Solutions for Data Centers\\nQuick \\nDeployment\\nLocation\\nFlexibility\\nSupports \\nHigh Density\\nEasy To\\nAdopt & Use \\nScaleable\\nCost-Effective\\nTokyo Institue \\nof Technology\\nOur Deployments Are in Twenty Countries Across the Globe\\nGRC immersion cooling drives mission-critical systems for \\nthese and many more organizations. \\nQuad | Duo\\nGreen/\\nSustainability\\nQuad Up to 92 kW per rack\\nDuo Up to 184 kW per rack\\n2021 Winner \\nProduct Innovation \\nof the Year\\n1   Utilizing a chilled water system.\\n2  General speciﬁ cation. \\n3  Additional redundancy options available.\\n4  Underﬂ oor CDU option for space constrained sites.\\n5 Does not include weight of IT equipment and accessories.\\n6  Warranty is void if ICEraQ units are run outside of their operating parameters deﬁ ned in the installation \\n speciﬁ cation.\\nQuad | Duo \\nInfrastructure / Site Requirements \\nClient to Provide \\nAccess to power & water\\n \\nLevel installation surface with slope < 1/650\\n \\n(raised ﬂ oor or concrete slab)\\nOperating Guidelines \\nAmbient temperature 5 to 40°C (41 to 104°F)  \\n \\nSecondary containment \\n \\nStandard data center ﬁ re suppression\\nMonitoring and Reporting\\nPlatform \\nSchneider Electric’s Machine Advisor cloud-based DCIM  \\n \\nand local DCIM hooks\\nAlerts \\nConﬁ gurable email alerts with PagerDuty application\\nDCIM/BMS\\nIntegration Protocols \\nModbus, BACnet, and RESTful API\\nData & Measurements \\nOperating temperatures (water and coolant)\\n \\nOperating pressures (water and coolant)\\n \\nPrimary coolant pump power consumption\\n \\nPrimary coolant pump speed\\n \\nRack temperatures\\n \\nLiquid level (multiple locations)\\n \\nSystem health, diagnostics, and early fault detection \\nDelivery & Installation\\nLead Time \\nTypically ships within three months of receipt of \\n \\npurchase order\\nShipping Terms \\nEx-Works\\nOn-site Installation \\nThree days for the ﬁ rst unit, plus two days for  \\n& Training \\nevery subsequent unit\\nWarranty \\n6\\nOne-Year, Limited \\nWarranty Includes \\n• 24/7 On-call GRC technical support staﬀ \\n \\n• 24/7 Remote monitoring\\n \\n• Worldwide on-site response for parts and labor \\n \\n• Regular maintenance as required  \\nExtended warranty and maintenance agreements are available at an \\nadditional cost.  \\nPlatform \\nSchneider Electric’s Machine Advisor cloud-based DCIM \\nand local DCIM hooks\\nGRC believes the information in this Data Sheet to be accurate; however, GRC does not make any representation or \\nwarranty, express or implied, as to the accuracy or completeness of any such information and shall have no liability \\nfor the consequences of the use of such information.\\nThis Data Sheet and its contents do not constitute an order by GRC to sell any product. An order is made only by \\na quotation provided by GRC. The terms of sale in such quotation may vary from those set forth in this Data Sheet. \\nGRC’s acceptance of any order shall be in GRC’s sole discretion, and all quotations and sales are subject to GRC’s \\nTerms and Conditions of Commercial Sale.\\nProduct \\nSpeciﬁ cations \\n    \\nQuad \\n  Duo   \\nNumber of Immersion \\nCooled Racks  \\n          \\n      4       \\n       2 \\nNumber of Cooling \\nDistribution Units (CDU)            \\nIntegrated              Integrated \\nChiller-Free Water @ 32° C (89.6°F) \\nCooling Capacity  \\n \\n200 kW \\n200 kW \\nPer Rack Density \\n \\n50 kW \\n100 kW \\nChilled Water @ 13° C (55.4°F) \\nCooling Capacity  \\n \\n368 kW \\n368 kW \\nPer Rack Density \\n \\n92 kW \\n184 kW \\nPartial PUE \\n2 \\n \\n<1.03  \\n<1.03   \\nRedundancy 3 \\n \\nCoolant pumps: 2N\\n  \\n \\nControl system: 2N\\nOverall Dimensions (l x w x h) 4 \\nSeries 10 Quad \\n5.09 m x 1.68 m x 1.42 m (200.38” x 66.25” x 56”)\\nSeries 10 Duo  \\n2.92 m x 1.68 m x 1.42 m (115.25” x 66.25” x 56”)\\nFloor Loading (Operational)5 \\n822 kg/m2  (168 lbs/ft2)\\nPower & Water Speciﬁ cations\\nFinal Heat Rejection Options \\nFlexible Options: \\n  \\n• Adiabatic/evaporative cooling tower\\n  \\n• Dry cooler \\n  \\n• Chilled water loop \\nWater Requirements \\nPossible water input temperature: \\n  \\n• 5 to 32˚C (41 to 89.6˚F) \\n  \\nRecirculating water ﬂ ow rate: \\n  \\n• 21 to 30 m3/hr (50 to 150 gpm)\\n  \\nConnections: \\n  \\n• 50.8 mm (2.0”) FNPT or hose barb \\nPower Requirements \\nTwo electrical feeds (primary & secondary) \\n  \\neach with the following characteristics:\\n  \\n• 3 Phase 200 to 240 VAC, \\n    \\n   OR 380 to 480 VAC, 50 to 60 Hz\\n  \\n• Max power consumption: 5.6kW\\nLead Time\\nTypically ships within three months of receipt of \\nf f\\npurchase order\\nOn-site Installation\\nThree days for the ﬁ rst unit, plus two days for \\n& Training \\nevery subsequent unit\\nCompatible with All Leading OEM Servers\\n11525 Stonehollow Drive, Suite A-135 Austin, TX 78758 \\n+1.512.692.8003 • ContactUs@grcooling.com • grcooling.com\\n©2022 GRC, Green Revolution Cooling, and The Immersion Cooling Authority are each registered trademarks of Green Revolution Cooling, Inc.\\n', ' \\n \\n \\n \\n \\n \\n \\n \\nWHITE PAPER \\n \\n \\n \\n \\n \\n \\nExecutive Summary \\nImmersion liquid cooling systems have proven their \\neffectiveness in multiple applications, focusing primarily on \\nimproving power efficiency. This means a tangible reduction in \\nutility costs and an enhancement in the ability to run greener in \\nthe data center. For software applications, such as EDA used in \\nthe semiconductor industry that could be running on per-core \\nlicensing models, the number of cores and clock speeds \\nrequired to achieve the same level of computing power \\ndetermines the runtime cost-effectiveness. With this type of \\nlicensing model, licensing cost reduction could be achieved by \\nutilizing a CPU with fewer cores of higher clock speeds at \\nequivalent computing power. However, the lower core-count \\nCPU at a comparative level of computing power comes at a \\nsubstantially higher processor case temperature, making \\nconventional air-cooling extremely inefficient. \\nSupermicro, joined by 3M, Intel®, Kaori, and Samsung, \\ndemonstrates a proof-of-concept single-phase immersion \\nliquid cooling system with our BigTwin server family of products. Supermicro’s partner Kaori developed an immersion-cooling \\ntank filled with 3M™ Fluorinert™ Electronic Liquid FC-40. The test results show that Intel high-frequency chips with a lower \\ncore count delivered high LINPACK performance, while Samsung DRAMs and Solid-State Drives achieved uncompromised \\nelectrical characteristics. Supermicro’s experimental outcome supports the idea that utilizing high clock speed chips with a \\nlower core count to drive for the same level of computing performance could benefit from the savings on software licensing\\nTABLE OF CONTENTS \\nExecutive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1 \\nEnhancing Heat Removal Capability with Immersion Liquid \\nCooling Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 \\nEDA Performance Boost Beyond Power Efficiency . . . . . . . . . . . .  3 \\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  4 \\nSUPERMICRO\\nSupermicro is a global leader in high performance, green \\ncomputing server technology and innovation. We provide our \\nglobal customers with application-optimized servers and \\nworkstations customized with blade, storage, and GPU \\nsolutions. Our products offer proven reliability, superior design, \\nand one of the industry’s broadest array of product \\nconfigurations, to fit all computational needs. \\n \\n \\n \\n \\n \\n2 \\nEDA Applications Get a Performance Boost  © Supermicro, Inc. May, 2022 \\nEnhancing Heat Removal Capability with \\nImmersion Liquid Cooling Systems \\nThe heavy demand for computing power drives the \\ndevelopment of newer processor chips with ever-\\nincreasing TDP. Compared to older generations, the \\nupcoming processors efficiently run at well over 250 \\nWatts TDP. As a result, this has the potential of \\ncollectively exceeding rack-level power capacity when \\nclustered. One way to mitigate this is by spreading out \\nthese “hot silos” areas to distribute heat sources better. \\nHowever, compromising compute density could result in \\nadditive burdens on the network infrastructure. The extra \\ncost and heat generated come with the need to rely more \\non fiber and high-speed networking. Supermicro \\nconsiders that densely clustered compute nodes would \\nstill be the commonly accepted configuration in the data \\ncenter and for high performance computing (HPC) \\napplications as long as the significant challenges in \\neffectively and economically removing heat can be \\nresolved. \\nAir as a medium for cooling will generally be less efficient \\nand less effective than liquid. For example, one way to \\nthink of this is that conventional air-cooling essentially \\ncools an object immersed in flowing air, utilizing air as an \\nagent to remove the heat. When comparing an object \\nimmersed in a flowing liquid, the liquid conjunction \\nintroduces more intimate and closer contact than the \\ngaseous conjunction. The higher heat transfer coefficient \\nwith liquid than gas manifests the adequate number of \\nmolecular-scale particles densely engaging with the \\nobject’s surface. \\nLiquid agents can carry more heat than air does of equal \\nvolume and be more thermally conductive than air. \\nHowever, the main reason conventional cooling systems \\nutilize air as an agent is not its heat transfer capability but \\nits nonconductivity and compatibility with most of the \\nwidely used electronic devices in the electronic hardware \\nindustry. \\nWith climate change, those used-to-be trivial problems \\nsuch as airborne particles, gaseous pollution, global \\nwarming, and volcanic activities make air-inlet \\nmanagement on data center premises more costly. \\nMoreover, given the ever-surging power density at the \\nrack level and the growing TDP at the chip level, it is \\nforeseeable that one day air-cooling systems may \\neventually hit the wall that may only be breached by \\nadvanced cooling technologies like immersion liquid \\ncooling systems. \\n \\n \\nTYPICAL LIQUID COOLING SOLUTIONS \\nLiquid cooling solutions are beneficial in improving \\ncooling efficiency, lowering overall wattage \\nconsumption, and mitigating cost. Various field-proven \\nliquid cooling techniques range from dissipating the heat \\ndirectly from the source to physically dissipating the heat \\noff-premises. \\nLiquid Engagement at Different Stages \\nIn the heat transfer and exchange process, there are \\nstages where liquid coolants could intervene as a cooling \\nagent: \\n• Direct-To-Chip Liquid Cooling Solutions ‐ a liquid \\ncoolant removes heat directly from high wattage \\nprocessors. Then the liquid agent may exchange heat \\nwith another liquid agent at the rack level or the \\nfacility level. \\n• In-Row or In-Rack Liquid Cooling Solutions ‐ rack-level \\nor row-level liquid cooling systems where air-to-liquid \\nheat exchange occurs at different system integration \\nlevels. The rear door heat exchanger (RDHx) is one \\ntypical example of a rack-level liquid cooling solution. \\n• Immersion Liquid Cooling Solutions ‐ the whole or \\npart of a system is immersed in nonconductive inert \\ndielectric liquids. Then these liquids transfer heat \\nwhile remaining in a liquid state via a liquid-to-liquid \\nheat exchanger or convert to a gaseous state while \\ncarrying the latent heat. Then the gaseous agent is \\ntransformed back into a fluid via a condenser. \\nVisit the Supermicro Liquid Cooling Solutions page for more \\ndetail. \\n \\n  \\n \\n \\n \\n \\n \\n3 \\nEDA Applications Get a Performance Boost  © Supermicro, Inc. May, 2022 \\nEDA Performance Boost Beyond Power Efficiency \\nImmersion liquid cooling systems offer efficiency in \\noperating IT and data center equipment at cooler \\ntemperatures, thus lowering total utility costs. Apart from \\nthe immediate benefit, additional tips brought tangible \\ncost savings to the data center by saving licensing fees. \\nThe software has typically been priced on a per-core or \\nper-socket model at the commercial level. Per-core \\nlicensing model becomes dominant as the market \\ndemands more compute power per socket or more cores. \\nEven industries that used to stick to the old school per-\\nsocket pricing model have started to charge higher \\nlicensing fees depending on the number of cores per CPU. \\nAs clock speeds increase and higher wattage CPUs \\nbecome a norm, IT workloads that leverage more cores \\nmaximizing software license utilization out of the less \\nper-socket fees, can no longer play the old trick after \\nsoftware companies pulled the plug. Thus, gearing up to \\nthe same level of computing power using fewer cores can \\nbe translated into a dollar value by reducing the number \\nof runtime licenses required. Low-core high-frequency \\nCPUs will be a rewarding substitute as long as the \\nexcessive heat driven by running the chips faster can be \\nresolved. Given the surging wattages in the next \\ngeneration of CPUs, a conventional approach using air-\\ncooling starts losing its edge. \\nSupermicro developed a proof-of-concept unit based on \\nthe X11 generation of our BigTwin® 2029BT-HNTR 2U 4-\\nNode high-density server system equipped with Intel® \\nXeon® Gold 6250 processors. The baseline comparison is \\nthe same system using the Intel® Xeon® Gold 6244 \\nprocessors. Both configurations are populated with \\nSamsung M393A2K40DB3-CWE 16GB 3200Mbps RDIMM \\nand PM983 960GB SSD. We immersed our system using \\nthe Intel® Xeon® Gold 6250 in a Kaori liquid cooling tank \\nfilled with 3M™ Fluorinert™ Electronic Liquid FC-40. The \\nsystem using the Intel® Xeon® Gold 6244 is cooled by \\nconventional air-cooling. The CPU calculation power \\nperformance is benchmarked with LINPACK, while \\nmemory and SSD performance are benchmarked with \\nStream and FIO. \\n \\nFigure 1 - BigTwin 2029BT-HNTR \\nThe resulting conclusions found that the Intel® Xeon® \\nGold 6250 in a low temperature chamber generated \\nnotably 8% higher LINPACK results, averaging from the \\nfour server nodes compared to the Intel® Xeon® Gold 6244 \\nin our air-cooled test environment. Notably, the \\nexperiment would have to be performed in an unrealistic \\nlow temperature chamber at freeze point to utilize the full \\nspeed of the Intel Xeon Gold 6250 processors. \\n \\nFigure 2 – Air Cooling Baseline Result \\nIn the case of the immersion-cooled Intel® Xeon® Gold \\n6250, the system at room temperature achieved slightly \\nhigher LINPACK results than the previous low \\ntemperature chamber results. Here the immersion liquid \\ncooling configuration delivered 12% lower power \\nconsumption than the previous low temperature \\nchamber. However, the actual power savings would have \\nachieved approximately 54% when accounting for the \\nexcessive power required to bring the temperature to \\nfreeze point in the air-cooled configuration. \\n \\n \\n \\n \\n \\n \\n4 \\nEDA Applications Get a Performance Boost  © Supermicro, Inc. May, 2022 \\nThe memory performance results from the Stream tool \\ndemonstrated a consistent memory bandwidth output as \\nopposed to the baseline configurations. In addition, \\nconsistency could also be observed in the NVMe Random \\nand Sequential Read/Write performance based on the FIO \\ntool. \\nThe Intel® Xeon® processors and the Samsung DIMMs and \\nSSDs demonstrated excellent compatibility with the 3M™ \\nFluorinert™ Electronic Liquid FC-40 without any signs of \\ndamage or degradation. In addition, the Kaori immersion \\nliquid cooling tank with the plated heat exchange system \\nprovided a solid stand-alone test bench in the lab \\nenvironment. \\nProcessors with higher frequencies will bring along higher \\nLINPACK results. Utilizing lower core count processors at \\nhigher frequencies to achieve similar performance levels \\nas higher core count processors at lower frequencies \\ncould bring cost saving benefits in the runtime licensing, \\na cherry on top of the significant power consumption \\nmerits. \\n \\nConclusion \\n Supermicro delivers the highest performance systems to \\ndata center operators. New technologies in the electronic \\ndesign field, AI, and data analytics will require the fastest \\nand most capable CPUs and GPUs for these new \\nworkloads. In addition, the higher wattage systems may \\nrequire liquid cooling for those data center environments \\npushing the limits of air cooling. \\nFigure 3 – Intel® Xeon® Gold 6250 Air vs. Liquid Performance Result \\nFigure 4 - Supermicro Proof-Of-Concept Immersion \\nLiquid Cooling Demo Unit Jointly Developed with \\n3M, Intel, Kaori, and Samsung \\n \\n \\n \\n \\n \\n5 \\nEDA Applications Get a Performance Boost  © Supermicro, Inc. May, 2022 \\nSupermicro, while teaming up with Intel, Samsung, Kaori, \\nand 3M, was able to demonstrate a unique high-\\nperformance CPU with high heat is workable in an actual \\nenvironment and got effective power consumption \\nreductions when adopting immersion cooling systems, \\neven in some extreme environments without air \\ncondition. \\nMeanwhile, it will reduce server system OPEX by lower \\ntotal power consumption. Additional benefits include the \\npotential savings in software runtime license fees due to \\nthe demonstration that similar performance levels can be \\nachieved with low core-count, high clock speed CPUs. \\nThese key points will be conducive to high-performance \\nservers implemented in the future, especially for some \\nfields such as the semiconductor industry, which is \\nalready familiar with the characteristics of fluorochemical \\nfor a long time. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nConfiguration 1 \\nConfiguration 2 \\nProcessor \\nIntel® Xeon® Gold 6250 \\nIntel® Xeon® Gold 6244 \\n# of Cores/CPU \\n8 \\n8 \\n# of Threads/CPU \\n16 \\n16 \\nProcessor Base \\nFrequency \\n3.90 GHz \\n3.60 GHz \\nMax Turbo \\nFrequency \\n4.50 GHz \\n4.40 GHz \\nProcessor TDP \\n185 W \\n150 W \\nTcase \\n60deg C \\n74deg C \\nSystem \\nSYS-2029BT-HNTR \\nSYS-2029BT-HNTR \\nCPU/System \\n8 \\n8 \\nMemory/System \\n16GB 3200 DDR4 RDIMM Samsung M393A2K40DB3-CWE x 96 \\n16GB 3200 DDR4 RDIMM Samsung M393A2K40DB3-CWE x 96 \\nNVMe \\nDrive/System \\n2.5” Gen3 NVMe 960GB Samsung PM983 x 16 \\n2.5” Gen3 NVMe 960GB Samsung PM983 x 16 \\nSATA SSD/System \\n2.5” SATA3 240GB Samsung PM883 x 8 \\n2.5” SATA3 240GB Samsung PM883 x 8 \\nSIOM/System \\nAOC-MGP-i2M x 4 \\nAOC-MGP-i2M x 4 \\n \\n \\n \\n \\n \\n6 \\nEDA Applications Get a Performance Boost  © Supermicro, Inc. May, 2022 \\nSystem Configurations \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTable 1 - Memory and NVMe Test Results \\n']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'filename': 'Sustainability-With-Substance-White-Paper.pdf'}, {'filename': 'Schneider Electric understand the total sustainability impact of liion UPS batteries.pdf'}, {'filename': 'shell-immersion-cooling-fluid-s5-x-brochure.pdf'}, {'filename': 'Five reasons to adop liquid cooling.pdf'}, {'filename': 'Wyoming_Use_Case_v2.pdf'}]]\n",
      "[[{'filename': 'Sustainability-With-Substance-White-Paper.pdf'}, {'filename': 'Schneider Electric understand the total sustainability impact of liion UPS batteries.pdf'}, {'filename': 'Wyoming_Use_Case_v2.pdf'}, {'filename': 'MergeIT-SustainableAppAdjacentVDIForAI&HPCWorkloads-Infographic-1280x720px-RGB-mk1.pdf'}, {'filename': 'Site Readiness Checklist Template JAN 2024 (2).pdf'}]]\n",
      "[[{'filename': 'Hypertec  Immersion-Born Trident Servers  5-13-2024.pdf'}, {'filename': 'Navigating Liquid Cooling Architectures for Data Centers with AI Workloads.pdf'}, {'filename': 'GRC-iceraq-series10-data-sheet Quad - Duo.pdf'}, {'filename': 'Vertiv-LiquidCooling-KIH-WP-EN-NA-SL.pdf'}, {'filename': 'Five reasons to adop liquid cooling.pdf'}]]\n"
     ]
    }
   ],
   "source": [
    "# Query the vector database\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"power usage\"],\n",
    "    n_results=5,\n",
    "    include=['documents', 'distances', 'metadatas']\n",
    ")\n",
    "print(results['metadatas'])\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"sustainability impact\"],\n",
    "    n_results=5,\n",
    "    include=['documents', 'distances', 'metadatas']\n",
    ")\n",
    "print(results['metadatas'])\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"cooling system\"],\n",
    "    n_results=5,\n",
    "    include=['documents', 'distances', 'metadatas']\n",
    ")\n",
    "print(results['metadatas'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

version: '3.8'

services:
  # vLLM - LLM inference service with the smallest model
  vllm:
    build:
      context: ./vllm
      dockerfile: Dockerfile
    container_name: vllm
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [ gpu ]
    ports:
      - "8000:8000"
    # environment:
    #   - MODEL_NAME=TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF # Smallest model (1.1B)
    #   - TENSOR_PARALLEL_SIZE=1
    volumes:
      - ./models:/models # Mount your model directory (if needed)
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./vllm/app:/app # Mount your code directory
    ipc: host
    secrets:
      - huggingface_key
    # command: '--model mistralai/Mistral-7B-v0.1 --chat-template /app/chat_template.yaml'
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://0.0.0.0:5000/v1/models" ]
      interval: 30s
      timeout: 5s
      retries: 20
  # LlamaIndex - For RAG handling
  llamaindex:
    image: python:3.9-slim
    container_name: llamaindex
    command: >
      bash -c "pip install llama-index llama-index-vector-stores-chroma openai && export OPENAI_API_KEY=$(cat /run/secrets/openai_api_key) && python3 -m llama_index.run"
    ports:
      - "5001:5000"
    volumes:
      - ./docs:/app/docs
    secrets:
      - openai_api_key

  # Open WebUI - Chat interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
      - ./chat_template.yaml:/app/chat_template.yaml # Mount the chat template file
    environment:
      - API_URL=http://vllm:8000/v1
      - 'OPENAI_API_BASE_URL=http://vllm:8000/v1'
      # - ENABLE_OLLAMA_API=false
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_WEB_SEARCH_ENGINE=duckduckgo
      - CHAT_TEMPLATE=/app/chat_template.yaml # Set the chat template path

  # LangGraph - AI Workflow Orchestrator
  langgraph:
    image: python:3.9-slim
    container_name: langgraph
    command: >
      bash -c "pip install langgraph langchain openai requests && python3 /app/langgraph_server.py"
    volumes:
      - ./langgraph:/app # Mount local directory for code
    ports:
      - "5000:5000" # Expose LangGraph API
    environment:
      - OPENAI_API_KEY=$(cat /run/secrets/openai_api_key) # If using OpenAI embeddings
      - VLLM_API=http://vllm:8000/v1 # Connect to vLLM
      - LLAMAINDEX_API=http://llamaindex:5001 # Connect to LlamaIndex

  # FlowiseAI - No-code RAG workflow manager
  flowise:
    image: flowiseai/flowise:latest
    container_name: flowise
    ports:
      - "3001:3000"
    environment:
      - DATABASE_URL=sqlite:///flowise.db
    volumes:
      - ./flowise_data:/app/data

secrets:
  huggingface_key:
    file: ./hf_key.txt
  openai_api_key:
    file: ./open_ai_key.txt

volumes:
  open-webui:


networks:
  default:
    name: llm-rag-network

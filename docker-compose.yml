services:

  # model_quantizer:
  #   build:
  #     context: ./model_quantizer
  #     dockerfile: Dockerfile
  #   container_name: model_quantizer
  #   runtime: nvidia
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [ gpu ]
  #   volumes:
  #     - ./models:/models # Mount your model directory
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   secrets:
  #     - huggingface_key
  #   command: python3 quantize_model.py

  # vLLM - LLM inference service with the smallest model
  vllm:
    build:
      context: ./vllm
      dockerfile: Dockerfile
    container_name: vllm
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [ gpu ]
    ports:
      - "8000:8000"
    # environment:
    #   - MODEL_NAME=TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF # Smallest model (1.1B)
    #   - TENSOR_PARALLEL_SIZE=1
    volumes:
      - ./models:/models # Mount your model directory (if needed)
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./vllm/app:/app # Mount your code directory
    ipc: host
    secrets:
      - huggingface_key
    command: '--model /models/Mistral-7B-Instruct-v0.3-quantized.w8a16 --chat-template /app/chat_template.yaml --gpu_memory_utilization 0.95 --max_model_len 3000'

    healthcheck:
      test: [ "CMD", "curl", "-f", "http://0.0.0.0:5000/v1/models" ]
      interval: 30s
      timeout: 5s
      retries: 20
    # Open WebUI - Chat interface
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    # ports:
    #   - "3010:8080"
    volumes:
      - open-webui:/app/backend/data
      - ./chat_template.yaml:/app/chat_template.yaml # Mount the chat template file
    environment:
      - API_URL=http://vllm:8000/v1
      - 'OPENAI_API_BASE_URL=http://vllm:8000/v1'
      # - ENABLE_OLLAMA_API=false
      - ENABLE_RAG_WEB_SEARCH=true
      - RAG_WEB_SEARCH_ENGINE=duckduckgo
      - CHAT_TEMPLATE=/app/chat_template.yaml # Set the chat template path
      - 'WEBUI_SECRET_KEY='
      # volumes:
      # - open-webui:/app/backend/data

    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    # ChromaDB - Database for RAG
  chroma:
    container_name: chromadb
    ports:
      - 8800:8000
    image: ghcr.io/chroma-core/chroma:latest
    # # LlamaIndex - For RAG handling
  llamaindex:
    image: python:3.9-slim
    container_name: llamaindex
    command: >
      bash -c "pip install llama-index llama-index-vector-stores-chroma llama-index-embeddings-huggingface openai && export OPENAI_API_KEY=$(cat /run/secrets/openai_api_key) && python3 /app/starter_local.py"

    # bash -c "pip install llama-index llama-index-vector-stores-chroma openai && export OPENAI_API_KEY=$(cat /run/secrets/openai_api_key) && python3 -m llama_index.run"

    volumes:
      - ./docs:/app/docs
      - ./llamaindex:/app
    ports:
      - "5001:5000"
    environment:
      - CHROMA_API_URL=http://localhost:8800
      # secrets:
      # - openai_api_key
      # LangGraph - AI Workflow Orchestrator
      # langgraph:
      #   image: python:3.9-slim
      #   container_name: langgraph
      #   command: >
      #     bash -c "pip install langgraph langchain openai requests && python3 /app/langgraph_server.py"
      #   volumes:
      #     - ./langgraph:/app # Mount local directory for code
      #   ports:
      #     - "5000:5000" # Expose LangGraph API
      #   environment:
      #     - OPENAI_API_KEY=$(cat /run/secrets/openai_api_key) # If using OpenAI embeddings
      #     - VLLM_API=http://vllm:8000/v1 # Connect to vLLM
      #     - LLAMAINDEX_API=http://llamaindex:5001 # Connect to LlamaIndex

      # # FlowiseAI - No-code RAG workflow manager
      # flowise:
      #   image: flowiseai/flowise:latest
      #   container_name: flowise
      #   ports:
      #     - "3001:3000"
      #   environment:
      #     - DATABASE_URL=sqlite:///flowise.db
      #   volumes:
      #     - ./flowise_data:/app/data

secrets:
  huggingface_key:
    file: ./hf_key.txt
  openai_api_key:
    file: ./open_ai_key.txt

volumes:
  open-webui:
